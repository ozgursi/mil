{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mode\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mode\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "import GPyOpt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import plot_prototypes\n",
    "from model import ShapeletGenerator, pairwise_dist\n",
    "from mil import get_data\n",
    "#from prototype_forest import PrototypeForest\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gram_matrix(mat):\n",
    "  mat = mat.squeeze(dim=0)\n",
    "  mat = torch.mm(mat, mat.t())\n",
    "  return mat\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "  x_norm = (x.norm(dim=2)[:, :, None])\n",
    "  y_t = y.permute(0, 2, 1).contiguous()\n",
    "  y_norm = (y.norm(dim=2)[:, None])\n",
    "  y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "  dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "  return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "class ShapeletGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_prototypes, bag_size, n_classes, features):\n",
    "        n_prototypes = int(n_prototypes)\n",
    "        super(ShapeletGenerator, self).__init__()\n",
    "\n",
    "        number_of_rows = features.shape[0]\n",
    "\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                          size=1, \n",
    "                                         replace=False)\n",
    "        \n",
    "        prot = features[random_indices, :]\n",
    "        print(features.shape)\n",
    "        prot = prot.reshape(1, n_prototypes, prot.shape[1])\n",
    "        prot = prot.astype(\"float32\")\n",
    "        self.prototypes = torch.from_numpy(prot).requires_grad_()\n",
    "        #self.prototypes = (torch.randn(\n",
    "        #    (1, n_prototypes, bag_size))).requires_grad_()\n",
    "        if n_classes == 2:\n",
    "            n_classes = 1\n",
    "        self.linear_layer = torch.nn.Linear(3 * n_prototypes, n_classes, bias=False)\n",
    "        #self.linear_layer.weight = torch.nn.Parameter(self.linear_layer.weight/100000)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def pairwise_distances(self, x, y):\n",
    "        x_norm = (x.norm(dim=2)[:, :, None])\n",
    "        y_t = y.permute(0, 2, 1).contiguous()\n",
    "        y_norm = (y.norm(dim=2)[:, None])\n",
    "        y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "        dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "        return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "    def get_output(self, batch_inp):\n",
    "        dist = self.pairwise_distances(batch_inp, self.prototypes)\n",
    "        min_dist = dist.min(dim=1)[0]\n",
    "        max_dist = dist.max(dim=1)[0]\n",
    "        mean_dist = dist.mean(dim=1)\n",
    "        all_features = torch.cat([min_dist, max_dist, mean_dist], dim=1)\n",
    "        logits = self.linear_layer(all_features)\n",
    "\n",
    "        return logits, all_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, distances = self.get_output(x)\n",
    "        if self.n_classes == 1:\n",
    "          logits = logits.view(1)\n",
    "        return logits, distances\n",
    "\n",
    "\n",
    "def convert_to_bags(data,\n",
    "                    split_instances=False,\n",
    "                    instance_norm=True,\n",
    "                    split_ratio=0.2,\n",
    "                    stride_ratio=0.5):\n",
    "  bags = []\n",
    "  labels = []\n",
    "  current_bag = []\n",
    "  current_label = data[0, 0]\n",
    "  cur = data[0, 1]\n",
    "  instance_size = np.round(split_ratio * data[0, 2:].shape[0]).astype(\"int\")\n",
    "  stride = np.round(stride_ratio * instance_size).astype(\"int\")\n",
    "\n",
    "  for i in range(data.shape[0]):\n",
    "    if data[i, 1] == cur:\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "    else:\n",
    "      bags.append(np.array(current_bag))\n",
    "      labels.append(np.array(current_label))\n",
    "      current_label = data[i, 0]\n",
    "      current_bag = []\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "      cur = data[i, 1]\n",
    "  bags.append(np.array(current_bag))\n",
    "  labels.append(np.array(current_label, dtype=\"int32\"))\n",
    "  return bags, labels\n",
    "\n",
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels,\n",
    "                   early_stopping_round = 10):\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = generate_random(parameters[0][0], parameters[0][1])\n",
    "    reg_lambda_w = generate_random(parameters[1][0], parameters[1][1])\n",
    "    reg_lambda_p = generate_random(parameters[2][0], parameters[2][1])\n",
    "    lr_prot = generate_random(parameters[3][0], parameters[3][1])\n",
    "    lr_weights = generate_random(parameters[4][0], parameters[4][1])\n",
    "    reg_w = 1\n",
    "    n_prototypes = 1\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    \n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "            \n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes, features)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            cont = True\n",
    "            \n",
    "            max_stagnation = 0 # number of epochs without improvement to tolerate\n",
    "            best_prototype = None\n",
    "            best_score = 0\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_steps and max_stagnation < early_stopping_round:\n",
    "                i += 1\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() - reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        score_auc = roc_auc_score(y_true_list, y_score_list)\n",
    "                        #print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                        #    \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        #score_auc)\n",
    "                        \n",
    "                        if score_auc > best_score:\n",
    "                            best_score = score_auc\n",
    "                            best_prototype = model.prototypes\n",
    "                            max_stagnation = 0\n",
    "                        else:\n",
    "                            max_stagnation += 1\n",
    "                        \n",
    "                        #print(\"max_stagnation \", max_stagnation)\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return best_prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self,\n",
    "                 train_features,\n",
    "                 feature_types = [\"min\", \"max\", \"mean\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner=True,\n",
    "                 early_stopping_round = 3):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.train_features = train_features\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        if self.use_prototype_learner:\n",
    "            prototypes = find_prototype(bags, features, labels, self.early_stopping_round)\n",
    "            check = prototypes.cpu().detach().numpy()\n",
    "\n",
    "            check.resize(check.shape[1], check.shape[2])\n",
    "            \n",
    "            return check\n",
    "        \n",
    "        else:\n",
    "            number_of_rows = self.train_features.shape[0]\n",
    "            random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=prototype_count, \n",
    "                                              replace=False)\n",
    "            \n",
    "            prot = self.train_features[random_indices, :]\n",
    "            if len(prot.shape) == 1:\n",
    "                prot = prot.reshape(1, prot.shape[0])\n",
    "            return prot\n",
    "\n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        for one_class in self.classes:\n",
    "            proba = y[y == one_class].shape[0] / y.shape[0]\n",
    "            probas.append(proba)\n",
    "        return np.asarray(probas)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "\n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_sum = np.add.reduceat(distances[:, i], index)\n",
    "                group_mean = np.add.reduceat(distances[:, i], index)\n",
    "                mean_vals = np.repeat(group_mean, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "\n",
    "        return np.array(np.transpose(feature_list))\n",
    "\n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_via_prototype, labels, bag_ids):\n",
    "        bdc = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  max_depth=1, \n",
    "                                  criterion=\"entropy\",\n",
    "                                  min_samples_split=2)\n",
    "        bdc.fit(features_via_prototype, labels.flatten())\n",
    "\n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        labels_left = labels[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "\n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if features.shape[0] < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "\n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if features_left.shape[0] < self.min_samples_leaf or features_right.shape[0] < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "\n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left)\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right)\n",
    "\n",
    "            # splitting recursevely\n",
    "\n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "\n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.classes = np.unique(labels)\n",
    "\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "\n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "\n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "\n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "\n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "\n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = np.argmax(self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree))\n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "\n",
    "        return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(features, labels, bag_ids, stratified, sample_rate):\n",
    "    if stratified:\n",
    "        pos_sample_size = math.ceil(np.where(labels == 1)[0].shape[0] * sample_rate)\n",
    "        neg_sample_size = math.ceil(np.where(labels == 0)[0].shape[0] * sample_rate)\n",
    "        indices_pos = np.random.choice(np.where(labels == 1)[0], pos_sample_size, replace=False)\n",
    "        indices_neg = np.random.choice(np.where(labels == 0)[0], neg_sample_size, replace=False)\n",
    "        inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "    else:\n",
    "        sample_size = math.ceil(labels.shape[0] * sample_rate)\n",
    "        inbag_indices = np.random.choice(np.where(labels == 1)[0], sample_size, replace=False)\n",
    "\n",
    "    oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "    oo_bag_mask[inbag_indices] = False\n",
    "\n",
    "    outbag_indices = np.where(oo_bag_mask == 1)\n",
    "\n",
    "    return inbag_indices, outbag_indices\n",
    "\n",
    "def get_parameter_scores(features, labels, bag_ids, params, fit_on_full = True):\n",
    "    keys, values = zip(*params.items())\n",
    "    params_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    param_vals_scores = dict()\n",
    "    for param_vals in params_list:\n",
    "        if param_vals[\"explained_variance\"] < 1:\n",
    "            pipe = Pipeline([('pca', PCA(n_components = param_vals[\"explained_variance\"], \n",
    "                             svd_solver = \"full\")), \n",
    "             ('scaler', StandardScaler()), ])\n",
    "        else:\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "        pipe.fit(features)\n",
    "\n",
    "        train_features = pipe.transform(features)\n",
    "        test_features = pipe.transform(features)\n",
    "\n",
    "        score_list = []\n",
    "        for i in range(0, param_vals[\"forest_size\"]):\n",
    "            (inbag_indices,\n",
    "             outbag_indices) = sample(features, labels, bag_ids, stratified = True, sample_rate = 0.8)      \n",
    "\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "\n",
    "            outbag_features = features[outbag_indices]\n",
    "            outbag_labels = labels[outbag_indices]\n",
    "            outbag_bag_ids = bag_ids[outbag_indices]\n",
    "\n",
    "            model = PrototypeTreeClassifier(max_depth=param_vals[\"max_depth\"], \n",
    "                                           min_samples_leaf=param_vals[\"min_samples_leaf\"],\n",
    "                                           min_samples_split=2)\n",
    "\n",
    "            model.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            preds = model.predict(outbag_features, outbag_bag_ids)\n",
    "\n",
    "            score = metrics.roc_auc_score(outbag_labels, preds)\n",
    "            score_list.append(score)\n",
    "\n",
    "        mean_score = sum(score_list)/len(score_list)\n",
    "        key = frozenset(param_vals.items())\n",
    "        param_vals_scores[key] = mean_score\n",
    "\n",
    "    return param_vals_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0, 1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "\n",
    "    sort_index = np.argsort(bag_ids)\n",
    "    bag_ids = bag_ids[sort_index]\n",
    "    features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False):\n",
    "    data = pd.read_csv(f\"/home/erdemb/Documents/data/{dataset}.csv\", header=None)\n",
    "    testbags =  pd.read_csv(f\"/home/erdemb/Documents/libs/multi-instance-learning/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "          \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin(['0','1'])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (train_features, train_labels, train_bag_ids,\n",
    "           test_features, test_labels, test_bag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"],\n",
    "                max_depth = 8, \n",
    "                min_samples_leaf = 2, \n",
    "                min_samples_split = 2, \n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        group_min = np.minimum.reduceat(labels, index)\n",
    "        pos_bag_size = math.ceil(np.where(group_min == 1)[0].shape[0] * 0.8)\n",
    "        neg_bag_size = math.ceil(np.where(group_min == 0)[0].shape[0] * 0.8)\n",
    "        bags_pos = np.random.choice(np.where(group_min == 1)[0], pos_bag_size, replace=False)\n",
    "        bags_neg = np.random.choice(np.where(group_min == 0)[0], neg_bag_size, replace=False)\n",
    "        df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1),\n",
    "                                          train_labels.reshape(train_labels.shape[0],1)],\n",
    "                                         axis=1))\n",
    "        indices_pos = df[df[0].isin(bags_pos)].index.to_numpy()\n",
    "        indices_neg = df[df[0].isin(bags_neg)].index.to_numpy()\n",
    "        inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "        \n",
    "        return inbag_indices, outbag_indices\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            if self.use_prototype_learner:\n",
    "                print(f\"Tree {i} will be trained\")\n",
    "\n",
    "            (inbag_indices,\n",
    "             outbag_indices) = self.sample(features, labels, bag_ids)\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            self._trees.append(tree)\n",
    "            \n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return mode(preds,1)[0]\n",
    "    \n",
    "    def predict_proba(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return np.sum(preds==1, axis=1)/self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is 13.408230543136597\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Web6\"\n",
    "\n",
    "forest_param = [[50, 100, 200],[4, 8, 10]]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#scores = pd.DataFrame(columns = [\"size\", \"max_depth\", \"score\"])\n",
    "scores = []\n",
    "\n",
    "for i in forest_param[0]:\n",
    "    for j in forest_param[1]:\n",
    "        dataset = \"Newsgroups6\"\n",
    "\n",
    "        (train_features,\n",
    "             train_labels,\n",
    "             train_bag_ids,\n",
    "             test_features,\n",
    "             test_labels,\n",
    "             test_bag_ids) = train_test_split(dataset, 5, 10, 1, fit_on_full = False, custom=True)\n",
    "\n",
    "        model = PrototypeForest(size=i,\n",
    "                                max_depth=j,\n",
    "                                min_samples_leaf=40,\n",
    "                                min_samples_split=80,\n",
    "                                prototype_count=1,\n",
    "                                early_stopping_round= 3,\n",
    "                                use_prototype_learner = False)\n",
    "\n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "        probas = model.predict_proba(test_features, test_bag_ids)\n",
    "\n",
    "        score = metrics.roc_auc_score(test_labels, probas)\n",
    "        scores.append([i, j, score])\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "df = pd.DataFrame(scores, columns = [\"size\", \"max_depth\", \"score\"])\n",
    "print(f\"Total time is {total_time}\")\n",
    "\n",
    "best_row = df.iloc[df[\"score\"].argmax()]\n",
    "best_size = int(best_row.get(\"size\"))\n",
    "best_depth = int(best_row.get(\"max_depth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 3, fold 5\n",
      "Tree 0 will be trained\n",
      "(1983, 200)\n",
      "Step :  325 Loss:  8.60169921875  accuracy:  0.5476923076923077\n",
      "Step :  325 Loss:  29.669359975961537  accuracy:  0.4\n",
      "Step :  325 Loss:  59.35506610576923  accuracy:  0.4461538461538462\n",
      "Step :  325 Loss:  59.20760216346154  accuracy:  0.39692307692307693\n",
      "Step :  325 Loss:  68.14852764423077  accuracy:  0.4276923076923077\n",
      "Step :  325 Loss:  83.88891826923077  accuracy:  0.40923076923076923\n",
      "Step :  325 Loss:  66.39602764423077  accuracy:  0.4369230769230769\n",
      "Step :  325 Loss:  106.9534375  accuracy:  0.41846153846153844\n",
      "Step :  325 Loss:  120.63668269230769  accuracy:  0.4584615384615385\n",
      "Step :  325 Loss:  81.30638822115385  accuracy:  0.4676923076923077\n",
      "Step :  325 Loss:  95.60231370192308  accuracy:  0.4553846153846154\n",
      "Step :  325 Loss:  97.90719951923077  accuracy:  0.4123076923076923\n",
      "Step :  325 Loss:  79.68255408653846  accuracy:  0.47384615384615386\n",
      "Step :  325 Loss:  130.7722235576923  accuracy:  0.49538461538461537\n",
      "Step :  325 Loss:  183.4928485576923  accuracy:  0.4707692307692308\n",
      "Step :  325 Loss:  244.37997596153846  accuracy:  0.4369230769230769\n",
      "Step :  325 Loss:  321.3730048076923  accuracy:  0.46153846153846156\n",
      "(572, 200)\n",
      "Step :  85 Loss:  9.505268411075368  accuracy:  0.7411764705882353\n",
      "Step :  85 Loss:  19.311830767463235  accuracy:  0.5529411764705883\n",
      "Step :  85 Loss:  21.395471909466913  accuracy:  0.6941176470588235\n",
      "Step :  85 Loss:  71.72720588235295  accuracy:  0.7294117647058823\n",
      "Step :  85 Loss:  24.50660328584559  accuracy:  0.7294117647058823\n",
      "Step :  85 Loss:  19.260248161764707  accuracy:  0.7411764705882353\n",
      "Step :  85 Loss:  69.92757927389705  accuracy:  0.7411764705882353\n",
      "Step :  85 Loss:  68.54966681985294  accuracy:  0.7411764705882353\n",
      "Step :  85 Loss:  50.67652803308823  accuracy:  0.7411764705882353\n",
      "Step :  85 Loss:  0.03261869093951057  accuracy:  0.8117647058823529\n",
      "(1411, 200)\n",
      "Step :  240 Loss:  34.907657877604166  accuracy:  0.4791666666666667\n",
      "Step :  240 Loss:  87.81180826822917  accuracy:  0.3458333333333333\n",
      "Step :  240 Loss:  142.86145833333333  accuracy:  0.3458333333333333\n",
      "Step :  240 Loss:  167.56787109375  accuracy:  0.4375\n",
      "Step :  240 Loss:  283.78447265625  accuracy:  0.44583333333333336\n",
      "Step :  240 Loss:  343.81865234375  accuracy:  0.32083333333333336\n",
      "Step :  240 Loss:  236.81209309895834  accuracy:  0.45\n",
      "Step :  240 Loss:  350.16315104166665  accuracy:  0.3541666666666667\n",
      "(330, 200)\n",
      "Step :  55 Loss:  5.697879305752841  accuracy:  0.6181818181818182\n",
      "Step :  55 Loss:  17.49748313210227  accuracy:  0.5818181818181818\n",
      "Step :  55 Loss:  18.992899946732955  accuracy:  0.2909090909090909\n",
      "Step :  55 Loss:  14.892478249289773  accuracy:  0.5636363636363636\n",
      "Step :  55 Loss:  28.079658647017045  accuracy:  0.6545454545454545\n",
      "Step :  55 Loss:  30.95341796875  accuracy:  0.6727272727272727\n",
      "(135, 200)\n",
      "Step :  25 Loss:  1.2041861724853515  accuracy:  0.52\n",
      "Step :  25 Loss:  2.38640625  accuracy:  0.4\n",
      "Step :  25 Loss:  2.0598260498046876  accuracy:  0.56\n",
      "Step :  25 Loss:  2.5383546447753904  accuracy:  0.32\n",
      "Step :  25 Loss:  1.724217987060547  accuracy:  0.48\n",
      "Step :  25 Loss:  5.594945068359375  accuracy:  0.52\n",
      "Step :  25 Loss:  9.149356689453125  accuracy:  0.4\n",
      "Step :  25 Loss:  6.9698388671875  accuracy:  0.44\n",
      "(1081, 200)\n",
      "Step :  185 Loss:  10.15242820945946  accuracy:  0.518918918918919\n",
      "Step :  185 Loss:  20.38651419974662  accuracy:  0.43783783783783786\n",
      "Step :  185 Loss:  68.80389041385135  accuracy:  0.43783783783783786\n",
      "Step :  185 Loss:  83.92629328547298  accuracy:  0.43783783783783786\n",
      "Step :  185 Loss:  123.03084881756757  accuracy:  0.4756756756756757\n",
      "Step :  185 Loss:  142.975390625  accuracy:  0.41621621621621624\n",
      "(294, 200)\n",
      "Step :  45 Loss:  11.509049479166666  accuracy:  0.6444444444444445\n",
      "Step :  45 Loss:  3.188417222764757  accuracy:  0.6444444444444445\n",
      "Step :  45 Loss:  30.39663357204861  accuracy:  0.4\n",
      "Step :  45 Loss:  22.194364420572917  accuracy:  0.5777777777777777\n",
      "Step :  45 Loss:  5.176738484700521  accuracy:  0.6\n",
      "Step :  45 Loss:  84.159765625  accuracy:  0.6\n",
      "Step :  45 Loss:  53.74648980034722  accuracy:  0.5777777777777777\n",
      "(787, 200)\n",
      "Step :  140 Loss:  36.43555385044643  accuracy:  0.7\n",
      "Step :  140 Loss:  71.783251953125  accuracy:  0.5\n",
      "Step :  140 Loss:  91.64398716517857  accuracy:  0.6642857142857143\n",
      "Step :  140 Loss:  81.05565011160714  accuracy:  0.5428571428571428\n",
      "Step :  140 Loss:  225.03680245535713  accuracy:  0.6\n",
      "Step :  140 Loss:  172.36143973214286  accuracy:  0.6642857142857143\n",
      "Step :  140 Loss:  280.99712611607146  accuracy:  0.6214285714285714\n",
      "Step :  140 Loss:  321.9943917410714  accuracy:  0.6428571428571429\n",
      "Step :  140 Loss:  139.21928013392858  accuracy:  0.6142857142857143\n",
      "Step :  140 Loss:  101.66639927455357  accuracy:  0.6857142857142857\n",
      "Tree 1 will be trained\n",
      "(1965, 200)\n",
      "Step :  320 Loss:  53.08197021484375  accuracy:  0.521875\n",
      "Step :  320 Loss:  280.012158203125  accuracy:  0.4125\n",
      "Step :  320 Loss:  342.9473388671875  accuracy:  0.4125\n",
      "Step :  320 Loss:  314.790283203125  accuracy:  0.40625\n",
      "Step :  320 Loss:  356.322216796875  accuracy:  0.4\n",
      "Step :  320 Loss:  651.3330078125  accuracy:  0.428125\n",
      "(817, 200)\n",
      "Step :  125 Loss:  64.42605078125  accuracy:  0.816\n",
      "Step :  125 Loss:  71.1263828125  accuracy:  0.528\n",
      "Step :  125 Loss:  152.744828125  accuracy:  0.712\n",
      "Step :  125 Loss:  279.38396875  accuracy:  0.656\n",
      "Step :  125 Loss:  167.910015625  accuracy:  0.68\n",
      "Step :  125 Loss:  89.279140625  accuracy:  0.736\n",
      "(1148, 200)\n",
      "Step :  195 Loss:  7.015050956530449  accuracy:  0.5230769230769231\n",
      "Step :  195 Loss:  19.52361528445513  accuracy:  0.37435897435897436\n",
      "Step :  195 Loss:  16.048968349358976  accuracy:  0.41025641025641024\n",
      "Step :  195 Loss:  24.034803185096155  accuracy:  0.46153846153846156\n",
      "Step :  195 Loss:  63.38253205128205  accuracy:  0.4307692307692308\n",
      "Step :  195 Loss:  56.909004407051285  accuracy:  0.4358974358974359\n",
      "Step :  195 Loss:  53.764337940705126  accuracy:  0.4358974358974359\n",
      "Step :  195 Loss:  92.84902844551281  accuracy:  0.49230769230769234\n",
      "(522, 200)\n",
      "Step :  85 Loss:  2.971081183938419  accuracy:  0.6\n",
      "Step :  85 Loss:  6.1637192670036764  accuracy:  0.4235294117647059\n",
      "Step :  85 Loss:  11.83831787109375  accuracy:  0.5529411764705883\n",
      "Step :  85 Loss:  14.377573529411764  accuracy:  0.4470588235294118\n",
      "Step :  85 Loss:  25.65963062959559  accuracy:  0.32941176470588235\n",
      "Step :  85 Loss:  35.82942899816177  accuracy:  0.49411764705882355\n",
      "Step :  85 Loss:  33.27926815257353  accuracy:  0.5176470588235295\n",
      "(404, 200)\n",
      "Step :  70 Loss:  3.8167894635881696  accuracy:  0.7428571428571429\n",
      "Step :  70 Loss:  2.0795739310128347  accuracy:  0.6285714285714286\n",
      "Step :  70 Loss:  0.5180787767682756  accuracy:  0.5857142857142857\n",
      "Step :  70 Loss:  2.2070630754743306  accuracy:  0.2857142857142857\n",
      "Step :  70 Loss:  1.231953866141183  accuracy:  0.6428571428571429\n",
      "Step :  70 Loss:  3.164076886858259  accuracy:  0.6714285714285714\n",
      "(353, 200)\n",
      "Step :  60 Loss:  1.8394694010416666  accuracy:  0.7\n",
      "Step :  60 Loss:  3.3383336385091145  accuracy:  0.7833333333333333\n",
      "Step :  60 Loss:  18.51700439453125  accuracy:  0.7\n",
      "Step :  60 Loss:  4.283440144856771  accuracy:  0.75\n",
      "Step :  60 Loss:  7.591634623209635  accuracy:  0.7333333333333333\n",
      "Step :  60 Loss:  9.455078125  accuracy:  0.6833333333333333\n",
      "(626, 200)\n",
      "Step :  110 Loss:  1.42943115234375  accuracy:  0.8181818181818182\n",
      "Step :  110 Loss:  1.7929518266157671  accuracy:  0.6909090909090909\n",
      "Step :  110 Loss:  6.005594149502841  accuracy:  0.6727272727272727\n",
      "Step :  110 Loss:  10.851646839488636  accuracy:  0.6181818181818182\n",
      "Step :  110 Loss:  9.39056729403409  accuracy:  0.6636363636363637\n",
      "Step :  110 Loss:  7.072057550603693  accuracy:  0.7272727272727273\n",
      "Step :  110 Loss:  1.7661279851740057  accuracy:  0.7545454545454545\n",
      "Step :  110 Loss:  1.3119451349431819  accuracy:  0.7181818181818181\n",
      "Step :  110 Loss:  3.2269287109375  accuracy:  0.6818181818181818\n",
      "Step :  110 Loss:  8.096645840731535  accuracy:  0.6727272727272727\n",
      "(86, 200)\n",
      "Step :  20 Loss:  1.843665313720703  accuracy:  0.3\n",
      "Step :  20 Loss:  0.578609561920166  accuracy:  0.7\n",
      "Step :  20 Loss:  2.0755706787109376  accuracy:  0.65\n",
      "Step :  20 Loss:  3.420557403564453  accuracy:  0.5\n",
      "Step :  20 Loss:  0.814627742767334  accuracy:  0.6\n",
      "Step :  20 Loss:  0.0  accuracy:  0.85\n",
      "Step :  20 Loss:  0.320505428314209  accuracy:  0.65\n",
      "Step :  20 Loss:  0.13862943649291992  accuracy:  0.85\n",
      "Tree 2 will be trained\n",
      "(2000, 200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :  325 Loss:  170.45561298076922  accuracy:  0.48\n",
      "Step :  325 Loss:  350.8255528846154  accuracy:  0.39384615384615385\n",
      "Step :  325 Loss:  511.2304326923077  accuracy:  0.4430769230769231\n",
      "Step :  325 Loss:  597.7634615384616  accuracy:  0.39692307692307693\n",
      "Step :  325 Loss:  813.9823076923077  accuracy:  0.4246153846153846\n",
      "Step :  325 Loss:  1065.3515384615384  accuracy:  0.41846153846153844\n",
      "Step :  325 Loss:  1710.3173076923076  accuracy:  0.4307692307692308\n",
      "Step :  325 Loss:  1755.97  accuracy:  0.38153846153846155\n",
      "(1358, 200)\n",
      "Step :  210 Loss:  25.736190941220237  accuracy:  0.638095238095238\n",
      "Step :  210 Loss:  48.65186476934524  accuracy:  0.5095238095238095\n",
      "Step :  210 Loss:  101.07299107142858  accuracy:  0.6190476190476191\n",
      "Step :  210 Loss:  74.68155691964286  accuracy:  0.5\n",
      "Step :  210 Loss:  184.72762276785716  accuracy:  0.49523809523809526\n",
      "Step :  210 Loss:  218.31385788690477  accuracy:  0.5190476190476191\n",
      "(1106, 200)\n",
      "Step :  165 Loss:  5.193255800189394  accuracy:  0.703030303030303\n",
      "Step :  165 Loss:  10.407776988636364  accuracy:  0.6787878787878788\n",
      "Step :  165 Loss:  17.819190932765153  accuracy:  0.6060606060606061\n",
      "Step :  165 Loss:  26.903497869318183  accuracy:  0.5696969696969697\n",
      "Step :  165 Loss:  70.25777107007576  accuracy:  0.6909090909090909\n",
      "Step :  165 Loss:  91.88734019886364  accuracy:  0.6363636363636364\n",
      "(414, 200)\n",
      "Step :  85 Loss:  5.431785314223346  accuracy:  0.6941176470588235\n",
      "Step :  85 Loss:  4.840735940372243  accuracy:  0.4\n",
      "Step :  85 Loss:  10.380960621553308  accuracy:  0.5411764705882353\n",
      "Step :  85 Loss:  11.919924747242646  accuracy:  0.4588235294117647\n",
      "Step :  85 Loss:  17.691378963694852  accuracy:  0.5294117647058824\n",
      "Step :  85 Loss:  19.89954762178309  accuracy:  0.47058823529411764\n",
      "Step :  85 Loss:  40.328779871323526  accuracy:  0.5647058823529412\n",
      "(304, 200)\n",
      "Step :  60 Loss:  12.648866780598958  accuracy:  0.5\n",
      "Step :  60 Loss:  17.250040690104168  accuracy:  0.4666666666666667\n",
      "Step :  60 Loss:  81.21024576822917  accuracy:  0.45\n",
      "Step :  60 Loss:  71.61785481770833  accuracy:  0.3333333333333333\n",
      "Step :  60 Loss:  52.347041829427084  accuracy:  0.5666666666666667\n",
      "Step :  60 Loss:  180.33201497395834  accuracy:  0.4166666666666667\n",
      "(236, 200)\n",
      "Step :  50 Loss:  1.844050750732422  accuracy:  0.54\n",
      "Step :  50 Loss:  5.684827880859375  accuracy:  0.6\n",
      "Step :  50 Loss:  4.790201416015625  accuracy:  0.44\n",
      "Step :  50 Loss:  11.958421630859375  accuracy:  0.26\n",
      "Step :  50 Loss:  33.61883544921875  accuracy:  0.52\n",
      "Step :  50 Loss:  34.96196044921875  accuracy:  0.52\n",
      "Step :  50 Loss:  59.13314453125  accuracy:  0.26\n",
      "Step :  50 Loss:  52.9974951171875  accuracy:  0.54\n",
      "Step :  50 Loss:  108.84833984375  accuracy:  0.48\n",
      "(185, 200)\n",
      "Step :  40 Loss:  1.1358363151550293  accuracy:  0.75\n",
      "Step :  40 Loss:  0.058076637983322146  accuracy:  0.775\n",
      "Step :  40 Loss:  1.1232582092285157  accuracy:  0.7\n",
      "Step :  40 Loss:  1.6365829467773438  accuracy:  0.725\n",
      "Step :  40 Loss:  0.12130075693130493  accuracy:  0.825\n",
      "Step :  40 Loss:  3.5445404052734375  accuracy:  0.675\n",
      "(252, 200)\n",
      "Step :  45 Loss:  2.3233016967773437  accuracy:  0.5777777777777777\n",
      "Step :  45 Loss:  5.460242716471354  accuracy:  0.4222222222222222\n",
      "Step :  45 Loss:  4.619883219401042  accuracy:  0.4444444444444444\n",
      "Step :  45 Loss:  9.067308892144098  accuracy:  0.2\n",
      "Step :  45 Loss:  8.587261284722222  accuracy:  0.5555555555555556\n",
      "Step :  45 Loss:  12.238205295138888  accuracy:  0.6444444444444445\n",
      "Step :  45 Loss:  17.68814425998264  accuracy:  0.5555555555555556\n",
      "Step :  45 Loss:  25.739664713541668  accuracy:  0.5111111111111111\n",
      "Step :  45 Loss:  9.217112901475694  accuracy:  0.6\n",
      "Step :  45 Loss:  14.763894314236111  accuracy:  0.37777777777777777\n",
      "Step :  45 Loss:  9.63695068359375  accuracy:  0.4\n",
      "(642, 200)\n",
      "Step :  115 Loss:  5.4961643384850545  accuracy:  0.7478260869565218\n",
      "Step :  115 Loss:  8.143848186990489  accuracy:  0.5565217391304348\n",
      "Step :  115 Loss:  21.10808636209239  accuracy:  0.6434782608695652\n",
      "Step :  115 Loss:  11.064501953125  accuracy:  0.591304347826087\n",
      "Step :  115 Loss:  38.15904381793478  accuracy:  0.6434782608695652\n",
      "Step :  115 Loss:  27.14140625  accuracy:  0.6173913043478261\n",
      "Step :  115 Loss:  25.595724354619566  accuracy:  0.5826086956521739\n",
      "Step :  115 Loss:  140.05344769021738  accuracy:  0.6086956521739131\n",
      "Step :  115 Loss:  61.01221127717391  accuracy:  0.6608695652173913\n",
      "Step :  115 Loss:  40.65506538722826  accuracy:  0.5739130434782609\n",
      "Step :  115 Loss:  38.0937160326087  accuracy:  0.6260869565217392\n",
      "(578, 200)\n",
      "Step :  100 Loss:  13.438603515625  accuracy:  0.12\n",
      "Step :  100 Loss:  1.6187117004394531  accuracy:  0.56\n",
      "Step :  100 Loss:  5.9509130859375  accuracy:  0.71\n",
      "Step :  100 Loss:  4.200585021972656  accuracy:  0.76\n",
      "Step :  100 Loss:  7.476343994140625  accuracy:  0.74\n",
      "Step :  100 Loss:  4.038834838867188  accuracy:  0.76\n",
      "(98, 200)\n",
      "Step :  15 Loss:  2.4304537455240887  accuracy:  0.4\n",
      "Step :  15 Loss:  0.9776901880900065  accuracy:  0.6\n",
      "Step :  15 Loss:  2.5007593790690104  accuracy:  0.3333333333333333\n",
      "Step :  15 Loss:  3.8774185180664062  accuracy:  0.26666666666666666\n",
      "Step :  15 Loss:  1.256387710571289  accuracy:  0.6\n",
      "Step :  15 Loss:  3.5507347106933596  accuracy:  0.6666666666666666\n",
      "Step :  15 Loss:  10.464815266927083  accuracy:  0.5333333333333333\n",
      "Tree 3 will be trained\n",
      "(2056, 200)\n",
      "Step :  330 Loss:  69.02978219696969  accuracy:  0.5060606060606061\n",
      "Step :  330 Loss:  320.06624053030305  accuracy:  0.396969696969697\n",
      "Step :  330 Loss:  460.2091856060606  accuracy:  0.48484848484848486\n",
      "Step :  330 Loss:  621.1857481060606  accuracy:  0.38181818181818183\n",
      "Step :  330 Loss:  608.7765151515151  accuracy:  0.45454545454545453\n",
      "Step :  330 Loss:  578.1742897727273  accuracy:  0.4121212121212121\n",
      "Step :  330 Loss:  1611.6454545454546  accuracy:  0.4\n",
      "Step :  330 Loss:  2427.0204545454544  accuracy:  0.4484848484848485\n",
      "(1285, 200)\n",
      "Step :  185 Loss:  7.356083720439189  accuracy:  0.6432432432432432\n",
      "Step :  185 Loss:  22.247579708614865  accuracy:  0.5945945945945946\n",
      "Step :  185 Loss:  16.84338840793919  accuracy:  0.4648648648648649\n",
      "Step :  185 Loss:  40.61265308277027  accuracy:  0.654054054054054\n",
      "Step :  185 Loss:  36.40680954391892  accuracy:  0.6594594594594595\n",
      "Step :  185 Loss:  16.717341902449323  accuracy:  0.518918918918919\n",
      "Step :  185 Loss:  24.173078547297298  accuracy:  0.6864864864864865\n",
      "Step :  185 Loss:  57.9712890625  accuracy:  0.6162162162162163\n",
      "Step :  185 Loss:  19.9265625  accuracy:  0.6216216216216216\n",
      "Step :  185 Loss:  42.77165857263513  accuracy:  0.5459459459459459\n",
      "Step :  185 Loss:  88.0969436233108  accuracy:  0.6162162162162163\n",
      "(1166, 200)\n",
      "Step :  170 Loss:  23.27500861672794  accuracy:  0.7529411764705882\n",
      "Step :  170 Loss:  147.27195542279412  accuracy:  0.6588235294117647\n",
      "Step :  170 Loss:  85.05923138786764  accuracy:  0.7\n",
      "Step :  170 Loss:  300.82856158088236  accuracy:  0.6\n",
      "Step :  170 Loss:  433.9068014705882  accuracy:  0.7\n",
      "Step :  170 Loss:  346.3345358455882  accuracy:  0.6823529411764706\n",
      "Step :  170 Loss:  130.6827895220588  accuracy:  0.7470588235294118\n",
      "Step :  170 Loss:  110.69099264705882  accuracy:  0.6470588235294118\n",
      "(336, 200)\n",
      "Step :  45 Loss:  8.817234293619792  accuracy:  0.4666666666666667\n",
      "Step :  45 Loss:  15.341221788194444  accuracy:  0.4\n",
      "Step :  45 Loss:  14.164942762586806  accuracy:  0.35555555555555557\n",
      "Step :  45 Loss:  25.860625542534724  accuracy:  0.4\n",
      "Step :  45 Loss:  17.9316162109375  accuracy:  0.5111111111111111\n",
      "Step :  45 Loss:  77.44776475694445  accuracy:  0.4\n",
      "Step :  45 Loss:  39.671535915798614  accuracy:  0.5777777777777777\n",
      "Step :  45 Loss:  127.29212239583333  accuracy:  0.5333333333333333\n",
      "(771, 200)\n",
      "Step :  145 Loss:  14.090145137392241  accuracy:  0.6896551724137931\n",
      "Step :  145 Loss:  30.76600552262931  accuracy:  0.5241379310344828\n",
      "Step :  145 Loss:  68.07972117456896  accuracy:  0.5793103448275863\n",
      "Step :  145 Loss:  29.86303542564655  accuracy:  0.5379310344827586\n",
      "Step :  145 Loss:  96.98296066810344  accuracy:  0.6413793103448275\n",
      "Step :  145 Loss:  105.5587957974138  accuracy:  0.5448275862068965\n",
      "(350, 200)\n",
      "Step :  85 Loss:  3.9417344037224264  accuracy:  0.43529411764705883\n",
      "Step :  85 Loss:  7.77738037109375  accuracy:  0.4117647058823529\n",
      "Step :  85 Loss:  9.3693603515625  accuracy:  0.4\n",
      "Step :  85 Loss:  23.862607709099265  accuracy:  0.4470588235294118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :  85 Loss:  11.549748678768383  accuracy:  0.4235294117647059\n",
      "Step :  85 Loss:  33.363513901654414  accuracy:  0.35294117647058826\n",
      "Step :  85 Loss:  11.490744916130515  accuracy:  0.4470588235294118\n",
      "Step :  85 Loss:  25.259869025735295  accuracy:  0.5294117647058824\n",
      "Step :  85 Loss:  32.21953986672794  accuracy:  0.49411764705882355\n",
      "Step :  85 Loss:  32.3012264476103  accuracy:  0.5058823529411764\n",
      "Step :  85 Loss:  23.34893296185662  accuracy:  0.5529411764705883\n",
      "Step :  85 Loss:  19.200245576746322  accuracy:  0.4470588235294118\n",
      "(269, 200)\n",
      "Step :  65 Loss:  3.3407419057992787  accuracy:  0.6153846153846154\n",
      "Step :  65 Loss:  9.382569298377405  accuracy:  0.6461538461538462\n",
      "Step :  65 Loss:  7.5535137469951925  accuracy:  0.35384615384615387\n",
      "Step :  65 Loss:  12.618706805889422  accuracy:  0.6\n",
      "Step :  65 Loss:  11.42743389423077  accuracy:  0.6153846153846154\n",
      "Step :  65 Loss:  7.364553598257212  accuracy:  0.6461538461538462\n",
      "Step :  65 Loss:  4.057443002554087  accuracy:  0.6153846153846154\n",
      "Step :  65 Loss:  3.5431612454927883  accuracy:  0.5846153846153846\n",
      "(92, 200)\n",
      "Step :  25 Loss:  3.3946160888671875  accuracy:  0.36\n",
      "Step :  25 Loss:  1.6085684204101562  accuracy:  0.52\n",
      "Step :  25 Loss:  4.266392211914063  accuracy:  0.36\n",
      "Step :  25 Loss:  0.4058391189575195  accuracy:  0.72\n",
      "Step :  25 Loss:  0.46817306518554686  accuracy:  0.52\n",
      "Step :  25 Loss:  1.7558076477050781  accuracy:  0.44\n",
      "Step :  25 Loss:  0.6185239791870117  accuracy:  0.68\n",
      "Tree 4 will be trained\n",
      "(1951, 200)\n",
      "Step :  320 Loss:  35.75882263183594  accuracy:  0.5\n",
      "Step :  320 Loss:  67.80618286132812  accuracy:  0.453125\n",
      "Step :  320 Loss:  196.39232177734374  accuracy:  0.378125\n",
      "Step :  320 Loss:  261.703564453125  accuracy:  0.371875\n",
      "Step :  320 Loss:  268.5170166015625  accuracy:  0.496875\n",
      "Step :  320 Loss:  156.38896484375  accuracy:  0.428125\n",
      "Step :  320 Loss:  323.798876953125  accuracy:  0.39375\n",
      "Step :  320 Loss:  390.836962890625  accuracy:  0.459375\n",
      "Step :  320 Loss:  756.426416015625  accuracy:  0.41875\n",
      "Step :  320 Loss:  654.26259765625  accuracy:  0.43125\n",
      "Step :  320 Loss:  789.066650390625  accuracy:  0.475\n",
      "Step :  320 Loss:  749.375  accuracy:  0.4875\n",
      "Step :  320 Loss:  1001.49599609375  accuracy:  0.484375\n",
      "Step :  320 Loss:  746.053759765625  accuracy:  0.425\n",
      "Step :  320 Loss:  1041.09970703125  accuracy:  0.540625\n",
      "Step :  320 Loss:  882.22841796875  accuracy:  0.515625\n",
      "(1200, 200)\n",
      "Step :  185 Loss:  28.09549197635135  accuracy:  0.772972972972973\n",
      "Step :  185 Loss:  66.93936338682433  accuracy:  0.6162162162162163\n",
      "Step :  185 Loss:  63.219298986486486  accuracy:  0.7243243243243244\n",
      "Step :  185 Loss:  48.193570523648646  accuracy:  0.4972972972972973\n",
      "Step :  185 Loss:  60.34712837837838  accuracy:  0.7027027027027027\n",
      "Step :  185 Loss:  92.06221494932433  accuracy:  0.7135135135135136\n",
      "(588, 200)\n",
      "Step :  115 Loss:  12.673684825067935  accuracy:  0.7478260869565218\n",
      "Step :  115 Loss:  21.184859035326088  accuracy:  0.4434782608695652\n",
      "Step :  115 Loss:  138.06567595108694  accuracy:  0.5739130434782609\n",
      "Step :  115 Loss:  84.06073369565217  accuracy:  0.4608695652173913\n",
      "Step :  115 Loss:  97.93376358695652  accuracy:  0.5217391304347826\n",
      "Step :  115 Loss:  185.1452785326087  accuracy:  0.6347826086956522\n",
      "(540, 200)\n",
      "Step :  105 Loss:  4.578065708705357  accuracy:  0.5428571428571428\n",
      "Step :  105 Loss:  7.685066150483631  accuracy:  0.6571428571428571\n",
      "Step :  105 Loss:  14.17202380952381  accuracy:  0.6476190476190476\n",
      "Step :  105 Loss:  7.414453125  accuracy:  0.4380952380952381\n",
      "Step :  105 Loss:  10.447255161830357  accuracy:  0.6476190476190476\n",
      "Step :  105 Loss:  15.073936244419643  accuracy:  0.7047619047619048\n",
      "Step :  105 Loss:  23.10531063988095  accuracy:  0.6761904761904762\n",
      "Step :  105 Loss:  15.316560872395833  accuracy:  0.6857142857142857\n",
      "Step :  105 Loss:  13.945131138392858  accuracy:  0.6761904761904762\n",
      "(249, 200)\n",
      "Step :  45 Loss:  4.544902208116319  accuracy:  0.4222222222222222\n",
      "Step :  45 Loss:  9.551288519965278  accuracy:  0.4\n",
      "Step :  45 Loss:  23.46848415798611  accuracy:  0.4666666666666667\n",
      "Step :  45 Loss:  19.001557074652776  accuracy:  0.4666666666666667\n",
      "Step :  45 Loss:  28.94057074652778  accuracy:  0.6\n",
      "Step :  45 Loss:  100.30141059027778  accuracy:  0.5555555555555556\n",
      "(132, 200)\n",
      "Step :  20 Loss:  0.8242348670959473  accuracy:  0.75\n",
      "Step :  20 Loss:  0.33900985717773435  accuracy:  0.65\n",
      "Step :  20 Loss:  0.8267668724060059  accuracy:  0.4\n",
      "Step :  20 Loss:  0.29063217639923095  accuracy:  0.6\n",
      "Step :  20 Loss:  0.3279138088226318  accuracy:  0.7\n",
      "Step :  20 Loss:  1.3889152526855468  accuracy:  0.55\n",
      "(751, 200)\n",
      "Step :  135 Loss:  3.2731736924913193  accuracy:  0.8444444444444444\n",
      "Step :  135 Loss:  2.5021095558449074  accuracy:  0.6074074074074074\n",
      "Step :  135 Loss:  0.47894942672164353  accuracy:  0.6888888888888889\n",
      "Step :  135 Loss:  0.5450927169234664  accuracy:  0.6592592592592592\n",
      "Step :  135 Loss:  0.9686704282407408  accuracy:  0.6370370370370371\n",
      "Step :  135 Loss:  0.12292805424442997  accuracy:  0.7037037037037037\n",
      "(143, 200)\n",
      "Step :  40 Loss:  16.48835144042969  accuracy:  0.5\n",
      "Step :  40 Loss:  10.7745361328125  accuracy:  0.425\n",
      "Step :  40 Loss:  31.55947265625  accuracy:  0.5\n",
      "Step :  40 Loss:  41.803652954101565  accuracy:  0.425\n",
      "Step :  40 Loss:  10.545968627929687  accuracy:  0.45\n",
      "Step :  40 Loss:  41.026922607421874  accuracy:  0.25\n",
      "Step :  40 Loss:  33.44247131347656  accuracy:  0.4\n",
      "Step :  40 Loss:  11.593354034423829  accuracy:  0.425\n",
      "Step :  40 Loss:  50.21815795898438  accuracy:  0.45\n",
      "Step :  40 Loss:  51.1337158203125  accuracy:  0.425\n",
      "Step :  40 Loss:  10.353548431396485  accuracy:  0.5\n",
      "Tree 5 will be trained\n",
      "(1971, 200)\n",
      "Step :  330 Loss:  79.45991358901514  accuracy:  0.4727272727272727\n",
      "Step :  330 Loss:  239.64701704545453  accuracy:  0.4303030303030303\n",
      "Step :  330 Loss:  303.26453598484846  accuracy:  0.4636363636363636\n",
      "Step :  330 Loss:  650.4130208333333  accuracy:  0.39090909090909093\n",
      "Step :  330 Loss:  829.1928977272727  accuracy:  0.396969696969697\n",
      "Step :  330 Loss:  1052.1151515151514  accuracy:  0.403030303030303\n",
      "Step :  330 Loss:  1621.501893939394  accuracy:  0.42424242424242425\n",
      "Step :  330 Loss:  2461.9651515151513  accuracy:  0.43636363636363634\n",
      "Step :  330 Loss:  2515.110227272727  accuracy:  0.4303030303030303\n",
      "Step :  330 Loss:  2110.649621212121  accuracy:  0.5121212121212121\n",
      "Step :  330 Loss:  1724.8030303030303  accuracy:  0.503030303030303\n",
      "Step :  330 Loss:  2429.784659090909  accuracy:  0.49696969696969695\n",
      "(1001, 200)\n",
      "Step :  160 Loss:  308.2262939453125  accuracy:  0.56875\n",
      "Step :  160 Loss:  555.754150390625  accuracy:  0.6375\n",
      "Step :  160 Loss:  293.935546875  accuracy:  0.5\n",
      "Step :  160 Loss:  291.4575927734375  accuracy:  0.45625\n",
      "Step :  160 Loss:  623.51025390625  accuracy:  0.6375\n",
      "Step :  160 Loss:  495.910302734375  accuracy:  0.66875\n",
      "(811, 200)\n",
      "Step :  130 Loss:  13.8175048828125  accuracy:  0.8461538461538461\n",
      "Step :  130 Loss:  19.191302959735577  accuracy:  0.7615384615384615\n",
      "Step :  130 Loss:  40.886057692307695  accuracy:  0.7153846153846154\n",
      "Step :  130 Loss:  27.10484149639423  accuracy:  0.5615384615384615\n",
      "Step :  130 Loss:  39.975030048076924  accuracy:  0.7307692307692307\n",
      "Step :  130 Loss:  7.172989126352164  accuracy:  0.7846153846153846\n",
      "Step :  130 Loss:  15.403650841346154  accuracy:  0.7615384615384615\n",
      "Step :  130 Loss:  1.3939115084134615  accuracy:  0.7846153846153846\n",
      "Step :  130 Loss:  5.0149733323317305  accuracy:  0.7461538461538462\n",
      "Step :  130 Loss:  1.3851138775165264  accuracy:  0.8\n",
      "Step :  130 Loss:  0.0373233098250169  accuracy:  0.8076923076923077\n",
      "Step :  130 Loss:  0.07464662698599009  accuracy:  0.8076923076923077\n",
      "(190, 200)\n",
      "Step :  30 Loss:  8.347640991210938  accuracy:  0.26666666666666666\n",
      "Step :  30 Loss:  1.7777952829996744  accuracy:  0.26666666666666666\n",
      "Step :  30 Loss:  1.3945912679036458  accuracy:  0.5\n",
      "Step :  30 Loss:  2.017757288614909  accuracy:  0.6333333333333333\n",
      "Step :  30 Loss:  7.235832722981771  accuracy:  0.36666666666666664\n",
      "Step :  30 Loss:  6.223343912760416  accuracy:  0.4666666666666667\n",
      "Step :  30 Loss:  3.9078577677408854  accuracy:  0.6\n",
      "Step :  30 Loss:  3.57652587890625  accuracy:  0.5666666666666667\n",
      "Step :  30 Loss:  3.8583465576171876  accuracy:  0.43333333333333335\n",
      "Step :  30 Loss:  0.7637278874715169  accuracy:  0.7\n",
      "Step :  30 Loss:  1.4901008605957031  accuracy:  0.3\n",
      "Step :  30 Loss:  5.061348978678385  accuracy:  0.2\n",
      "(87, 200)\n",
      "Step :  15 Loss:  10.069594319661459  accuracy:  0.13333333333333333\n",
      "Step :  15 Loss:  1.1089627583821615  accuracy:  0.5333333333333333\n",
      "Step :  15 Loss:  2.496868387858073  accuracy:  0.6\n",
      "Step :  15 Loss:  4.482291158040365  accuracy:  0.6\n",
      "Step :  15 Loss:  7.7291015625  accuracy:  0.5333333333333333\n",
      "Step :  15 Loss:  15.016282145182291  accuracy:  0.3333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(970, 200)\n",
      "Step :  170 Loss:  13.661635454963236  accuracy:  0.6176470588235294\n",
      "Step :  170 Loss:  42.01692899816177  accuracy:  0.4823529411764706\n",
      "Step :  170 Loss:  82.62868221507352  accuracy:  0.5882352941176471\n",
      "Step :  170 Loss:  101.03082490808823  accuracy:  0.36470588235294116\n",
      "Step :  170 Loss:  120.33732766544118  accuracy:  0.6705882352941176\n",
      "Step :  170 Loss:  153.9894646139706  accuracy:  0.6\n",
      "Step :  170 Loss:  109.45990349264706  accuracy:  0.4294117647058823\n",
      "Step :  170 Loss:  256.21447610294115  accuracy:  0.5529411764705883\n",
      "(713, 200)\n",
      "Step :  105 Loss:  3.3081034342447917  accuracy:  0.5142857142857142\n",
      "Step :  105 Loss:  1.3357894170851934  accuracy:  0.7523809523809524\n",
      "Step :  105 Loss:  3.0261640276227677  accuracy:  0.7142857142857143\n",
      "Step :  105 Loss:  3.480189732142857  accuracy:  0.7238095238095238\n",
      "Step :  105 Loss:  1.5772472563244047  accuracy:  0.7619047619047619\n",
      "Step :  105 Loss:  2.5883722214471727  accuracy:  0.7619047619047619\n",
      "Step :  105 Loss:  2.9428397042410714  accuracy:  0.7047619047619048\n",
      "Step :  105 Loss:  6.065568033854166  accuracy:  0.6952380952380952\n",
      "Step :  105 Loss:  3.218536667596726  accuracy:  0.7333333333333333\n",
      "(285, 200)\n",
      "Step :  40 Loss:  0.8889500617980957  accuracy:  0.575\n",
      "Step :  40 Loss:  5.639378356933594  accuracy:  0.625\n",
      "Step :  40 Loss:  3.4951854705810548  accuracy:  0.55\n",
      "Step :  40 Loss:  11.390700531005859  accuracy:  0.25\n",
      "Step :  40 Loss:  12.483918762207031  accuracy:  0.55\n",
      "Step :  40 Loss:  13.858816528320313  accuracy:  0.7\n",
      "Step :  40 Loss:  9.837527465820312  accuracy:  0.55\n",
      "Step :  40 Loss:  27.045745849609375  accuracy:  0.55\n",
      "Step :  40 Loss:  5.62482795715332  accuracy:  0.6\n",
      "Step :  40 Loss:  22.09418640136719  accuracy:  0.25\n",
      "Step :  40 Loss:  2.9202836990356444  accuracy:  0.55\n",
      "Step :  40 Loss:  12.714845275878906  accuracy:  0.575\n",
      "Step :  40 Loss:  44.908016967773435  accuracy:  0.6\n",
      "Step :  40 Loss:  26.82532653808594  accuracy:  0.575\n",
      "(94, 200)\n",
      "Step :  15 Loss:  1.772253163655599  accuracy:  0.6666666666666666\n",
      "Step :  15 Loss:  1.0563656489054363  accuracy:  0.3333333333333333\n",
      "Step :  15 Loss:  1.2174967447916667  accuracy:  0.3333333333333333\n",
      "Step :  15 Loss:  4.086112721761068  accuracy:  0.5333333333333333\n",
      "Step :  15 Loss:  4.354889933268229  accuracy:  0.5333333333333333\n",
      "Step :  15 Loss:  0.1662955125172933  accuracy:  0.6666666666666666\n",
      "Step :  15 Loss:  4.383250935872396  accuracy:  0.5333333333333333\n",
      "(257, 200)\n",
      "Step :  65 Loss:  9.646917255108173  accuracy:  0.5230769230769231\n",
      "Step :  65 Loss:  20.4337646484375  accuracy:  0.36923076923076925\n",
      "Step :  65 Loss:  17.941368689903847  accuracy:  0.47692307692307695\n",
      "Step :  65 Loss:  55.77914663461539  accuracy:  0.47692307692307695\n",
      "Step :  65 Loss:  36.71943359375  accuracy:  0.38461538461538464\n",
      "Step :  65 Loss:  96.59989483173077  accuracy:  0.36923076923076925\n",
      "Step :  65 Loss:  59.77491736778846  accuracy:  0.5230769230769231\n",
      "Step :  65 Loss:  174.85166766826924  accuracy:  0.38461538461538464\n",
      "Step :  65 Loss:  45.14907602163461  accuracy:  0.4461538461538462\n",
      "Step :  65 Loss:  192.3074669471154  accuracy:  0.4\n",
      "Step :  65 Loss:  108.2613055889423  accuracy:  0.38461538461538464\n",
      "Step :  65 Loss:  129.70286959134614  accuracy:  0.4\n",
      "Step :  65 Loss:  115.1669696514423  accuracy:  0.5076923076923077\n",
      "Step :  65 Loss:  74.31561748798077  accuracy:  0.5076923076923077\n",
      "Step :  65 Loss:  271.11147836538464  accuracy:  0.4461538461538462\n",
      "Step :  65 Loss:  157.1689453125  accuracy:  0.36923076923076925\n",
      "Step :  65 Loss:  76.30649038461539  accuracy:  0.5538461538461539\n",
      "Step :  65 Loss:  202.94191706730768  accuracy:  0.49230769230769234\n",
      "Tree 6 will be trained\n",
      "(2036, 200)\n",
      "Step :  325 Loss:  5.429022310697116  accuracy:  0.48\n",
      "Step :  325 Loss:  16.16453125  accuracy:  0.37538461538461537\n",
      "Step :  325 Loss:  27.705652043269232  accuracy:  0.36615384615384616\n",
      "Step :  325 Loss:  36.62255108173077  accuracy:  0.4307692307692308\n",
      "Step :  325 Loss:  36.105456730769234  accuracy:  0.42153846153846153\n",
      "Step :  325 Loss:  45.68264723557692  accuracy:  0.39076923076923076\n",
      "Step :  325 Loss:  57.19425480769231  accuracy:  0.40923076923076923\n",
      "Step :  325 Loss:  86.817890625  accuracy:  0.37846153846153846\n",
      "(1177, 200)\n",
      "Step :  175 Loss:  17.97363978794643  accuracy:  0.7028571428571428\n",
      "Step :  175 Loss:  47.905066964285716  accuracy:  0.68\n",
      "Step :  175 Loss:  88.42853794642858  accuracy:  0.5885714285714285\n",
      "Step :  175 Loss:  232.38357142857143  accuracy:  0.64\n",
      "Step :  175 Loss:  98.36879464285714  accuracy:  0.5142857142857142\n",
      "Step :  175 Loss:  195.3103125  accuracy:  0.6742857142857143\n",
      "Step :  175 Loss:  326.54888392857146  accuracy:  0.6685714285714286\n",
      "(515, 200)\n",
      "Step :  100 Loss:  5.697033081054688  accuracy:  0.8\n",
      "Step :  100 Loss:  9.8369873046875  accuracy:  0.62\n",
      "Step :  100 Loss:  10.029816284179688  accuracy:  0.61\n",
      "Step :  100 Loss:  8.229617919921875  accuracy:  0.59\n",
      "Step :  100 Loss:  11.502896728515625  accuracy:  0.53\n",
      "Step :  100 Loss:  8.38695068359375  accuracy:  0.61\n",
      "(240, 200)\n",
      "Step :  35 Loss:  1.082351793561663  accuracy:  0.4857142857142857\n",
      "Step :  35 Loss:  0.2560187203543527  accuracy:  0.7142857142857143\n",
      "Step :  35 Loss:  1.3307551792689731  accuracy:  0.5714285714285714\n",
      "Step :  35 Loss:  4.364412580217634  accuracy:  0.37142857142857144\n",
      "Step :  35 Loss:  5.468495832170759  accuracy:  0.45714285714285713\n",
      "Step :  35 Loss:  5.033831787109375  accuracy:  0.6571428571428571\n",
      "Step :  35 Loss:  6.76189226422991  accuracy:  0.5142857142857142\n",
      "(167, 200)\n",
      "Step :  25 Loss:  1.5056524658203125  accuracy:  0.56\n",
      "Step :  25 Loss:  6.670360717773438  accuracy:  0.4\n",
      "Step :  25 Loss:  15.68860107421875  accuracy:  0.36\n",
      "Step :  25 Loss:  12.079149169921875  accuracy:  0.4\n",
      "Step :  25 Loss:  16.33970703125  accuracy:  0.56\n",
      "Step :  25 Loss:  27.94725830078125  accuracy:  0.52\n",
      "Step :  25 Loss:  38.2966552734375  accuracy:  0.52\n",
      "Step :  25 Loss:  14.56464599609375  accuracy:  0.64\n",
      "(275, 200)\n",
      "Step :  65 Loss:  8.614839993990385  accuracy:  0.7846153846153846\n",
      "Step :  65 Loss:  7.56435546875  accuracy:  0.7384615384615385\n",
      "Step :  65 Loss:  0.1240926008958083  accuracy:  0.7846153846153846\n",
      "Step :  65 Loss:  1.0492181631234976  accuracy:  0.7846153846153846\n",
      "Step :  65 Loss:  1.8171546349158654  accuracy:  0.7538461538461538\n",
      "Step :  65 Loss:  1.5991281362680287  accuracy:  0.7846153846153846\n",
      "Step :  65 Loss:  0.02132760561429537  accuracy:  0.8153846153846154\n",
      "Step :  65 Loss:  0.05331901403573843  accuracy:  0.8153846153846154\n",
      "(859, 200)\n",
      "Step :  150 Loss:  3.331429443359375  accuracy:  0.62\n",
      "Step :  150 Loss:  10.313255208333333  accuracy:  0.6533333333333333\n",
      "Step :  150 Loss:  5.884082845052084  accuracy:  0.54\n",
      "Step :  150 Loss:  20.3085205078125  accuracy:  0.68\n",
      "Step :  150 Loss:  19.98223795572917  accuracy:  0.62\n",
      "Step :  150 Loss:  18.082755533854165  accuracy:  0.44666666666666666\n",
      "Step :  150 Loss:  32.30753255208333  accuracy:  0.6066666666666667\n",
      "Step :  150 Loss:  32.807545572916666  accuracy:  0.6266666666666667\n",
      "Step :  150 Loss:  22.73919759114583  accuracy:  0.6266666666666667\n",
      "Step :  150 Loss:  8.600703938802083  accuracy:  0.66\n",
      "(708, 200)\n",
      "Step :  110 Loss:  0.9820665532892401  accuracy:  0.7454545454545455\n",
      "Step :  110 Loss:  2.3930630770596593  accuracy:  0.7090909090909091\n",
      "Step :  110 Loss:  2.821009687943892  accuracy:  0.7454545454545455\n",
      "Step :  110 Loss:  2.11043437610973  accuracy:  0.5363636363636364\n",
      "Step :  110 Loss:  1.62578125  accuracy:  0.7454545454545455\n",
      "Step :  110 Loss:  2.0229109330610795  accuracy:  0.7363636363636363\n",
      "Step :  110 Loss:  9.420893998579546  accuracy:  0.7636363636363637\n",
      "Step :  110 Loss:  9.022596324573863  accuracy:  0.7181818181818181\n",
      "Step :  110 Loss:  3.043091652610085  accuracy:  0.7909090909090909\n",
      "Step :  110 Loss:  9.429566539417614  accuracy:  0.7454545454545455\n",
      "(151, 200)\n",
      "Step :  40 Loss:  2.7920087814331054  accuracy:  0.475\n",
      "Step :  40 Loss:  8.02216796875  accuracy:  0.375\n",
      "Step :  40 Loss:  14.146385192871094  accuracy:  0.3\n",
      "Step :  40 Loss:  13.111294555664063  accuracy:  0.45\n",
      "Step :  40 Loss:  9.006449890136718  accuracy:  0.425\n",
      "Step :  40 Loss:  23.444194030761718  accuracy:  0.425\n",
      "Step :  40 Loss:  13.801490783691406  accuracy:  0.45\n",
      "Step :  40 Loss:  41.93551940917969  accuracy:  0.425\n",
      "Tree 7 will be trained\n",
      "(1966, 200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :  320 Loss:  40.542486572265624  accuracy:  0.521875\n",
      "Step :  320 Loss:  122.6745849609375  accuracy:  0.43125\n",
      "Step :  320 Loss:  203.7013671875  accuracy:  0.45\n",
      "Step :  320 Loss:  434.82431640625  accuracy:  0.403125\n",
      "Step :  320 Loss:  611.470068359375  accuracy:  0.403125\n",
      "Step :  320 Loss:  595.2  accuracy:  0.44375\n",
      "(1131, 200)\n",
      "Step :  180 Loss:  17.485247124565973  accuracy:  0.8277777777777777\n",
      "Step :  180 Loss:  67.39214952256944  accuracy:  0.5944444444444444\n",
      "Step :  180 Loss:  106.17329644097222  accuracy:  0.7055555555555556\n",
      "Step :  180 Loss:  76.57671983506944  accuracy:  0.5722222222222222\n",
      "Step :  180 Loss:  133.00425347222222  accuracy:  0.7388888888888889\n",
      "Step :  180 Loss:  160.6703884548611  accuracy:  0.7666666666666667\n",
      "Step :  180 Loss:  281.07745225694447  accuracy:  0.7222222222222222\n",
      "Step :  180 Loss:  249.9734375  accuracy:  0.7166666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e930d8991824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                 use_prototype_learner = True)\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1f81afa85ca6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mearly_stopping_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             )\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-56c44347df8b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-56c44347df8b>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# splitting recursevely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-56c44347df8b>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mfeatures_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_via_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# calculating current split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-56c44347df8b>\u001b[0m in \u001b[0;36mprototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_prototype_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mprototypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-54ae0b2c992f>\u001b[0m in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mmin_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0moptim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def generate_random(lower, upper):\n",
    "    random_number = random.random()\n",
    "    random_number = random_number + lower\n",
    "    random_range = upper - lower\n",
    "    random_number = random_number*random_range\n",
    "    return random_number\n",
    "\n",
    "all_accuracy = []\n",
    "parameters = [[0.00001, 0.05], [0.00001, 0.05],[0.00001, 0.05], [0.00001, 0.05], [0.00001, 0.05], [1],[1]]\n",
    "\n",
    "for i in range(3,6):\n",
    "    for j in range(5, 11):\n",
    "        print(f\"Rep {i}, fold {j}\")\n",
    "        (train_features,\n",
    "             train_labels,\n",
    "             train_bag_ids,\n",
    "             test_features,\n",
    "             test_labels,\n",
    "             test_bag_ids) = train_test_split(dataset, i, j, 1, fit_on_full = False)\n",
    "        \n",
    "        model = PrototypeForest(size=best_size,\n",
    "                                max_depth=best_depth,\n",
    "                                min_samples_leaf=40,\n",
    "                                min_samples_split=80,\n",
    "                                prototype_count=1,\n",
    "                                early_stopping_round= 5,\n",
    "                                use_prototype_learner = True)\n",
    "        \n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "        probas = model.predict_proba(test_features, test_bag_ids)\n",
    "\n",
    "        score = metrics.roc_auc_score(test_labels, probas)\n",
    "        print(f\"Score is {score}\")\n",
    "        all_accuracy.append(metrics.roc_auc_score(test_labels, probas))\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5513.336589336395"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.408230543136597"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9504685408299867,\n",
       " 0.9554469273743017,\n",
       " 0.6169175627240143,\n",
       " 0.8965827338129496]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235852664357629"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_accuracy)/len(all_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9351851851851852\n",
      "0.8095238095238095\n",
      "0.9097222222222222\n",
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Musk1\"\n",
    "\n",
    "(train_features, \n",
    "     train_labels, \n",
    "     train_bag_ids,\n",
    "     test_features, \n",
    "     test_labels, \n",
    "     test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = False)\n",
    "\n",
    "for i in range(0, 2):\n",
    "    model = PrototypeForest(\n",
    "        size = 100, \n",
    "        max_depth = 8, \n",
    "        min_samples_leaf= 40, \n",
    "        min_samples_split= 80, \n",
    "        feature_types=[\"min\", \"max\", \"mean\"]\n",
    "    )\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "    preds = model.predict(test_features, test_bag_ids)\n",
    "    probas = model.predict_proba(test_features, test_bag_ids)\n",
    "    print(metrics.roc_auc_score(test_labels, probas))\n",
    "    print(metrics.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-12bff3dfbaec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                           \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                           \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                           \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"explained_variance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                                           fit_on_full = False)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "rep = 5\n",
    "nofCVfold = 10\n",
    "dataset = \"BrownCreeper\"\n",
    "accuracy = []\n",
    "\n",
    "#param_scores = get_parameter_scores(train_features, train_labels, train_bag_ids, params)\n",
    "#best_params = dict(max(param_scores, key=param_scores.get))\n",
    "\n",
    "for rep in range(1, 2):\n",
    "    for fold in range(1, 11):\n",
    "        (train_features, \n",
    "         train_labels, \n",
    "         train_bag_ids,\n",
    "         test_features, \n",
    "         test_labels, \n",
    "         test_bag_ids) = train_test_split(dataset, \n",
    "                                          rep, \n",
    "                                          fold, \n",
    "                                          best_params[\"explained_variance\"], \n",
    "                                          fit_on_full = False)\n",
    "        \n",
    "        model = PrototypeForest(size = 15, \n",
    "                                max_depth = best_params[\"max_depth\"], \n",
    "                                min_samples_leaf=best_params[\"min_samples_leaf\"], \n",
    "                                min_samples_split=2,\n",
    "                                prototype_count=best_params[\"nof_prototypes\"])\n",
    "        \n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "        preds = model.predict(test_features, test_bag_ids)\n",
    "        accuracy.append(metrics.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 5\n",
    "nofCVfold = 10\n",
    "dataset = \"BrownCreeper\"\n",
    "accuracy = []\n",
    "\n",
    "#param_scores = get_parameter_scores(train_features, train_labels, train_bag_ids, params)\n",
    "#best_params = dict(max(param_scores, key=param_scores.get))\n",
    "\n",
    "for rep in range(1, 2):\n",
    "    for fold in range(1, 11):\n",
    "        (train_features, \n",
    "         train_labels, \n",
    "         train_bag_ids,\n",
    "         test_features, \n",
    "         test_labels, \n",
    "         test_bag_ids) = train_test_split(dataset, \n",
    "                                          rep, \n",
    "                                          fold, \n",
    "                                          1, \n",
    "                                          fit_on_full = False)\n",
    "        \n",
    "        model = PrototypeForest(size = 100, \n",
    "                                max_depth = 3, \n",
    "                                min_samples_leaf=3, \n",
    "                                min_samples_split=2,\n",
    "                                prototype_count=3)\n",
    "        \n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "        preds = model.predict(test_features, test_bag_ids)\n",
    "        accuracy.append(metrics.accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8384378229864422"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdc = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  max_depth=2, \n",
    "                                  criterion=\"entropy\",\n",
    "                                  min_samples_split=2)\n",
    "\n",
    "bdc.fit(features, labels)\n",
    "for i in range(0, 5):\n",
    "    bdc.fit(train_features, train_labels)\n",
    "    predictions = bdc.predict(test_features)\n",
    "    print(metrics.accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674985538729156\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    bdc = RandomForestClassifier(n_estimators=15,\n",
    "                                 random_state=0, \n",
    "                                 max_depth=2, \n",
    "                                 criterion=\"entropy\",\n",
    "                                 min_samples_split=2)\n",
    "    bdc.fit(train_features, train_labels)\n",
    "    preds = bdc.predict_proba(test_features)\n",
    "    print(metrics.roc_auc_score(test_labels, preds[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdc = RandomForestClassifier(n_estimators=15,\n",
    "                             random_state=0, \n",
    "                             max_depth=2, \n",
    "                             criterion=\"entropy\",\n",
    "                             min_samples_split=2)\n",
    "accuracy = []\n",
    "\n",
    "for rep in range(1, 2):\n",
    "    for fold in range(1, 11):\n",
    "        (train_features, \n",
    "         train_labels, \n",
    "         train_bag_ids,\n",
    "         test_features, \n",
    "         test_labels, \n",
    "         test_bag_ids) = train_test_split(dataset, rep, fold, 0.95, fit_on_full = False)\n",
    "        \n",
    "        bdc.fit(train_features, train_labels)\n",
    "        predictions = bdc.predict(test_features)\n",
    "        accuracy.append(metrics.accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3\n",
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
