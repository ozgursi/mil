{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e21de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37c458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "        \n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self,\n",
    "                train_features,\n",
    "                 feature_types = [\"min\", \"max\", \"mean\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner=True,\n",
    "                 early_stopping_round = 3):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.train_features = train_features\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        number_of_rows = features.shape[0]\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                          size=prototype_count, \n",
    "                                          replace=False)\n",
    "\n",
    "        prot = features[random_indices, :]\n",
    "        if len(prot.shape) == 1:\n",
    "            prot = prot.reshape(1, prot.shape[0])\n",
    "        return prot\n",
    "\n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        return np.asarray(np.sum(y)/y.size)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "        \n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        _, index  = np.unique(bag_ids, return_index=True)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_mean = np.add.reduceat(distances[:, i], index)\n",
    "                mean_vals = np.repeat(group_mean/bin_count, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "        \n",
    "        return np.array(np.transpose(feature_list))\n",
    "\n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        \n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_via_prototype, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        \n",
    "        bdc = tree.DecisionTreeRegressor(\n",
    "            min_samples_split=2,\n",
    "            criterion=\"mae\"\n",
    "        )\n",
    "        bdc.fit(features_via_prototype[index], labels[index])\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        labels_left = labels[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "\n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids)) < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            \n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "            \n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids_left)) < self.min_samples_leaf or len(np.unique(bag_ids_right)) < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "            \n",
    "            _, index_left  = np.unique(bag_ids_left, return_index=True)\n",
    "            _, index_right  = np.unique(bag_ids_right, return_index=True)\n",
    "            \n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left[index_left])\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right[index_right])\n",
    "\n",
    "            # splitting recursively\n",
    "            \n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "\n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "\n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "\n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "\n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "\n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree)\n",
    "            \n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)\n",
    "\n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"],\n",
    "                max_depth = 8, \n",
    "                min_samples_leaf = 2, \n",
    "                min_samples_split = 2, \n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        group_min = np.minimum.reduceat(labels, index)\n",
    "        bag_size = math.ceil(group_min.shape[0] * 0.8)\n",
    "        bags_all = np.random.choice(ids, bag_size, replace=False)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1),\n",
    "                                          train_labels.reshape(train_labels.shape[0],1)],\n",
    "                                         axis=1))\n",
    "        \n",
    "        indices_all = df[df[0].isin(bags_all)].index.to_numpy()\n",
    "        inbag_indices = indices_all\n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "        \n",
    "        return inbag_indices, outbag_indices\n",
    "\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            \n",
    "            (inbag_indices, _) = self.sample(features, labels, bag_ids)\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            while tree.Tree.right is None:\n",
    "                tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            self._trees.append(tree)\n",
    "            \n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return np.sum(preds, axis=1)/self.size\n",
    "   \n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0, 1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "\n",
    "    #sort_index = np.argsort(bag_ids)\n",
    "    #bag_ids = bag_ids[sort_index]\n",
    "    #features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "\n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "          \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    \n",
    "    #for i in range(2, 94):\n",
    "    #    clean_data = train_data[(train_data[i] != 0) & (train_data[i] != -32767)]\n",
    "    #    mean = clean_data[i].mean()\n",
    "    #    train_data[(train_data[i] == 0) | (train_data[i] == -32767)] = mean\n",
    "\n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9c3ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-4fb104074b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             early_stopping_round= 5)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mearly_stopping_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             )\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# splitting recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# splitting recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    165\u001b[0m              \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m              \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m              \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                                                  \u001b[0mfeatures_updated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2428ac363f44>\u001b[0m in \u001b[0;36mcalcBestSplit\u001b[0;34m(self, features, features_via_prototype, labels, bag_ids)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mbdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_via_prototype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \"\"\"\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1253\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"WheatYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=100,\n",
    "                            max_depth=8,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "#perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "#perf_df.to_csv(f\"./performance/performance_{dataset}.csv\")\n",
    "\n",
    "#all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "#all_df.to_csv(f\"./performance/predictions_{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b782bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.38023899640732683]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1604d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets_regression/cv/CornYields.csv_rep1_fold1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-92c4697f06d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m          \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m          \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m          test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     model = PrototypeForest(size=100,\n",
      "\u001b[0;32m<ipython-input-22-ba21a5cb1a6c>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(dataset, rep, fold, explained_variance, fit_on_full, custom, cols)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m#testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/{dataset}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtestbags\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets_regression/cv/CornYields.csv_rep1_fold1.txt'"
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"CornYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=100,\n",
    "                            max_depth=8,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_{dataset}.csv\")\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "all_df.to_csv(f\"./performance/predictions_{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0cf5b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    data = pd.read_csv(f\"./datasets_regression/syn_new/{dataset}.csv\", header=None, sep=\",\")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/syn_new/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", sep=\",\")\n",
    "    \n",
    "    data = np.round(data,2)\n",
    "    \n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "\n",
    "    train_data = data[~data[1].isin(testbags[\"x\"].tolist())]    \n",
    "    \n",
    "    test_data = data[data[1].isin(testbags[\"x\"].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    train_features = np.round(train_features,2)\n",
    "    test_features = np.round(test_features,2)\n",
    "\n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ae61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 2, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 3, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 4, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 5, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 6, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 7, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 8, fold 9\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 1\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 2\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 3\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 4\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 5\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 6\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 7\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 8\n",
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 9, fold 9\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 1\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 2\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 3\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 4\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 5\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 6\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 7\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 8\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 1, fold 9\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 1\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 2\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 3\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 4\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 5\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 6\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 7\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 8\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 2, fold 9\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 3, fold 1\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 3, fold 2\n",
      "dataset nBag_50_nFeat_2_nInsPerBag_10, rep 3, fold 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets_regression/syn_new/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 10):\n",
    "        for fold in range(1, 10):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True)\n",
    "\n",
    "            model = PrototypeForest(size=100,\n",
    "                                    max_depth=8,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_synthetic.csv\")\n",
    "\n",
    "all_df.to_csv(f\"./performance/predictions_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1543e862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nBag_25_nFeat_2_nInsPerBag_2', 1, 1, 0.934356161080299],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 2, 0.7496518567639255],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 3, 0.45288091219125703],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 4, 0.9372973769525493],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 5, 0.9476056271314891],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 6, 0.45965959328028283],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 7, 0.7524484799020607],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 8, 0.8630360300618921],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 1, 9, 0.7903035661656351],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 1, 0.9651801503094606],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 2, 1.1105532398635845],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 3, 0.8936505305039786],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 4, 0.5358642793987621],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 5, 0.7626215738284703],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 6, 0.8804874005305039],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 7, 0.8651098901098899],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 8, 0.9066136162687886],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 2, 9, 0.371737400530504],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 1, 0.9174889478337754],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 2, 1.0338362068965516],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 3, 0.7895490716180371],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 4, 0.9776451517830826],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 5, 1.0155669761273207],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 6, 0.7941036693191865],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 7, 0.6255117296496605],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 8, 0.9253258972091862],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 3, 9, 1.1372799614178923],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 1, 0.33104264435829417],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 2, 1.0967119805481873],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 3, 0.9632038220400287],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 4, 0.948988726790451],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 5, 0.9263790089221122],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 6, 0.7087288308508466],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 7, 1.1104040356195528],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 8, 0.8918671845395982],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 4, 9, 1.0131564986737398],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 1, 1.0345301250473664],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 2, 1.1817948717948716],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 3, 0.843837312113174],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 4, 0.8805481874447392],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 5, 1.0380268199233715],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 6, 0.9899535809018566],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 7, 1.0893348031279064],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 8, 1.0898342175066311],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 5, 9, 0.8890316579971753],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 1, 0.8496408045977011],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 2, 0.8946330680813438],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 3, 1.0133442154662313],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 4, 0.7502188328912467],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 5, 0.8782697408692102],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 6, 0.8285941644562335],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 7, 0.8723309018567638],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 8, 0.7323625847332742],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 6, 9, 0.31001024837231733],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 1, 0.7176226790450927],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 2, 0.9928713527851458],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 3, 0.7942639257294429],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 4, 1.2152804092459262],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 5, 0.9539135720601237],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 6, 0.803189400122424],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 7, 0.6190318302387267],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 8, 0.5282861774241083],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 7, 9, 0.8844861592872202],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 1, 1.0087533156498671],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 2, 0.8990753020925433],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 3, 0.8505923961096373],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 4, 0.8215683023872677],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 5, 0.6959885737604569],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 6, 0.9671253315649867],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 7, 1.088523430592396],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 8, 0.7807976506252368],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 8, 9, 0.6974573702159909],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 1, 0.8711538461538461],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 2, 0.8600298408488062],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 3, 0.7372617833095286],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 4, 1.1943332529539425],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 5, 0.9471444603142214],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 6, 0.7920437155682513],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 7, 1.0018236074270557],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 8, 0.9003599848427435],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_2', 9, 9, 1.169437076333628],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 1, 2.5379565766046084],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 2, 2.4422418710781515],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 3, 3.414309424010185],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 4, 3.6857243642932462],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 5, 2.820397870811447],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 6, 3.3599673287351552],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 7, 2.916930520068838],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 8, 2.7737046016352918],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 1, 9, 2.790926863472143],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 1, 2.440198256622732],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 2, 2.6019080463176296],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 3, 2.61900069886379],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 4, 2.48651744401887],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 5, 2.2785890853774484],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 6, 2.1159248338655066],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 7, 3.0121778347751866],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 8, 2.623191592191164],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 2, 9, 2.7230930649498815],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 1, 3.158369511264548],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 2, 2.3556387807237282],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 3, 2.448947146506609],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 4, 3.212937005948985],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 5, 3.381593458832478],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 6, 2.735892531570529],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 7, 2.0000869258142506],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 8, 2.980039067226745],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 3, 9, 2.813674024253354],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 1, 3.0354781893857767],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 2, 2.8914973598373486],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 3, 2.9706322390292725],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 4, 3.197165299047786],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 5, 2.5389990785027865],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 6, 2.761371787020569],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 7, 2.7393052849413375],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 8, 2.7459481225462232],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 4, 9, 3.0195949800342263],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 1, 2.9154677695379347],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 2, 2.458430445766441],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 3, 3.0550084566807776],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 4, 2.3398323961643985],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 5, 2.086931489553943],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 6, 2.402322862533929],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 7, 3.0734320487516684],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 8, 2.877161913955981],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 5, 9, 3.1353423750217213],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 1, 3.2933490639423324],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 2, 2.9448891186537365],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 3, 3.058094287344145],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 4, 2.1901057316563515],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 5, 3.135082715345123],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 6, 2.4975914305634785],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 7, 3.953752441701672],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 8, 2.7373525084817003],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 6, 9, 3.458155981687071],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 1, 2.669026723419193],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 2, 3.180518271198953],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 3, 2.4722441800450925],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 4, 2.7555064334157313],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 5, 2.5253300450790466],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 6, 2.4043781031459295],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 7, 3.493874766107241],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 8, 3.7780940345650835],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 7, 9, 2.011785238910168],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 1, 2.7627632972905736],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 2, 2.2109997702013446],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 3, 3.2514749389574327],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 4, 2.84408095438875],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 5, 2.582236527943048],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 6, 3.145191869064245],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 7, 3.108465314871476],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 8, 2.605168848476503],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 8, 9, 3.328226611523103],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 1, 2.9615503390535665],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 2, 3.4056260695949794],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 3, 2.7184622280172763],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 4, 3.1120222475755845],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 5, 3.2070871702931028],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 6, 2.7454782135029525],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 7, 2.5196371204680266],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 8, 3.074142913561054],\n",
       " ['nBag_50_nFeat_2_nInsPerBag_10', 9, 9, 2.7179279964505287],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 1, -0.675294379880711],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 2, -0.6190919059624097],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 3, -0.6572256084021674],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 4, -0.9102635840045914],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 5, -0.6815303312905232],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 6, -0.7361070717185826],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 7, -0.6289747422840949],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 8, -0.7006678747910763],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 1, 9, -0.7965984355372846],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 1, -0.6390176747490897],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 2, -0.9629513056221691],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 3, -0.9163713717670555],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 4, -0.8377393323436492],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 5, -0.8764928228357489],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 6, -0.7320907163158363],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 7, -0.7469163131033636],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 8, -0.7313698491756047],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 2, 9, -0.84793293570272],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 1, -0.8239302737704018],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 2, -0.9019257898154782],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 3, -0.7175059952038371],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 4, -0.7448263611333158],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 5, -0.748872013500311],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 6, -0.9119519199455254],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 7, -0.8372228499251883],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 8, -0.8464520050626169],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 3, 9, -0.7545477506883383],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 1, -0.9945088301653051],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 2, -0.8361782795985436],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 3, -0.849636957100986],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 4, -0.8321632437639633],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 5, -0.7118255395683456],\n",
       " ['nBag_25_nFeat_2_nInsPerBag_10', 4, 6, -0.8827959277599569]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
