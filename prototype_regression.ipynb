{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37c458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "        \n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self,\n",
    "                train_features,\n",
    "                 feature_types = [\"min\", \"max\", \"mean\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner=True,\n",
    "                 early_stopping_round = 3):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.train_features = train_features\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        number_of_rows = features.shape[0]\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                          size=prototype_count, \n",
    "                                          replace=False)\n",
    "\n",
    "        prot = features[random_indices, :]\n",
    "        if len(prot.shape) == 1:\n",
    "            prot = prot.reshape(1, prot.shape[0])\n",
    "        return prot\n",
    "\n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        return np.asarray(np.sum(y)/y.size)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "        \n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        _, index  = np.unique(bag_ids, return_index=True)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_mean = np.add.reduceat(distances[:, i], index)\n",
    "                mean_vals = np.repeat(group_mean/bin_count, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "        \n",
    "        return np.array(np.transpose(feature_list))\n",
    "\n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        \n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_via_prototype, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        \n",
    "        bdc = tree.DecisionTreeRegressor(\n",
    "            min_samples_split=2,\n",
    "            criterion=\"mae\"\n",
    "        )\n",
    "        bdc.fit(features_via_prototype[index], labels[index])\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        labels_left = labels[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "\n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids)) < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            \n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "            \n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids_left)) < self.min_samples_leaf or len(np.unique(bag_ids_right)) < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "            \n",
    "            _, index_left  = np.unique(bag_ids_left, return_index=True)\n",
    "            _, index_right  = np.unique(bag_ids_right, return_index=True)\n",
    "            \n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left[index_left])\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right[index_right])\n",
    "\n",
    "            # splitting recursively\n",
    "            \n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "\n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "\n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "\n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "\n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "\n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree)\n",
    "            \n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)\n",
    "\n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"],\n",
    "                max_depth = 8, \n",
    "                min_samples_leaf = 2, \n",
    "                min_samples_split = 2, \n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        group_min = np.minimum.reduceat(labels, index)\n",
    "        pos_bag_size = math.ceil(np.where(group_min == 1)[0].shape[0] * 0.8)\n",
    "        neg_bag_size = math.ceil(np.where(group_min == 0)[0].shape[0] * 0.8)\n",
    "        \n",
    "        bags_pos = np.random.choice(ids[np.where(group_min == 1)], pos_bag_size, replace=False)\n",
    "        bags_neg = np.random.choice(ids[np.where(group_min == 0)], neg_bag_size, replace=False)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1),\n",
    "                                          train_labels.reshape(train_labels.shape[0],1)],\n",
    "                                         axis=1))\n",
    "        indices_pos = df[df[0].isin(bags_pos)].index.to_numpy()\n",
    "        indices_neg = df[df[0].isin(bags_neg)].index.to_numpy()\n",
    "        inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "        \n",
    "        return inbag_indices, outbag_indices\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            if (self.use_prototype_learner) & (i%10==1):\n",
    "                print(f\"Tree {i} will be trained\")\n",
    "            \n",
    "            (inbag_indices, _) = self.sample(features, labels, bag_ids)\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            while tree.Tree.right is None:\n",
    "                tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            self._trees.append(tree)\n",
    "            \n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return np.sum(preds, axis=1)/self.size\n",
    "   \n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0, 1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "\n",
    "    #sort_index = np.argsort(bag_ids)\n",
    "    #bag_ids = bag_ids[sort_index]\n",
    "    #features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False):\n",
    "    data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "          \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3bf0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 will be trained\n",
      "Tree 11 will be trained\n",
      "Tree 21 will be trained\n",
      "Tree 31 will be trained\n",
      "Tree 41 will be trained\n",
      "Tree 51 will be trained\n",
      "Tree 61 will be trained\n",
      "Tree 71 will be trained\n",
      "Tree 81 will be trained\n",
      "Tree 91 will be trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3743417134141107"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"WheatYields\"\n",
    "\n",
    "(train_features,\n",
    "     train_labels,\n",
    "     train_bag_ids,\n",
    "     test_features,\n",
    "     test_labels,\n",
    "     test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True)\n",
    "\n",
    "model = PrototypeForest(size=100,\n",
    "                        max_depth=8,\n",
    "                        min_samples_leaf=2,\n",
    "                        min_samples_split=4,\n",
    "                        prototype_count=1,\n",
    "                        early_stopping_round= 5)\n",
    "\n",
    "model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "probas = model.predict(test_features, test_bag_ids)\n",
    "\n",
    "_, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "score/(np.sum(test_labels)/test_labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c4c45cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 will be trained\n",
      "Tree 11 will be trained\n",
      "Tree 21 will be trained\n",
      "Tree 31 will be trained\n",
      "Tree 41 will be trained\n",
      "Tree 51 will be trained\n",
      "Tree 61 will be trained\n",
      "Tree 71 will be trained\n",
      "Tree 81 will be trained\n",
      "Tree 91 will be trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5250362515717638"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"CornYields\"\n",
    "\n",
    "(train_features,\n",
    "     train_labels,\n",
    "     train_bag_ids,\n",
    "     test_features,\n",
    "     test_labels,\n",
    "     test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True)\n",
    "\n",
    "model = PrototypeForest(size=100,\n",
    "                        max_depth=8,\n",
    "                        min_samples_leaf=2,\n",
    "                        min_samples_split=4,\n",
    "                        prototype_count=1,\n",
    "                        early_stopping_round= 5)\n",
    "\n",
    "model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "probas = model.predict(test_features, test_bag_ids)\n",
    "\n",
    "_, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "score/(np.sum(test_labels)/test_labels.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
