{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.5.1\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# flake8: noqa\n",
    "# pylint: skip-file\n",
    "\n",
    "# # Noob Pipeline Examples\n",
    "\n",
    "# +\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from noob.utils.test import PySparkTest\n",
    "from noob.utils.io import read_parquet\n",
    "from warp.spark.date import create_calendar_table\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ## [Daily data generation](api/noob.data.daily_data.rst#noob.data.daily_data.base.BaseStockedDayGenerator)\n",
    "\n",
    "# ### Basic Usage\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from noob.data.daily_data import BaseStockedDayGenerator\n",
    "from os.path import dirname, abspath, join\n",
    "from tests.data.test_daily_data import DailyDataTest as tables\n",
    "\n",
    "\n",
    "data_path = join(dirname(dirname(abspath(''))), \"data\")\n",
    "tables.create_test_data(spark)\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "daily_data_path = join(data_path, \"model-data\", \"daily-data\")\n",
    "\n",
    "stocked_day_generator = BaseStockedDayGenerator(sales=sales, inventory=inventory, calendar=calendar, target_date=\"2019-07-10\")\n",
    "stocked_day_generator.generate_daily_data()\n",
    "stocked_day_generator.write(daily_data_path)\n",
    "\n",
    "# ### Daily data with extra_info\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "extra_info_df = tables.products\n",
    "extra_info_df = extra_info_df.select(\"product_id\", \"product_description\").distinct()\n",
    "extra_info = [\n",
    "    {\n",
    "    \"dataframe\": extra_info_df,\n",
    "    \"name\": \"brands\",\n",
    "    \"join_keys\": [\"product_id\"],\n",
    "    \"join_columns\": [\"product_description\"],\n",
    "    \"how\": \"left\",\n",
    "    \"rename\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "inventory.printSchema()\n",
    "\n",
    "from noob.data.daily_data import BaseStockedDayGenerator\n",
    "b = BaseStockedDayGenerator(sales=sales, inventory=inventory, calendar=calendar, target_date=\"2019-07-10\")\n",
    "b.generate_daily_data(extra_data_config=extra_info)\n",
    "b.inventory_sales.printSchema()\n",
    "\n",
    "# ### Daily data with sku_family and multiplier\n",
    "\n",
    "# Let's assume that the product 0 is sold in a pack of six and it has two different parents for\n",
    "# two different stores e.g. 3 and 4. Both parents are sold as a single item. Thus, we need to\n",
    "# set the multiplier as 6. The rest of the products do not have any parents. First, let's generate daily data\n",
    "# without multiplier adjustment.\n",
    "#\n",
    "\n",
    "# +\n",
    "sku_family = spark.createDataFrame(\n",
    "            [(3, 2, 0),\n",
    "             (4, 1, 0)],\n",
    "            [\"store_id\", \"parent_id\", \"product_id\"])\n",
    "\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "\n",
    "stocked_day_generator = BaseStockedDayGenerator(sales=sales, inventory=inventory, calendar=calendar, target_date=\"2019-07-10\")\n",
    "stocked_day_generator.generate_daily_data(sku_family=sku_family)\n",
    "\n",
    "daily_wo_multiplier = stocked_day_generator.inventory_sales\n",
    "# -\n",
    "\n",
    "# Now, let's generate the daily data with multiplier adjustment and observe the difference. columns_to_multiply is given as\n",
    "# [\"sales_quantity\", \"inventory\", \"incoming_inventory\"]\n",
    "\n",
    "# +\n",
    "sku_family = spark.createDataFrame(\n",
    "        [(3, 2, 0, 6),\n",
    "         (4, 1, 0, 6)],\n",
    "        [\"store_id\", \"parent_id\", \"product_id\", \"multiplier\"]\n",
    ")\n",
    "\n",
    "stocked_day_generator = BaseStockedDayGenerator(sales=sales, inventory=inventory, calendar=calendar, target_date=\"2019-07-10\")\n",
    "stocked_day_generator.generate_daily_data(sku_family=sku_family, multiply_exclude_cols=(\"sales_revenue\",))\n",
    "\n",
    "daily_with_multiplier = stocked_day_generator.inventory_sales\n",
    "# -\n",
    "\n",
    "daily_with_multiplier.exceptAll(daily_wo_multiplier).orderBy(\"date\", \"store_id\", \"product_id\").show(10)\n",
    "\n",
    "daily_wo_multiplier.exceptAll(daily_with_multiplier).orderBy(\"date\", \"store_id\", \"product_id\").show(10)\n",
    "\n",
    "# Now assume that the parent is sold as six pack instead. Then the multiplier should be 1/6.\n",
    "\n",
    "# +\n",
    "sku_family = spark.createDataFrame(\n",
    "    [(3, 2, 0, 1 / 6),\n",
    "     (4, 1, 0, 1 / 6)],\n",
    "    [\"store_id\", \"parent_id\", \"product_id\", \"multiplier\"]\n",
    ")\n",
    "\n",
    "stocked_day_generator = BaseStockedDayGenerator(sales=sales, inventory=inventory, calendar=calendar, target_date=\"2019-07-10\")\n",
    "stocked_day_generator.generate_daily_data(sku_family=sku_family, multiply_exclude_cols=(\"sales_revenue\",))\n",
    "\n",
    "daily_reverse = stocked_day_generator.inventory_sales\n",
    "# -\n",
    "\n",
    "daily_reverse.exceptAll(daily_wo_multiplier).show(10)\n",
    "\n",
    "\n",
    "# ### Daily data with extra inventory column and store aggregated\n",
    "\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "\n",
    "daily_data = BaseStockedDayGenerator(\n",
    "    sales=sales,\n",
    "    inventory=inventory,\n",
    "    target_date=\"2019-07-10\",\n",
    "    calendar=calendar,\n",
    "    update_range=None,\n",
    "    groupby_columns=(\"year\", \"week\", \"date\", \"product_id\"), # store dimension is removed\n",
    "    inventory_columns=(\"inventory\", \"incoming_inventory\", \"outgoing_inventory\"), # outgoing_inventory is added\n",
    "    sales_columns=(\"sales_quantity\", \"sales_revenue\"))\n",
    "\n",
    "daily_data.generate_daily_data()\n",
    "\n",
    "daily_data.inventory_sales.show(5)\n",
    "\n",
    "# ### Daily data with different aggregation types\n",
    "\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "\n",
    "daily_data = BaseStockedDayGenerator(\n",
    "    sales=sales,\n",
    "    inventory=inventory,\n",
    "    target_date=\"2019-07-10\",\n",
    "    calendar=calendar,\n",
    "    update_range=None,\n",
    "    groupby_columns=(\"year\", \"week\", \"date\", \"product_id\"),\n",
    "    inventory_columns=({\"agg_name\": \"min\",\n",
    "                        \"col_name\": \"inventory\",\n",
    "                        \"alias\": \"inventory\"},\n",
    "                       {\"agg_name\": \"min\",\n",
    "                        \"col_name\": \"incoming_inventory\",\n",
    "                        \"alias\": \"incoming_inventory\"},\n",
    "                       {\"agg_name\": \"min\",\n",
    "                        \"col_name\": \"outgoing_inventory\",\n",
    "                        \"alias\": \"outgoing_inventory\"}),\n",
    "    sales_columns=(\"sales_quantity\", \"sales_revenue\")) # defaults to {\"agg_name\": \"sum\",\n",
    "                                                       #              \"col_name\": \"sales_quantity\",\n",
    "                                                       #              \"alias\": \"sales_quantity\"}\n",
    "\n",
    "daily_data.generate_daily_data()\n",
    "\n",
    "daily_data.inventory_sales.show(5)\n",
    "\n",
    "# ### Daily data with Input/Output Control\n",
    "\n",
    "sales = tables.sales\n",
    "inventory = tables.inventory\n",
    "calendar = tables.calendar\n",
    "daily_data = BaseStockedDayGenerator(\n",
    "    sales=sales, inventory=inventory,\n",
    "    target_date=\"2019-07-10\",\n",
    "    calendar=calendar,\n",
    "    update_range=None)\n",
    "daily_data.generate_daily_data()\n",
    "daily_data.inventory_sales.cache()\n",
    "\n",
    "new_data = daily_data.inventory_sales \\\n",
    "    .filter(F.col(\"date\") == \"2019-07-10\")\n",
    "old_data = daily_data.inventory_sales \\\n",
    "    .filter(F.col(\"date\") == \"2019-07-03\")\n",
    "\n",
    "summary_new = daily_data.generate_summary_rowwise(new_data)\n",
    "summary_old = daily_data.generate_summary_rowwise(old_data)\n",
    "comparison = daily_data \\\n",
    "    .control(summary_old, summary_new, controller=None, logger=None, groupby_features=[\"store_id\", \"product_id\"])\n",
    "summary_old.show()\n",
    "summary_new.show()\n",
    "\n",
    "# Input output controls have passed and thus, we have an empty comparison dataframe.\n",
    "\n",
    "comparison.show()\n",
    "\n",
    "# ## [Outlier detection](api/noob.outlier.rst#noob.outlier.base.BaseOutlierDetection)\n",
    "#\n",
    "\n",
    "# ### Store-Product level\n",
    "\n",
    "from noob.outlier import BaseOutlierDetection\n",
    "from tests.outlier.test_outlier_detection import OutlierDetectionTest\n",
    "\n",
    "OutlierDetectionTest.create_test_data(spark)\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data = OutlierDetectionTest.data,\n",
    "    groupby_features=[\"store_id\", \"product_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar = OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=10)\n",
    "outliers, thresholds = bod.calculate(sales)\n",
    "\n",
    "# ### Store-product level with rules & extra info\n",
    "\n",
    "OutlierDetectionTest.rules.show()\n",
    "OutlierDetectionTest.extra_info_df.show()\n",
    "extra_info = [{\n",
    "    \"dataframe\": OutlierDetectionTest.extra_info_df,\n",
    "    \"name\": \"brands\",\n",
    "    \"join_keys\": [\"product_id\"],\n",
    "    \"join_columns\": [\"brand\"],\n",
    "    \"how\": \"left\"\n",
    "}]\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data=OutlierDetectionTest.data,\n",
    "    groupby_features=[\"store_id\", \"product_id\"],\n",
    "    stdev_multip=1,\n",
    "    rules=OutlierDetectionTest.rules\n",
    ")\n",
    "sales = bod.preprocess(\n",
    "    calendar = OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    extra_info=extra_info,\n",
    "    lookback_days=10)\n",
    "outliers, thresholds = bod.calculate(sales)\n",
    "\n",
    "# Assume that we have calculated these thresholds beforehand. We can give it to the class as a parameter and it will not calculate thresholds for the pairs that already have a threshold calculated.\n",
    "# New pairs' thresholds will be calculated from a higher level (new_thresholds_level) in this case.\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data=OutlierDetectionTest.data,\n",
    "    groupby_features=[\"store_id\", \"product_id\"],\n",
    "    stdev_multip=1,\n",
    "    recalculation_limit=8,\n",
    "    current_thresholds=OutlierDetectionTest.thresholds,\n",
    "    new_thresholds_level=[\"brand\"]\n",
    ")\n",
    "sales = bod.preprocess(\n",
    "    calendar = OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=7,\n",
    "    extra_info=extra_info)\n",
    "\n",
    "outliers_new, thresholds_new = bod.calculate(sales)\n",
    "\n",
    "# ### Product-Promo level\n",
    "\n",
    "# Add artificial promo info to the data\n",
    "# Sales increase by 1000% in summer\n",
    "import pyspark.sql.functions as F\n",
    "data = OutlierDetectionTest.data\n",
    "\n",
    "data = data.withColumn(\"sales_quantity\", F.when(F.col(\"date\") > \"2020-01-21\", F.col(\"sales_quantity\")*10).otherwise(F.col(\"sales_quantity\")))\n",
    "data = data.withColumn(\"promo\", F.when(F.col(\"date\") > \"2020-01-21\", F.lit(1)).otherwise(F.lit(0)))\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data=data,\n",
    "    groupby_features=[\"promo\", \"product_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar=OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=30)\n",
    "outliers, thresholds = bod.calculate(sales)\n",
    "outliers.show(5)\n",
    "thresholds.show(5)\n",
    "# ### Store level\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data=data,\n",
    "    groupby_features=[\"store_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar=OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=30)\n",
    "outliers, thresholds = bod.calculate(sales)\n",
    "outliers.show(5)\n",
    "thresholds.show(5)\n",
    "# ### Product level\n",
    "\n",
    "bod = BaseOutlierDetection(\n",
    "    data=data,\n",
    "    groupby_features=[\"product_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar=OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=30)\n",
    "outliers, thresholds = bod.calculate(sales)\n",
    "outliers.show(5)\n",
    "thresholds.show(5)\n",
    "\n",
    "# ### Controls\n",
    "\n",
    "# +\n",
    "# Take an old run\n",
    "bod = BaseOutlierDetection(\n",
    "    data=data,\n",
    "    groupby_features=[\"store_id\", \"product_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar=OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-15\",\n",
    "    lookback_days=10)\n",
    "outliers_old, _ = bod.calculate(sales)\n",
    "\n",
    "# Take a new run\n",
    "bod = BaseOutlierDetection(\n",
    "    data=data,\n",
    "    groupby_features=[\"store_id\", \"product_id\"],\n",
    "    stdev_multip=1)\n",
    "sales = bod.preprocess(\n",
    "    calendar=OutlierDetectionTest.calendar,\n",
    "    target_date=\"2020-01-31\",\n",
    "    lookback_days=10)\n",
    "outliers_new, _ = bod.calculate(sales)\n",
    "\n",
    "summary_old = bod.generate_summary(outliers_old, [\"store_id\", \"product_id\"])\n",
    "summary_new = bod.generate_summary(outliers_new, [\"store_id\", \"product_id\"])\n",
    "rowwise_summary_old = bod.generate_summary_rowwise(outliers_old, [\"product_id\"])\n",
    "rowwise_summary_new = bod.generate_summary_rowwise(outliers_new, [\"product_id\"])\n",
    "\n",
    "# +\n",
    "# Control general sum and row count of two results, also check if duplicate exists\n",
    "try:\n",
    "    bod.control(summary_old, summary_new, controller={\"row_count\": 0.5, \"outlier_sum\": 0.5}, logger=bod.logger, check_duplicates=True)\n",
    "except:\n",
    "    print(\"Test 1 failed\")\n",
    "\n",
    "# Control product based outlier sum of two results\n",
    "try:\n",
    "    bod.control(rowwise_summary_old, rowwise_summary_new, controller={\"outlier_sum\": 0.5}, logger=bod.logger, groupby_features=[\"product_id\"])\n",
    "except:\n",
    "    print(\"Test 2 failed\")\n",
    "# -\n",
    "\n",
    "# ## [Enriched data](api/noob.outlier.rst#noob.preprocess.enrich.base.BaseEnrich)\n",
    "#\n",
    "\n",
    "# ### Daily enrich store product level\n",
    "\n",
    "from noob.preprocessing.enrich import BaseEnrich\n",
    "\n",
    "scope = spark.createDataFrame([(1, 1)], [\"product_id\", \"store_id\"])\n",
    "outlier_df = spark.createDataFrame(\n",
    "    [(1, 1, \"2018-03-08\", 1)],\n",
    "    [\"store_id\", \"product_id\", \"date\", \"outlier\"],\n",
    ")\n",
    "extra_info = [\n",
    "    {\n",
    "        \"name\": \"outlier_product_store\",\n",
    "        \"dataframe\": outlier_df,\n",
    "        \"join_columns\": [\"outlier\"],\n",
    "        \"join_keys\": [\"store_id\", \"product_id\", \"date\"],\n",
    "        \"how\": \"left\",\n",
    "        \"default_value\": 0\n",
    "    }\n",
    "]\n",
    "calendar = create_calendar_table().select(\n",
    "    F.col(\"date\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_year\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_week\").alias(\"week\").cast(\"string\"),\n",
    "    F.col(\"month\").cast(\"string\"),\n",
    "    F.col(\"day_of_week\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_year\").alias(\"year\").cast(\"string\"),\n",
    "    F.col(\"week_start_date\").cast(\"string\"),\n",
    "    F.col(\"week_end_date\").cast(\"string\"),\n",
    ")\n",
    "daily_data = spark.createDataFrame(\n",
    "    [\n",
    "        (2018, 10, 1, 1, \"2018-03-08\", 1, 1, 1, 1),\n",
    "        (2018, 10, 1, 1, \"2018-03-09\", 1, 1, 1, 1),\n",
    "    ],\n",
    "    [\n",
    "        \"year\",\n",
    "        \"week\",\n",
    "        \"store_id\",\n",
    "        \"product_id\",\n",
    "        \"date\",\n",
    "        \"inventory\",\n",
    "        \"sales_quantity\",\n",
    "        \"sales_revenue\",\n",
    "        \"usable\",\n",
    "    ],\n",
    ")\n",
    "aggs = [\n",
    "    {\n",
    "        \"groupby_features\": [\n",
    "            \"store_id\",\n",
    "            \"product_id\"\n",
    "        ],\n",
    "        \"aggs\": [\n",
    "            {\n",
    "                \"agg_name\": \"agg\",\n",
    "                \"params\": {\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"col_name\": \"sales_quantity\",\n",
    "                    \"alias\": \"sales_quantity\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"agg_name\": \"agg\",\n",
    "                \"params\": {\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"col_name\": \"sales_revenue\",\n",
    "                    \"alias\": \"sales_revenue\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"agg_name\": \"agg\",\n",
    "                \"params\": {\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"col_name\": \"inventory\",\n",
    "                    \"alias\": \"inventory\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"agg_name\": \"agg\",\n",
    "                \"params\": {\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"col_name\": \"usable\",\n",
    "                    \"alias\": \"usable\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"agg_name\": \"agg\",\n",
    "                \"params\": {\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"col_name\": \"outlier\",\n",
    "                    \"alias\": \"outlier\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"agg_name\": \"cond_feature\",\n",
    "                \"params\": {\n",
    "                    \"col_name\": \"outlier\",\n",
    "                    \"operation\": \"==\",\n",
    "                    \"val\": 1,\n",
    "                    \"value_for_true_case\": 0,\n",
    "                    \"value_for_false_case\": \"F.col('sales_quantity')\",\n",
    "                    \"agg_name\": \"sum\",\n",
    "                    \"alias\": \"replaced_sales\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "lags = [\n",
    "    # Store-product sales lag\n",
    "    {\n",
    "        \"lags\": [1, 7],\n",
    "        \"features\": [\"sales_quantity\", \"sales_revenue\"],\n",
    "        \"groupby_features\": [\"store_id\", \"product_id\"],\n",
    "        \"prefix\": \"\",\n",
    "        \"operation\": \"lag\"\n",
    "    },\n",
    "    # Product sales lag\n",
    "    {\n",
    "        \"lags\": [1],\n",
    "        \"features\": [\"sales_quantity\"],\n",
    "        \"groupby_features\": [\"product_id\"],\n",
    "        \"prefix\": \"product\",\n",
    "        \"operation\": \"lag\"\n",
    "    },\n",
    "    # lags of MA values\n",
    "    {\n",
    "        \"lags\": [3],\n",
    "        \"features\": [\"avg_-14_-1_sales_quantity\", \"avg_-7_0_sales_quantity\"],\n",
    "        \"groupby_features\": [\"product_id\", \"store_id\"],\n",
    "        \"prefix\": \"\",\n",
    "        \"operation\": \"lag\"\n",
    "    }\n",
    "]\n",
    "\n",
    "ma_calculations = [\n",
    "    {\n",
    "        \"ma_ranges\": [(-14, -1), (-7, 0)],\n",
    "        \"features\": [\"sales_quantity\"],\n",
    "        \"groupby_features\": [\"store_id\", \"product_id\"],\n",
    "        \"prefix\": \"\",\n",
    "        \"operation\": \"avg\"\n",
    "    },\n",
    "    {\n",
    "        \"ma_ranges\": [(-14, -1), (-7, 0)],\n",
    "        \"features\": [\"sales_quantity\"],\n",
    "        \"groupby_features\": [\"product_id\"],\n",
    "        \"prefix\": \"product\",\n",
    "        \"operation\": \"avg\"\n",
    "    }\n",
    "]\n",
    "\n",
    "enrich = BaseEnrich(\n",
    "    calendar,\n",
    "    start_date=\"2018-03-05\",\n",
    "    target_date=\"2018-03-12\",\n",
    "    mode=\"create\",\n",
    "    period=\"day\",\n",
    "    partition_cols=[\"year\", \"week\"],\n",
    "    scope_df=scope\n",
    ")\n",
    "\n",
    "enrich.enrich(daily_data, 0, daily_extra_info=extra_info,\n",
    "              aggs=aggs, ma_calculations=ma_calculations, lags=lags)\n",
    "\n",
    "enrich.data.show()\n",
    "\n",
    "# #### Daily enrich summary generation & control\n",
    "\n",
    "enrich_summary_metadata = enrich.generate_summary(\n",
    "    enrich.data,\n",
    "    groupby_columns_list=[[\"store_id\", \"product_id\", \"day_index\"]],\n",
    "    negative_count_cols=[\"sales_quantity\", \"sales_revenue\", \"inventory\"],\n",
    "    null_count_cols=[\"sales_quantity\", \"sales_revenue\", \"inventory\"]\n",
    ")\n",
    "\n",
    "# passing case\n",
    "prev_summary = enrich_summary_metadata\n",
    "compared = BaseEnrich.control(prev_summary, enrich_summary_metadata,\n",
    "    duplicate_check_key=\"|\".join([\"store_id\", \"product_id\", \"day_index\"]))\n",
    "\n",
    "# failing case\n",
    "try:\n",
    "    prev_summary = enrich_summary_metadata.withColumn(\"row_count\",\n",
    "        F.col(\"row_count\") * F.lit(1000))\n",
    "    compared = BaseEnrich.control(prev_summary, enrich_summary_metadata)\n",
    "except:\n",
    "    print(\"IO control failed!\")\n",
    "\n",
    "# #### Daily enrich summary generation & control without specifying params\n",
    "\n",
    "enrich_summary_metadata = enrich.generate_summary(enrich.data)\n",
    "\n",
    "# passing case\n",
    "prev_summary = enrich_summary_metadata\n",
    "compared = BaseEnrich.control(prev_summary, enrich_summary_metadata)\n",
    "\n",
    "# failing case\n",
    "try:\n",
    "    prev_summary = enrich_summary_metadata.withColumn(\"row_count\",\n",
    "        F.col(\"row_count\") * F.lit(1000))\n",
    "    compared = BaseEnrich.control(prev_summary, enrich_summary_metadata)\n",
    "except:\n",
    "    print(\"IO control failed!\")\n",
    "\n",
    "# #### Daily enrich summary generation rowwise & control rowwise\n",
    "\n",
    "rowwise_summary_agg_config = {\n",
    "    \"groupby_columns\": [\"product_id\", \"store_id\",\n",
    "                        \"{}_index\".format(enrich.period)],\n",
    "    \"aggs\": [\n",
    "        {\n",
    "            \"agg_name\": \"agg\",\n",
    "            \"params\": {\n",
    "                \"agg_name\": \"sum\",\n",
    "                \"col_name\": \"sales_quantity\",\n",
    "                \"alias\": \"sales_quantity\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"agg_name\": \"agg\",\n",
    "            \"params\": {\n",
    "                \"agg_name\": \"sum\",\n",
    "                \"col_name\": \"inventory\",\n",
    "                \"alias\": \"inventory\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "enrich_summary_rowwise = enrich.generate_summary_rowwise(\n",
    "    enrich.data, agg_config=rowwise_summary_agg_config)\n",
    "\n",
    "# passing case\n",
    "prev_summary = enrich_summary_rowwise\n",
    "compared = BaseEnrich.control_rowwise(\n",
    "    prev_summary,\n",
    "    enrich_summary_rowwise,\n",
    "    key_columns=rowwise_summary_agg_config[\"groupby_columns\"],\n",
    "    column_tolerances={\"inventory\": 1, \"sales_quantity\": 1}\n",
    ")\n",
    "\n",
    "# failing case\n",
    "try:\n",
    "    prev_summary = enrich_summary_rowwise.withColumn(\"sales_quantity\",\n",
    "        F.col(\"sales_quantity\") * F.lit(1000))\n",
    "    compared = BaseEnrich.control_rowwise(\n",
    "        prev_summary,\n",
    "        enrich_summary_rowwise,\n",
    "        key_columns=rowwise_summary_agg_config[\"groupby_columns\"],\n",
    "        column_tolerances={\"sales_quantity\": 0.5}\n",
    "    )\n",
    "except:\n",
    "    print(\"IO control failed\")\n",
    "\n",
    "# ### Daily enrich product level\n",
    "\n",
    "from noob.preprocessing.enrich import BaseEnrich\n",
    "\n",
    "# +\n",
    "aggs[0][\"groupby_features\"] = [\"product_id\"]\n",
    "aggs[0][\"aggs\"].append({\n",
    "    \"agg_name\": \"agg\",\n",
    "    \"params\": {\n",
    "        \"agg_name\": \"countDistinct\",\n",
    "        \"col_name\": \"store_id\",\n",
    "        \"alias\": \"store_count\"\n",
    "    }\n",
    "})\n",
    "\n",
    "enrich = BaseEnrich(\n",
    "    calendar,\n",
    "    start_date=\"2018-03-05\",\n",
    "    target_date=\"2018-03-12\",\n",
    "    mode=\"create\",\n",
    "    period=\"day\",\n",
    "    partition_cols=[\"year\", \"week\"],\n",
    "    scope_df=scope\n",
    ")\n",
    "# -\n",
    "\n",
    "enrich.enrich(daily_data, 0, daily_extra_info=extra_info, aggs=aggs)\n",
    "\n",
    "enrich.data.show()\n",
    "\n",
    "# ### Daily enrich product level with extra_info having join_expr and select_expr\n",
    "\n",
    "from noob.preprocessing.enrich import BaseEnrich\n",
    "\n",
    "# +\n",
    "aggs[0][\"groupby_features\"] = [\"product_id\"]\n",
    "\n",
    "daily_extra_info = [\n",
    "    {\n",
    "        \"name\": \"outlier_product_store\",\n",
    "        \"dataframe\": outlier_df,\n",
    "        \"join_columns\": [\"outlier\"],\n",
    "        \"join_keys\": [\"store_id\", \"product_id\", \"date\"],\n",
    "        \"how\": \"left\",\n",
    "        \"default_value\": 0,\n",
    "        \"join_expr\": lambda left_df, right_df: (\n",
    "            (left_df.store_id == right_df.store_id) &\n",
    "            (left_df.product_id == right_df.product_id) &\n",
    "            (left_df.date == right_df.date)\n",
    "        ),\n",
    "        \"select_expr\": lambda left_df, right_df: (\n",
    "            [left_df[\"*\"], right_df.outlier]\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "enrich = BaseEnrich(\n",
    "    calendar,\n",
    "    start_date=\"2018-03-05\",\n",
    "    target_date=\"2018-03-12\",\n",
    "    mode=\"create\",\n",
    "    period=\"day\",\n",
    "    partition_cols=[\"year\", \"week\"],\n",
    "    scope_df=scope\n",
    ")\n",
    "# -\n",
    "\n",
    "enrich.enrich(daily_data, 0, daily_extra_info=daily_extra_info, aggs=aggs)\n",
    "\n",
    "enrich.data.show()\n",
    "\n",
    "# ### Weekly enrich store-product level\n",
    "\n",
    "from noob.preprocessing.enrich import BaseEnrich\n",
    "\n",
    "# We need to remove the following aggregation since we don't want to aggregate stores.\n",
    "\n",
    "aggs[0]['aggs'][-1]\n",
    "\n",
    "del aggs[0]['aggs'][-1]\n",
    "\n",
    "# We also need to change the groupby features\n",
    "\n",
    "aggs[0][\"groupby_features\"] = [\"store_id\", \"product_id\"]\n",
    "\n",
    "# And set the period as week\n",
    "\n",
    "enrich = BaseEnrich(\n",
    "    calendar,\n",
    "    start_date=\"2018-03-05\",\n",
    "    target_date=\"2018-03-12\",\n",
    "    mode=\"create\",\n",
    "    period=\"week\",\n",
    "    partition_cols=[\"year\", \"week\"],\n",
    "    scope_df=scope\n",
    ")\n",
    "\n",
    "enrich.enrich(daily_data, 0, daily_extra_info=extra_info, aggs=aggs)\n",
    "\n",
    "enrich.data.show()\n",
    "\n",
    "# ### Weekly enrich product level\n",
    "\n",
    "from noob.preprocessing.enrich import BaseEnrich\n",
    "\n",
    "aggs[0][\"groupby_features\"] = [\"product_id\"]\n",
    "aggs[0][\"aggs\"].append({\n",
    "    \"agg_name\": \"agg\",\n",
    "    \"params\": {\n",
    "        \"agg_name\": \"countDistinct\",\n",
    "        \"col_name\": \"store_id\",\n",
    "        \"alias\": \"store_count\"\n",
    "    }\n",
    "})\n",
    "\n",
    "enrich = BaseEnrich(\n",
    "    calendar,\n",
    "    start_date=\"2018-03-05\",\n",
    "    target_date=\"2018-03-12\",\n",
    "    mode=\"create\",\n",
    "    period=\"week\",\n",
    "    partition_cols=[\"year\", \"week\"],\n",
    "    scope_df=scope\n",
    ")\n",
    "\n",
    "enrich.enrich(daily_data, 0, daily_extra_info=extra_info, aggs=aggs)\n",
    "\n",
    "enrich.data.show()\n",
    "\n",
    "# ## [Lost sales](api/noob.lostsales.rst#noob.lostsales.base.StoreShareLostSales)\n",
    "\n",
    "# #### Case 1: Default lost sales run\n",
    "\n",
    "from noob.lostsales import StoreShareLostSales\n",
    "\n",
    "# Load example dataframes\n",
    "from tests.lostsales.test_lostsales import LostSalesTest as tables\n",
    "tables.create_test_data(spark)\n",
    "\n",
    "lostsales = StoreShareLostSales(\n",
    "    calendar=tables.calendar,\n",
    "    agg_cols_day_effect=[\"cluster\"],\n",
    "    agg_cols_store_share=[\"product_id\"],\n",
    "    lookback_days=30\n",
    ")\n",
    "\n",
    "results = lostsales.calculate(\n",
    "    df=tables.data,\n",
    "    scope=tables.scope,\n",
    "    target_date=\"2010-07-18\",\n",
    "    life_start_table=tables.life_start_table,\n",
    "    extra_info=tables.extra_info\n",
    "    )\n",
    "\n",
    "results.show()\n",
    "\n",
    "# #### Case 2: Lost sales run with external store shares\n",
    "\n",
    "# +\n",
    "# Calculate shares with BaseShareCalculation module\n",
    "\n",
    "from noob.postprocessing.breakdown import BaseShareCalculation\n",
    "from noob.utils.features import union\n",
    "from noob.utils.date import get_period_calendar\n",
    "\n",
    "daily_calendar, _ = get_period_calendar(tables.calendar, \"day\")\n",
    "df = tables.data.join(daily_calendar.select(\"date\", \"day_index\"), on=\"date\", how=\"left\")\n",
    "\n",
    "# We want to calculate lost sales for the week '2010-07-12' to '2010-07-18'.\n",
    "# That's why we will filter out the data after that week and calculate\n",
    "# store shares for the following day, which is day_index == 3370.\n",
    "df = df.filter(\"date < '2010-07-12'\")\n",
    "future = df.select(\"store_id\", \"product_id\").distinct().crossJoin(\n",
    "    spark.createDataFrame([(199,)], [\"day_index\"]))\n",
    "df = union([df, future], col_handling=\"pad\")\n",
    "\n",
    "share_calculator = BaseShareCalculation(\n",
    "    target_date=\"2010-07-12\",\n",
    "    calendar=tables.calendar,\n",
    "    period=\"day\",\n",
    "    share_level=\"store_id\",\n",
    "    forecast_level=[\"product_id\"],\n",
    "    demand_col=\"sales_quantity\"\n",
    ")\n",
    "shares = share_calculator.calculate_level_shares(df, [(366,1)])\n",
    "shares = shares.select(\"store_id\", \"product_id\", \"store_id_share_0\")\n",
    "# -\n",
    "\n",
    "shares.show(5)\n",
    "\n",
    "lostsales = StoreShareLostSales(\n",
    "    calendar=tables.calendar,\n",
    "    agg_cols_day_effect=[\"cluster\"],\n",
    "    agg_cols_store_share=[\"product_id\"]\n",
    ")\n",
    "results_1 = lostsales.calculate(\n",
    "    df=tables.data,\n",
    "    target_date=\"2010-07-18\",\n",
    "    scope=tables.scope,\n",
    "    store_shares=shares,\n",
    "    extra_info=tables.extra_info,\n",
    "    store_share_col=\"store_id_share_0\")\n",
    "\n",
    "results_1.show(5)\n",
    "\n",
    "# +\n",
    "# I/O Control step\n",
    "\n",
    "# First, generate summary of the previous run and current run\n",
    "summary = lostsales.generate_summary(results)\n",
    "summary_1 = lostsales.generate_summary(results_1)\n",
    "\n",
    "# Now, run the control step\n",
    "try:\n",
    "    lostsales.control(summary_1, summary, tables.scope)\n",
    "except:\n",
    "    print('Control failed')\n",
    "# -\n",
    "\n",
    "# #### Case 3: Smartlag lost sales\n",
    "\n",
    "from noob.lostsales.smartlag import SmartlagLostSales\n",
    "\n",
    "sls = SmartlagLostSales(\n",
    "    target_date=\"2010-07-18\",\n",
    "    calendar=tables.calendar,\n",
    "    groupby_cols_list=[[\"store_id\", \"product_id\"], [\"product_id\"], [\"cluster\"]],\n",
    "    window=7,\n",
    "    minimum_periods=3)\n",
    "\n",
    "df = sls.preprocess(\n",
    "    df=tables.data,\n",
    "    scope=tables.scope,\n",
    "    life_start_table=tables.life_start_table,\n",
    "    extra_info=tables.extra_info,\n",
    "    lookback_days=30,\n",
    "    lookforward_days=7)\n",
    "\n",
    "df = sls.predict(df)\n",
    "\n",
    "df = sls.calculate(df)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# #### Case 4: ml lost sales\n",
    "\n",
    "from noob.utils.date import get_period_calendar\n",
    "from noob.lostsales.ml_lib import MLLostSales\n",
    "from tests.lostsales.test_mllib_lostsales import MLLostSalesTest as tables\n",
    "tables.create_test_data(spark)\n",
    "\n",
    "sls = MLLostSales(\n",
    "    target_date=\"2010-07-18\",\n",
    "    calendar=tables.calendar)\n",
    "\n",
    "df = sls.preprocess(\n",
    "    df=tables.data,\n",
    "    scope=tables.data.select(\"store_id\", \"product_id\").distinct(),\n",
    "    extra_info=tables.extra_info,\n",
    "    lookback_days=60,\n",
    "    lookforward_days=7,\n",
    ")\n",
    "\n",
    "df = sls.enrich(df, 119)\n",
    "\n",
    "df.columns\n",
    "\n",
    "# +\n",
    "import pyspark.sql.functions as F\n",
    "import xgboost as xgb\n",
    "from noob.forecasting.ml_lib import MachineLearningModel\n",
    "\n",
    "df = df.withColumn(\"forecast_group_id\", F.lit(1))\n",
    "forecast_group_column = \"forecast_group_id\"\n",
    "\n",
    "cols_to_use = [\n",
    " 'year',\n",
    " 'week',\n",
    " 'sp_avg_-375_-351_in_stock_sales',\n",
    " 'sp_avg_-368_-362_in_stock_sales',\n",
    " 'sp_avg_-14_-7_in_stock_sales',\n",
    " 'sp_avg_7_14_in_stock_sales',\n",
    " 'p_avg_-375_-351_in_stock_sales',\n",
    " 'p_avg_-368_-362_in_stock_sales',\n",
    " 'p_avg_-14_-7_in_stock_sales',\n",
    " 'p_avg_7_14_in_stock_sales',\n",
    " 'lag_-21_in_stock_sales',\n",
    " 'lag_-14_in_stock_sales',\n",
    " 'lag_-7_in_stock_sales',\n",
    " 'lag_-1_in_stock_sales',\n",
    " 'lag_1_in_stock_sales',\n",
    " 'lag_7_in_stock_sales',\n",
    " 'lag_14_in_stock_sales',\n",
    " 'lag_21_in_stock_sales',\n",
    " 'product_lag_-21_in_stock_sales',\n",
    " 'product_lag_-14_in_stock_sales',\n",
    " 'product_lag_-7_in_stock_sales',\n",
    " 'product_lag_-1_in_stock_sales',\n",
    " 'product_lag_in_stock_sales',\n",
    " 'product_lag_1_in_stock_sales',\n",
    " 'product_lag_7_in_stock_sales',\n",
    " 'product_lag_14_in_stock_sales',\n",
    " 'product_lag_21_in_stock_sales']\n",
    "\n",
    "param = {}\n",
    "param['eta'] = 0.1\n",
    "param['grow_policy'] = 'lossguide'\n",
    "param['colsample_bytree'] = 0.8\n",
    "param['min_child_weight'] = 1\n",
    "param['booster'] = 'gbtree'\n",
    "param['objective'] = 'reg:squarederror'\n",
    "param['n_estimators'] = 100\n",
    "\n",
    "output_columns = [\n",
    "    \"store_id\", \"product_id\", \"date\", \"sales_quantity\"\n",
    "]\n",
    "xgbm = MachineLearningModel(model=xgb.XGBRegressor(**param))\n",
    "params = {\n",
    "    \"categorical_cols\": [],\n",
    "    \"numerical_cols\": [],\n",
    "    \"cols_to_use\": cols_to_use,\n",
    "    \"target_col\": 'in_stock_sales',\n",
    "    \"group_by_cols\": [forecast_group_column],\n",
    "    \"use_eval_set\": 0.5,\n",
    "    \"summary_output\": output_columns\n",
    "}\n",
    "# -\n",
    "\n",
    "df = sls.predict(df, xgbm, params, 7)\n",
    "\n",
    "ls_weekly = sls.calculate(df)\n",
    "\n",
    "ls_weekly.show(5, False)\n",
    "\n",
    "# ## [Volatility estimation](api/noob.volatility_estimation.rst#noob.volatility_estimation.base.VolatilityEstimator)\n",
    "\n",
    "from noob.volatility_estimation import VolatilityEstimator\n",
    "from tests.volatiliy_estimation.test_volatility_estimation import (\n",
    "    VolatiliyEstimationTest)\n",
    "from pyspark.sql.types import IntegerType, DoubleType, TimestampType\n",
    "vol_est = VolatilityEstimator(aggregation_levels=[\"h1\", \"h2\", \"h3\"],\n",
    "                              min_observation=3,\n",
    "                              mode=\"from_sales\",\n",
    "                              window_length=5,\n",
    "                              weight=\"base\")\n",
    "VolatiliyEstimationTest._create_test_data(spark)\n",
    "df = VolatiliyEstimationTest.df\n",
    "# mode: from_sales\n",
    "estimation_df_a = vol_est.calculate_abc(df)\n",
    "estimation_df_a.cache()\n",
    "estimation_df_a.show(3)\n",
    "# mode: from_forecast\n",
    "vol_est = VolatilityEstimator(aggregation_levels=[\"h1\", \"h2\", \"h3\"],\n",
    "                              min_observation=3,\n",
    "                              mode=\"from_forecast\",\n",
    "                              window_length=5,\n",
    "                              weight=\"base\")\n",
    "estimation_df_b = vol_est.calculate_abc(df)\n",
    "estimation_df_b.cache()\n",
    "estimation_df_b.show(3)\n",
    "# IO control\n",
    "# compare 1: dup or null check\n",
    "summ_a = vol_est.generate_summary(estimation_df_a)\n",
    "summ_b = vol_est.generate_summary(estimation_df_b)\n",
    "# compare 2: rowwise check\n",
    "summ_row_a = vol_est.generate_summary_rowwise(estimation_df_a)\n",
    "\n",
    "vol_est.control_io(summ_a, summ_b)\n",
    "vol_est.control_io(summ_row_a, summ_row_a, groupby_features=['level'])\n",
    "\n",
    "# ## Forecasting\n",
    "\n",
    "# ## [LLTD](api/noob.forecasting.rst#noob.forecasting.lltd.LLTD)\n",
    "# #### Daily Product Store Level\n",
    "\n",
    "# +\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from noob.utils.date import get_period_calendar\n",
    "from noob.utils import forecast_preprocess as fp\n",
    "from noob.forecasting.lltd import LLTD\n",
    "from tests.forecasting.test_lltd import LLTDTest\n",
    "from tests.forecasting.test_smartlag import SmartlagTest\n",
    "from warp.spark.date import create_calendar_table\n",
    "# -\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "window_size = 7\n",
    "hierarchy = [\"product_id\", \"store_id\"]\n",
    "\n",
    "calendar = create_calendar_table(start_date=\"2019-01-01\",\n",
    "                                 end_date=\"2019-06-01\",\n",
    "                                 week_start_day=1)\n",
    "calendar = calendar.withColumnRenamed(\"iso_8601_week\", \"week\")\n",
    "calendar, _ = get_period_calendar(calendar, period=\"day\")\n",
    "calendar.show(2)\n",
    "\n",
    "SmartlagTest.create_test_data(spark)\n",
    "enriched_df = SmartlagTest.enriched_df\n",
    "enriched_df.show(2)\n",
    "# +\n",
    "# Optional scope filter:\n",
    "# enriched_df = fp.scope_preprocess(enriched_df, scope_data, [\"product_id\", \"store_id\"])\n",
    "# -\n",
    "\n",
    "# MULTIPLIER PREPROCESS\n",
    "enriched_df = fp.multiplier_preprocess(enriched_df)\n",
    "\n",
    "enriched_df.show(2)\n",
    "\n",
    "# SPLIT ENRICHED DATA\n",
    "input_df, future_df = fp.split_train_test(\n",
    "    input_data=enriched_df,\n",
    "    forecast_start_date=\"2019-05-01\",\n",
    "    prediction_length=7,\n",
    "    calendar_df=calendar,\n",
    "    period=\"day\",\n",
    "    max_lookback=None,\n",
    ")\n",
    "\n",
    "input_df.show(1)\n",
    "\n",
    "future_df.show(1)\n",
    "\n",
    "future_df = fp.future_preprocess(future_df, hierarchy, 1)\n",
    "future_df.show(1)\n",
    "\n",
    "# STOCK OUT CORRECTION\n",
    "input_df = fp.stock_out_preprocess(input_df, hierarchy)\n",
    "input_df.show(1)\n",
    "input_df.count()\n",
    "\n",
    "# REMOVE OUTLIERS\n",
    "input_df = fp.outlier_preprocess(input_df, \"outlier\", \"0\", hierarchy, window_size)\n",
    "input_df.show(3)\n",
    "input_df.count()\n",
    "\n",
    "# ### Model\n",
    "model = LLTD([\"product_id\", \"store_id\"])\n",
    "base_fc = model.train(\n",
    "    input_df,\n",
    "    window_size=7,\n",
    "    min_window_size=7,\n",
    "    weight_mode=\"base\")\n",
    "preds = model.predict(base_fc, future_df)\n",
    "\n",
    "# #### Weekly Product Level\n",
    "\n",
    "window_size = 2\n",
    "hierarchy = [\"product_id\"]\n",
    "\n",
    "SmartlagTest.create_test_data(spark)\n",
    "enriched_df = SmartlagTest.enriched_df\n",
    "groupby_cols = [\"product_id\", \"year\", \"week\"]\n",
    "enriched_df = enriched_df.groupBy(groupby_cols).agg(\n",
    "    F.sum(\"sales_quantity\").alias(\"sales_quantity\"),\n",
    "    F.sum(\"sales_revenue\").alias(\"sales_revenue\"),\n",
    "    F.sum(\"replaced_sales\").alias(\"replaced_sales\"),\n",
    "    F.sum(\"inventory\").alias(\"inventory\"),\n",
    "    F.sum(\"outlier\").alias(\"outlier\"),\n",
    "    F.count(\"store_id\").alias(\"store_count\"),\n",
    "    F.sum(\"usable\").alias(\"usable\"))\n",
    "\n",
    "calendar = create_calendar_table(start_date=\"2019-01-01\",\n",
    "                                 end_date=\"2019-06-01\",\n",
    "                                 week_start_day=1)\n",
    "calendar = calendar.withColumnRenamed(\"iso_8601_week\", \"week\").drop(\"week_index\")\n",
    "calendar, _ = get_period_calendar(calendar, period=\"week\")\n",
    "\n",
    "c = calendar.select(F.col(\"iso_8601_year\").alias(\"year\"),\n",
    "                    F.col(\"week\"),\n",
    "                    F.col(\"week_index\"),\n",
    "                    F.col(\"week_start_date\").alias(\"start_date\"),\n",
    "                    F.col(\"week_end_date\").alias(\"end_date\")).distinct()\n",
    "enriched_df = enriched_df.join(c, on=[\"year\", \"week\"])\n",
    "# We will need a column named date\n",
    "enriched_df = enriched_df.withColumn(\"date\", F.col(\"start_date\"))\n",
    "# +\n",
    "# Optional Scope Filter\n",
    "# enriched_df = scope_preprocess(enriched_df, scope_data, [\"product_id\", \"store_id\"])\n",
    "# -\n",
    "\n",
    "# MULTIPLIER PREPROCESS\n",
    "enriched_df = fp.multiplier_preprocess(enriched_df)\n",
    "\n",
    "# SPLIT ENRICHED DATA\n",
    "input_df, future_df = fp.split_train_test(\n",
    "    input_data=enriched_df,\n",
    "    forecast_start_date=\"2019-04-29\",\n",
    "    prediction_length=4,\n",
    "    calendar_df=calendar,\n",
    "    period=\"week\",\n",
    "    max_lookback=None,\n",
    ")\n",
    "\n",
    "future_df = future_df.select(\n",
    "    *hierarchy,\n",
    "    F.col(\"start_date\").alias(\"forecast_start_date\"),\n",
    "    F.col(\"end_date\").alias(\"forecast_end_date\"),\n",
    "    F.col(\"future_multiplier\"),\n",
    "    F.lit(1).alias(\"prediction_size\"),\n",
    ")\n",
    "\n",
    "future_df.show(1)\n",
    "\n",
    "# STOCK OUT CORRECTION\n",
    "input_df = fp.stock_out_preprocess(input_df, hierarchy)\n",
    "\n",
    "# REMOVE OUTLIERS\n",
    "input_df = fp.outlier_preprocess(input_df, \"outlier\", \"0\", hierarchy, window_size)\n",
    "\n",
    "# ### Model\n",
    "model = LLTD([\"product_id\"])\n",
    "base_fc = model.train(\n",
    "    input_df,\n",
    "    window_size=2,\n",
    "    min_window_size=2,\n",
    "    weight_mode=\"base\")\n",
    "preds = model.predict(base_fc, future_df)\n",
    "base_fc.show(2)\n",
    "preds.show(2)\n",
    "\n",
    "# ## [RollingLLTD](api/noob.forecasting.rst#noob.forecasting.rolling_lltd.RollingLLTD)\n",
    "\n",
    "from noob.forecasting.rolling_lltd import RollingLLTD, get_bootstrap_period\n",
    "\n",
    "LLTDTest.create_test_data(spark)\n",
    "\n",
    "train_data = LLTDTest.train.drop(\"future_multiplier\")\n",
    "scope = train_data.select(\"product_id\", \"store_id\").distinct()\n",
    "future = get_bootstrap_period(\"2019-04-04\", \"2019-04-11\", spark)\n",
    "future = future.crossJoin(scope)\n",
    "\n",
    "model = RollingLLTD(\n",
    "    aggregation_level=[\"product_id\", \"store_id\"],\n",
    "    time_index=\"day_index\"\n",
    ")\n",
    "base_fc = model.train(\n",
    "    input_data=train_data,\n",
    "    window_size=7,\n",
    "    min_window_size=7,\n",
    "    max_lookback=10,\n",
    "    future_data=future\n",
    ")\n",
    "\n",
    "base_fc.show(2)\n",
    "\n",
    "\n",
    "preds = model.predict(\n",
    "    trained_model=base_fc,\n",
    "    future_data=future,\n",
    ")\n",
    "\n",
    "preds.show(2)\n",
    "\n",
    "# ## [Ensemble Forecasting](api/noob.forecasting.ensemble.rst#noob.forecasting.ensemble.non_negative_least_squares.NonNegativeLeastSquares)\n",
    "\n",
    "# +\n",
    "from noob.forecasting.ensemble.non_negative_least_squares import NonNegativeLeastSquares\n",
    "from tests.ensemble.NonNegativeLeastSquares.test_nnls import NNLSTest\n",
    "\n",
    "ensemble = NonNegativeLeastSquares(\n",
    "    group_by_cols=[\"attr_1\", \"attr_2\"],\n",
    "    forecast_cols=[\n",
    "        \"actual\",\n",
    "        \"lltd_49_base\",\n",
    "        \"lltd_28_exponential\",\n",
    "        \"lltd_105_base\",\n",
    "    ],\n",
    "    level=[\"date\", \"store_id\", \"product_id\"],\n",
    ")\n",
    "NNLSTest.create_test_data(spark)\n",
    "train = NNLSTest.train\n",
    "predict = NNLSTest.test\n",
    "\n",
    "train.show(2)\n",
    "predict.show(2)\n",
    "\n",
    "weight_frame = ensemble.train_weights(train, \"lsq\")\n",
    "ensemble_forecasts = ensemble.assemble_models(predict, weight_frame)\n",
    "weight_frame.show(2, truncate=False)\n",
    "ensemble_forecasts.show(2, truncate=False)\n",
    "# -\n",
    "\n",
    "# ## [Smart Selection](api/noob.forecasting.smart_selection.rst#noob.forecasting.smart_selection.base.BaseSmartSelect)\n",
    "\n",
    "# ### Smart Selection with time based features\n",
    "\n",
    "# +\n",
    "from noob.forecasting.smart_selection import BaseSmartSelect\n",
    "from tests.smartselect.test_smartselect import SmartSelectTest\n",
    "\n",
    "SmartSelectTest.create_test_data(spark)\n",
    "\n",
    "error = SmartSelectTest.train\n",
    "error_ = SmartSelectTest.test\n",
    "smart = BaseSmartSelect(\n",
    "    [\"attr\"])\n",
    "error_ranked = smart.calculate_metrics(error,metric_scale=False)\n",
    "smart_select_forecasts = smart.get_smart_selection(error_ranked, error_)\n",
    "smart_select_forecasts.show(2)\n",
    "# -\n",
    "\n",
    "# #### Prepare train data\n",
    "\n",
    "columns = [\n",
    "    \"store_id\", \"product_id\", \"date\", \"actual\", \"prediction\",\n",
    "    \"forecast_horizon\", \"error\", \"abs_error\", \"model_id\", \"time_attr_0\",\n",
    "    \"time_attr_1\"\n",
    "]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-01\", 5, 5, 1, 0, 0, \"a\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 0, 1, 5, 5, \"b\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 2, 1, 3, 3, \"c\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 2, 1, 3, 3, \"d\", 0, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-02\", 5, 0, 1, 5, 5, \"a\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 5, 1, 0, 0, \"b\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 4, 1, 1, 1, \"c\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 3, 1, 2, 2, \"d\", 1, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-03\", 5, 3, 1, 2, 2, \"a\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 2, 1, 3, 3, \"b\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 5, 1, 0, 0, \"c\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 1, 1, 4, 4, \"d\", 0, 1),\n",
    "\n",
    "    (1, 1, \"2020-02-04\", 5, 2, 1, 3, 3, \"a\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 0, 1, 5, 5, \"b\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 3, 1, 2, 2, \"c\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 5, 1, 0, 0, \"d\", 1, 1),\n",
    "]\n",
    "train = spark.createDataFrame(data, columns)\n",
    "train.show()\n",
    "\n",
    "# #### Prepare test data\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"date\", \"prediction\", \"model_id\",\n",
    "           \"time_attr_0\", \"time_attr_1\"]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-09\", 1, \"a\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 3, \"b\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 8, \"c\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 4, \"d\", 0, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-10\", 1, \"a\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 3, \"b\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 8, \"c\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 4, \"d\", 1, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-11\", 1, \"a\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 3, \"b\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 8, \"c\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 4, \"d\", 0, 1),\n",
    "\n",
    "    (1, 1, \"2020-02-12\", 1, \"a\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 3, \"b\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 8, \"c\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 4, \"d\", 1, 1),\n",
    "]\n",
    "test = spark.createDataFrame(data, columns)\n",
    "test.show()\n",
    "\n",
    "# #### Create smart selection model\n",
    "\n",
    "smart_selection = BaseSmartSelect(group_by_cols=[\"time_attr_0\", \"time_attr_1\"])\n",
    "\n",
    "# #### Find model ranks\n",
    "\n",
    "metrics = smart_selection.calculate_metrics(train)\n",
    "metrics.show()\n",
    "\n",
    "# #### Make prediction\n",
    "\n",
    "result = smart_selection.get_smart_selection(metrics, test)\n",
    "result.show()\n",
    "\n",
    "# ## [Smartlag](api/noob.forecasting.rst#noob.forecasting.smartlag.BaseSmartlag)\n",
    "\n",
    "# #### Basic usage\n",
    "\n",
    "# +\n",
    "from noob.forecasting.smartlag import BaseSmartlag\n",
    "from tests.forecasting.test_smartlag import SmartlagTest\n",
    "SmartlagTest.create_test_data(spark)\n",
    "\n",
    "enriched_df = SmartlagTest.enriched_df\n",
    "calendar = SmartlagTest.calendar\n",
    "\n",
    "smartlag = BaseSmartlag(\n",
    "    enriched_df=enriched_df,\n",
    "    calendar=calendar,\n",
    "    forecast_start_date='2019-05-01',\n",
    "    lookback_weeks=10,\n",
    "    forecast_horizon_weeks=4,\n",
    "    y_column='sales_quantity',\n",
    "    fixed_features=[\"product_id\", \"store_id\"],\n",
    "    groupby_cols_list=[[\"product_id\", \"store_id\",\n",
    "                        \"day_of_week\"], [\"product_id\", \"store_id\"]],\n",
    "    extra_output_columns=[])\n",
    "\n",
    "smartlag.predict(window=7,\n",
    "                 minimum_periods=3)\n",
    "smartlag.result.cache()\n",
    "\n",
    "smartlag.result.show(5)\n",
    "# -\n",
    "\n",
    "# #### Smartlag with lostsales\n",
    "\n",
    "# +\n",
    "from noob.forecasting.smartlag import BaseSmartlag\n",
    "\n",
    "SmartlagTest.create_test_data(spark)\n",
    "\n",
    "enriched_df = SmartlagTest.enriched_df\n",
    "calendar = SmartlagTest.calendar\n",
    "lostsales = SmartlagTest.lost_sales\n",
    "\n",
    "smartlag = BaseSmartlag(\n",
    "    enriched_df=enriched_df,\n",
    "    calendar=calendar,\n",
    "    forecast_start_date=\"2019-05-01\",\n",
    "    lookback_weeks=10,\n",
    "    forecast_horizon_weeks=4,\n",
    "    y_column=\"sales_quantity\",\n",
    "    fixed_features=[\"product_id\", \"store_id\"],\n",
    "    groupby_cols_list=[[\"product_id\", \"store_id\", \"day_of_week\", \"lostsales\"],\n",
    "                       [\"product_id\", \"store_id\"]],\n",
    "    extra_output_columns=[])\n",
    "\n",
    "smartlag.predict(window=7,\n",
    "                 minimum_periods=3,\n",
    "                 lost_sales_df=lostsales)\n",
    "smartlag.result.cache()\n",
    "\n",
    "smartlag.result.show(5)\n",
    "# -\n",
    "\n",
    "# ## [Rolling Smartlag](api/noob.forecasting.rst#noob.forecasting.rolling_smartlag.RollingSmartlag)\n",
    "\n",
    "# ### Rolling Smartlag without lostsales\n",
    "\n",
    "# +\n",
    "from noob.utils.io import read_parquet\n",
    "from noob.forecasting.rolling_smartlag import RollingSmartlag\n",
    "from tests.forecasting.test_rolling_smartlag import RollingSmartlagTest\n",
    "\n",
    "SmartlagTest.create_test_data(spark)\n",
    "enriched_df = SmartlagTest.enriched_df\n",
    "calendar = SmartlagTest.calendar\n",
    "\n",
    "rolling_smartlag = RollingSmartlag(\n",
    "    enriched_df=enriched_df,\n",
    "    test_df=enriched_df,\n",
    "    calendar=calendar,\n",
    "    forecast_start_date=\"2019-05-01\",\n",
    "    lookback_weeks=5,\n",
    "    forecast_horizon_weeks=2,\n",
    "    y_column=\"sales_quantity\",\n",
    "    fixed_features=[\"product_id\", \"store_id\"],\n",
    "    groupby_cols_list=[[\"product_id\", \"store_id\", \"day_of_week\"],\n",
    "                       [\"product_id\", \"store_id\"],\n",
    "                       [\"product_id\", \"day_of_week\"],\n",
    "                       [\"product_id\"]],\n",
    "    extra_output_columns=[])\n",
    "\n",
    "rolling_smartlag.predict(window=7,\n",
    "                 minimum_periods=3)\n",
    "rolling_smartlag.result.show()\n",
    "# -\n",
    "\n",
    "# ## [Machine learning forecasting](api/noob.forecasting.ml_lib.rst#noob.forecasting.ml_lib.base.MachineLearningModel)\n",
    "\n",
    "# +\n",
    "# Trained Model\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from noob.forecasting.ml_lib import MachineLearningModel\n",
    "from tests.forecasting.test_ml_lib import MachineLearningModelTest\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "MachineLearningModelTest.create_test_data(spark)\n",
    "train_data = MachineLearningModelTest.train_df\n",
    "test_data = MachineLearningModelTest.test_df\n",
    "\n",
    "# +\n",
    "cols_to_use = [\n",
    "    \"product_id\", \"week\", \"day_of_week\", \"avg_sales_lag_1\", \"avg_sales_lag_2\",\n",
    "    \"store_type\"]\n",
    "\n",
    "numerical_cols = [\n",
    "    \"product_id\", \"week\", \"day_of_week\", \"avg_sales_lag_1\", \"avg_sales_lag_2\"]\n",
    "\n",
    "categorical_cols = [\"store_type\"]\n",
    "\n",
    "group_by_cols = [\"forecast_group_id\"]\n",
    "\n",
    "lgbm = MachineLearningModel(\n",
    "    model=lgb.LGBMRegressor(\n",
    "        min_data=1,\n",
    "        n_jobs=-1,\n",
    "        seed=0,\n",
    "        early_stopping_rounds=3,\n",
    "        verbose=1))\n",
    "params = {\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"numerical_cols\": numerical_cols,\n",
    "    \"cols_to_use\":cols_to_use,\n",
    "    \"target_col\": 'avg_sales',\n",
    "    \"group_by_cols\": group_by_cols,\n",
    "    \"use_eval_set\": 0.1,\n",
    "    \"fit_params\": {\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# +\n",
    "# Train and predict separately\n",
    "\n",
    "model = lgbm.train(train_data, **params)\n",
    "prediction = lgbm.predict(model, test_data, **params)\n",
    "# -\n",
    "\n",
    "# #### Train and predict separately\n",
    "\n",
    "model = lgbm.train(train_data, **params)\n",
    "prediction = lgbm.predict(model, test_data, **params)\n",
    "\n",
    "model.show()\n",
    "\n",
    "prediction.show(2, truncate=False)\n",
    "\n",
    "# #### Train and predict together\n",
    "\n",
    "models, preds = lgbm.fit_predict(train_data, test_data, **params)\n",
    "# -\n",
    "\n",
    "models[0].show()\n",
    "\n",
    "# #### Train with different model parameters for each group\n",
    "\n",
    "params = {\n",
    "    **params,\n",
    "    \"model_params_by_group\": {\n",
    "        (23,): {\n",
    "            \"early_stopping_rounds\": 5,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "model = lgbm.train(train_data, **params)\n",
    "\n",
    "model.show()\n",
    "\n",
    "# #### Print feature importances of the model\n",
    "\n",
    "# +\n",
    "from noob.utils.analysis import get_feature_importances\n",
    "\n",
    "df = get_feature_importances(model_row=models[0].collect()[0])\n",
    "df\n",
    "# -\n",
    "\n",
    "# #### Print descriptive tree plots and SHAP plots of the model\n",
    "\n",
    "# +\n",
    "from noob.utils.analysis import get_shap_plots\n",
    "\n",
    "pdf, explainer, shap_values, force_plot, evals_result = get_shap_plots(\n",
    "    model_row=models[0].collect()[0],\n",
    "    test_data=test_data,\n",
    "    start=0,\n",
    "    end=5,\n",
    "    plot_trees=[0, -10, -5, -1])\n",
    "\n",
    "# +\n",
    "# See the force plot of your choice by using returned explainer, shap_values and pdf\n",
    "\n",
    "example_row = 0\n",
    "\n",
    "shap.force_plot(\n",
    "        explainer.expected_value, shap_values[example_row, :],\n",
    "        pdf.iloc[example_row, :])\n",
    "\n",
    "# +\n",
    "# See the force plot returned by the function call\n",
    "\n",
    "force_plot\n",
    "\n",
    "# +\n",
    "# See the loss plot by number of estimators\n",
    "\n",
    "plt.plot(evals_result[\"valid_0\"][\"l2\"])\n",
    "# -\n",
    "\n",
    "# #### Rolling ML\n",
    "\n",
    "# +\n",
    "params.update({\"period_col\": \"week_index\", \"summary_output\": [\"store_type\", \"product_id\", \"week_index\"]})\n",
    "\n",
    "preds = lgbm.rolling_fit_predict(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    horizon=4,  # Generate forecasts for 4 periods in each iteration\n",
    "    step=1,  # Generate forecasts in each period\n",
    "    test_periods=None,  # None for generating forecasts for each period in the test data\n",
    "    **params)\n",
    "# -\n",
    "\n",
    "preds.show(2, truncate=False)\n",
    "\n",
    "# ## [Long term forecasting](api/noob.forecasting.long_term.rst#noob.forecasting.long_term.base.BaseLongTermForecaster)\n",
    "#\n",
    "\n",
    "# +\n",
    "import lightgbm as lgb\n",
    "from noob.forecasting.ml_lib import MachineLearningModel\n",
    "from noob.forecasting.long_term import BaseLongTermForecaster\n",
    "from tests.forecasting.test_long_term import BaseLongTermForecasterTest\n",
    "from noob.forecasting.lltd import LLTD\n",
    "from warp.spark.date import create_calendar_table\n",
    "from noob.utils import forecast_preprocess as fp\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "# -\n",
    "\n",
    "BaseLongTermForecasterTest.create_test_data(spark)\n",
    "train_data = BaseLongTermForecasterTest.train_data\n",
    "validation_data = BaseLongTermForecasterTest.validation_data\n",
    "test_data =  BaseLongTermForecasterTest.test_data\n",
    "\n",
    "train_data.show(1)\n",
    "validation_data.show(1)\n",
    "test_data.show(1)\n",
    "\n",
    "data = train_data.unionByName(validation_data).unionByName(test_data)\n",
    "\n",
    "calendar = create_calendar_table(week_start_day=1)\n",
    "calendar = calendar.select(\n",
    "    \"date\",\n",
    "    F.col(\"week_start_date\").alias(\"start_date\"),\n",
    "    F.col(\"week_end_date\").alias(\"end_date\"))\n",
    "\n",
    "data = data.join(calendar, on=['date'], how='left')\n",
    "data = data.withColumn('sales_quantity', F.col('avg_sales'))\n",
    "data = data.withColumn('inventory', F.lit(0))\n",
    "\n",
    "data = fp.multiplier_preprocess(data)\n",
    "data = fp.stock_out_preprocess(data, ['product_id', 'store_type'])\n",
    "data = fp.outlier_preprocess(data, 'outlier', '0', ['product_id', 'store_type'], 2)\n",
    "data = data.withColumn('prediction_size', F.lit(1))\n",
    "data = data.withColumn('forecast_start_date', F.col('start_date'))\n",
    "data = data.withColumn('forecast_end_date', F.col('end_date'))\n",
    "\n",
    "train_data = data.filter(F.col(\"date\") < \"2020-02-24\")\n",
    "validation_data = data.filter(\n",
    "    (F.col(\"date\") >= \"2020-02-24\") &\n",
    "    (F.col(\"date\") < \"2020-03-16\"))\n",
    "test_data = data.filter(F.col(\"date\") >= \"2020-03-16\")\n",
    "\n",
    "train_data.show(1)\n",
    "\n",
    "test_data.show(1)\n",
    "\n",
    "# +\n",
    "lgbm = MachineLearningModel(\n",
    "    model=lgb.LGBMRegressor(\n",
    "        n_jobs=-1,\n",
    "        num_leaves=127,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=10,\n",
    "        min_child_samples=1000,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        early_stopping_rounds=3,\n",
    "        verbose=0\n",
    "    )\n",
    ")\n",
    "\n",
    "lltd = LLTD(['product_id', 'store_type'])\n",
    "# -\n",
    "\n",
    "lltd_params = {\n",
    "    \"window_size\": 1,\n",
    "    \"min_window_size\": 1,\n",
    "    \"weight_mode\": 'base',\n",
    "    \"summary_output\": ['avg_sales', 'week_index'],\n",
    "    \"replace_sales_quantity\": True\n",
    "}\n",
    "\n",
    "ml_params = {\n",
    "    \"categorical_cols\": ['store_type'],\n",
    "    \"numerical_cols\": [\n",
    "        \"product_id\", \"week\", \"day_of_week\", \"avg_sales_lag_1\",\n",
    "        \"avg_sales_lag_2\"],\n",
    "    \"cols_to_use\": [\n",
    "        \"product_id\", \"week\", \"day_of_week\", \"avg_sales_lag_1\",\n",
    "        \"avg_sales_lag_2\", \"store_type\"],\n",
    "    \"target_col\": 'avg_sales',\n",
    "    \"group_by_cols\": ['forecast_group_id'],\n",
    "    \"use_eval_set\": 0.5,\n",
    "    \"cols_to_pop\": [\n",
    "        ['avg_sales_lag_1'],\n",
    "        ['avg_sales_lag_2']],\n",
    "    \"run_for\": [12, 13, 14],\n",
    "    \"period_col\": 'week_index',\n",
    "    \"summary_output\": ['product_id', 'store_type', 'week_index'],\n",
    "    \"fit_params\": {\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "models_params = [\n",
    "    (lltd, lltd_params),\n",
    "    (lgbm, ml_params)\n",
    "]\n",
    "\n",
    "forecaster = BaseLongTermForecaster(\n",
    "    models_params=models_params,\n",
    "    target_col='avg_sales',\n",
    "    eval_based_on=['product_id', 'store_type', 'week_index'],\n",
    "    base_forecast_id='LGBMRegressor',\n",
    "    period_col='week_index')\n",
    "\n",
    "models, predictions = forecaster.run(train_data, validation_data, test_data, 4)\n",
    "\n",
    "predictions.groupby('model_id').count().show()\n",
    "\n",
    "\n",
    "# ## [Prophet forecasting](api/noob.forecasting.long_term.rst#noob.forecasting.prophet.base.ProphetForecastSpark)\n",
    "\n",
    "# +\n",
    "from noob.forecasting.prophet import ProphetForecast, ProphetForecastSpark\n",
    "from tests.forecasting.test_prophet import (\n",
    "    create_train_test_data as create_train_test_data_prophet)\n",
    "\n",
    "# ### train, test inputs\n",
    "\n",
    "# +\n",
    "train_data, test_data = create_train_test_data_prophet(spark)\n",
    "\n",
    "train_data.orderBy(\"ds\").show(2)\n",
    "test_data.orderBy(\"ds\", ascending=False).show(2)\n",
    "\n",
    "# ### create internal prophet model\n",
    "\n",
    "# +\n",
    "prophet = ProphetForecast(\n",
    "    model_params=None, # use defaults\n",
    "    period=\"day\",\n",
    "    coef_columns=[\"yearly\"],\n",
    "    normalize_coef_columns=True\n",
    ")\n",
    "\n",
    "# ### create prophet spark wrapper\n",
    "\n",
    "# +\n",
    "prophet_model = ProphetForecastSpark(prophet=prophet)\n",
    "\n",
    "# ### train & test\n",
    "\n",
    "# +\n",
    "prophet_args = {\n",
    "    \"group_by_cols\": [\"product_id\"],\n",
    "    \"cols_to_use\": [\"ds\", \"y\"],\n",
    "    \"target_col\": \"y\",\n",
    "    \"prediction_col\": \"yhat\",\n",
    "    \"additional_predictions\": [\"yearly\"],\n",
    "    \"remove_null_targets\": True\n",
    "}\n",
    "models, predictions = prophet_model.fit_predict(\n",
    "    train_data, test_data, **prophet_args)\n",
    "\n",
    "# ### trained models\n",
    "\n",
    "# +\n",
    "models.cache()\n",
    "models.show()\n",
    "\n",
    "# ### predictions\n",
    "\n",
    "# +\n",
    "predictions.show(2)\n",
    "\n",
    "# ### fit-predict straight (if your dataset contains both train & test records and you want to obtain predictions for training set as well)\n",
    "\n",
    "# +\n",
    "_, input_data = create_train_test_data_prophet(spark)\n",
    "\n",
    "prophet_args = {\n",
    "    \"group_by_cols\": [\"product_id\"],\n",
    "    \"cols_to_use\": [\"ds\", \"y\"],\n",
    "    \"target_col\": \"y\",\n",
    "    \"prediction_col\": \"yhat\",\n",
    "    \"additional_predictions\": [\"yearly\"],\n",
    "    \"remove_null_targets\": True\n",
    "}\n",
    "predictions = prophet_model.fit_predict_straight(input_data, **prophet_args)\n",
    "\n",
    "# ### predictions\n",
    "\n",
    "# +\n",
    "predictions.show(2)\n",
    "\n",
    "\n",
    "# ## [Store / Product Share Calculation](api/noob.postprocessing.breakdown.rst#noob.postprocessing.breakdown.base.BaseShareCalculation)\n",
    "\n",
    "from noob.postprocessing.breakdown import BaseShareCalculation\n",
    "from noob.utils.breakdown import clean_inactives, clean_demand, calculate_time_shares\n",
    "import pyspark.sql.functions as F\n",
    "from noob.utils.date import get_period_calendar\n",
    "from tests.postprocessing.test_share_calculation import ShareCalculationTest as tables\n",
    "tables.create_test_data(spark=spark, calendar=True)\n",
    "\n",
    "# ### Calculate store shares\n",
    "\n",
    "# +\n",
    "calendar = tables.calendar\n",
    "full_data = tables.data.select(\"store_id\", \"product_id\", \"week_index\", \"sales_quantity\")\n",
    "full_data = full_data.withColumn(\"hierarchy5\", F.lit(1))\n",
    "\n",
    "forecast = full_data.filter(F.col(\"week_index\") > 494)\n",
    "data = full_data.filter(F.col(\"week_index\") <= 494)\n",
    "forecast = forecast.groupby(\"product_id\", \"week_index\", \"hierarchy5\").agg(F.sum(F.col(\"sales_quantity\")).alias(\"prediction\"))\n",
    "# -\n",
    "\n",
    "forecast.select(\"week_index\").drop_duplicates().show()\n",
    "\n",
    "c, _ = get_period_calendar(calendar, \"week\")\n",
    "c.filter(F.col(\"date\") == \"2019-06-26\").show()\n",
    "\n",
    "target_date = \"2019-06-26\"\n",
    "period = \"week\"  # data is a weekly demand data (a property of the dataframe)\n",
    "share_level = \"store_id\"  # store share calculation\n",
    "forecast_level = [\"product_id\"]  # store shares are calculated for product_id level\n",
    "min_threshold = 10  # If a product is not sold at more than 10 stores, filter it out\n",
    "min_reliability_ratio = 0.3  # A product must at least 30% present, otherwise filter it out\n",
    "quantile_threshold = 0.97  # Demand smoothing first threshold\n",
    "quantile_reliable_threshold = 0.8  # Demand smoothing second threshold\n",
    "stdev_multip = 1.5  # Demand smoothing outlier std. dev. multiplier\n",
    "demand_col = \"sales_quantity\"  # Enter the column name of demand in the data\n",
    "\n",
    "share_calculator = BaseShareCalculation(\n",
    "    target_date=target_date,\n",
    "    calendar=calendar,\n",
    "    period=period,\n",
    "    share_level=share_level,\n",
    "    forecast_level=forecast_level,\n",
    "    demand_col=demand_col\n",
    ")\n",
    "\n",
    "common_offsets = [(53, 1), (5, 1) ]  # Last year & last 4 weeks are calculated without any changes for all the forecast horizon\n",
    "sliding_offsets = [(53, 49)]  # Last year next 4 weeks, slide with the forecast horizon\n",
    "\n",
    "shares = share_calculator.calculate_level_shares(\n",
    "    data=full_data,\n",
    "    common_offsets=common_offsets,\n",
    "    sliding_offsets=sliding_offsets\n",
    ")\n",
    "\n",
    "shares.orderBy(\"week_index\", \"store_id\", \"product_id\").show()\n",
    "\n",
    "# ### Example usage with an aggregated forecast\n",
    "\n",
    "broken_fc = forecast.join(shares, on=[\"product_id\", \"week_index\"], how=\"left\")\n",
    "\n",
    "broken_fc.show()\n",
    "\n",
    "broken_fc = broken_fc.withColumn(\"prediction\", F.col(\"sales_quantity\") * F.col(\"store_id_share_0\"))\n",
    "\n",
    "broken_fc.show(2)\n",
    "\n",
    "# ### Calculate time shares\n",
    "\n",
    "# + load example dataframes\n",
    "from tests.utils.test_breakdown import BreakdownUtilsTest as tables\n",
    "tables.create_test_data(spark)\n",
    "# -\n",
    "\n",
    "tables.daily_data.printSchema()\n",
    "\n",
    "shares = calculate_time_shares(\n",
    "    data=tables.daily_data,\n",
    "    calendar=tables.calendar,\n",
    "    period=\"day\",\n",
    "    wide_period=\"week\",\n",
    "    hierarchy=[\"store_id\", \"product_id\"],\n",
    "    demand_col=\"sales_quantity\"\n",
    ")\n",
    "shares.show(2)\n",
    "\n",
    "\"\"\"\n",
    "full_data = clean_inactives(\n",
    "    data=full_data,\n",
    "    period=period,\n",
    "    share_level=share_level,\n",
    "    forecast_level=forecast_level,\n",
    "    min_threshold=min_threshold,\n",
    "    min_reliability_ratio=min_reliability_ratio)\n",
    "full_data = clean_demand(\n",
    "    main_data=full_data,\n",
    "    share_level=share_level,\n",
    "    forecast_level=forecast_level,\n",
    "    cleaning_level=[\"hierarchy5\"],\n",
    "    quantile_threshold=quantile_threshold,\n",
    "    quantile_reliable_threshold=quantile_reliable_threshold,\n",
    "    stdev_multip=stdev_multip,\n",
    "    period=period,\n",
    "    demand_col=demand_col)\n",
    "\"\"\"\n",
    "\n",
    "# ## [Seasonality Calculation](api/noob.seasonality.rst#noob.seasonality.base.Seasonality)\n",
    "\n",
    "# ### Coefficients calculation\n",
    "# Data Preparation\n",
    "\n",
    "from noob.seasonality import Seasonality\n",
    "from pyspark.sql.types import FloatType\n",
    "from tests.seasonality.test_seasonality import SeasonalityTest as tables\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "tables.create_test_data(spark)\n",
    "sales_df = tables.sales_df\n",
    "holidays = tables.holidays\n",
    "calendar = tables.calendar\n",
    "aggregation_levels = tables.aggregation_levels\n",
    "promo = tables.promo\n",
    "join_cols = ['product_id']\n",
    "\n",
    "# Initialization\n",
    "\n",
    "seasonality = Seasonality(sales_df=sales_df,\n",
    "                          aggregation_levels=aggregation_levels,\n",
    "                          join_cols=join_cols,\n",
    "                          calendar=calendar)\n",
    "\n",
    "# no holiday, no muslim effect, single method, 20 week future coefs\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(methods=['prophet'],\n",
    "                                      seasonality_period=20)\n",
    "\n",
    "coefs.show(3)\n",
    "\n",
    "# Multiple methods\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(methods=['prophet', 'dummy'])\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "# with muslim effect (method defaults to prophet)\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(muslim=True)\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "# with holiday effects\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(df_extra=[holidays, promo])\n",
    "\n",
    "# holiday and muslim effects, (effects will be a tuple)\n",
    "# all model - holiday - hijri effect selection combinations are possible\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    df_extra=[holidays, promo],\n",
    "    muslim=True)\n",
    "\n",
    "# combined\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    methods=[{'method': 'prophet',\n",
    "              'name': 'prophet0', # name key defaults to method value\n",
    "              'params': {}}, # params defaults to empty dict\n",
    "             {'method': 'prophet',\n",
    "              'name': 'prophet_1',\n",
    "              'params': {'yearly_seasonality': 5}},\n",
    "             {'method': 'prophet',\n",
    "              'name': 'prophet_2',\n",
    "              'params': {'yearly_seasonality': 5, 'mcmc_samples': 0}},\n",
    "             {'method': 'dummy'}],\n",
    "    df_extra=[holidays],\n",
    "    holiday_params={'yearly_seasonality': 10,\n",
    "                    'seasonality_prior_scale': 1,\n",
    "                    'holidays_prior_scale': 10},\n",
    "    muslim=True,\n",
    "    muslim_params={'yearly_seasonality': 10,\n",
    "                   'seasonality_prior_scale': 1},\n",
    "    seasonality_period=20)\n",
    "\n",
    "coefs.cache()\n",
    "coefs.show(5)\n",
    "\n",
    "# ### Smart selection\n",
    "\n",
    "final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "    seasonality.smart_selection(coefs)\n",
    "final_coef.cache()\n",
    "\n",
    "agg_level_summary.show()\n",
    "\n",
    "final_coef.show(2)\n",
    "\n",
    "lltd_output.show(1)\n",
    "\n",
    "lltd_summary.show()\n",
    "\n",
    "# ### Running seasonality calculations with updated holidays\n",
    "with tempfile.TemporaryDirectory() as output_path:\n",
    "    join_cols = ['product_id']\n",
    "    # calculating seasonality coefficients in a normal fashion\n",
    "    seasonality = Seasonality(sales_df=sales_df,\n",
    "                            aggregation_levels=aggregation_levels,\n",
    "                            join_cols=join_cols,\n",
    "                            calendar=calendar)\n",
    "    coefs, effects = seasonality.calculate_coef(df_extra=[holidays, promo])\n",
    "    \n",
    "    coefs.write.parquet(os.path.join(output_path, \"coefs\"))\n",
    "    coefs = spark.read.parquet(os.path.join(output_path, \"coefs\"))\n",
    "\n",
    "    effects.write.parquet(os.path.join(output_path, \"effects\"))\n",
    "    effects = spark.read.parquet(os.path.join(output_path, \"effects\"))\n",
    "    \n",
    "    final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "        seasonality.smart_selection(coefs)\n",
    "\n",
    "    final_coef.write.parquet(os.path.join(output_path, \"final_coef\"))\n",
    "    final_coef = spark.read.parquet(os.path.join(output_path, \"final_coef\"))\n",
    "    \n",
    "    lltd_summary.write.parquet(os.path.join(output_path, \"lltd_summary\"))\n",
    "    lltd_summary = spark.read.parquet(os.path.join(output_path, \"lltd_summary\"))\n",
    "    \n",
    "    agg_level_summary.write.parquet(os.path.join(output_path, \"agg_level_summary\"))\n",
    "    agg_level_summary = spark.read.parquet(os.path.join(output_path, \"agg_level_summary\"))\n",
    "\n",
    "    # assumption is that user stores coefs, effects, lltd_summary and\n",
    "    # agg_level_summary dataframes since these will be needed to update final\n",
    "    # coefficients with new holidays\n",
    "\n",
    "    new_promo = promo.unionByName(\n",
    "        spark.createDataFrame(\n",
    "            [(2020, 7, 2, \"low_promo\")],\n",
    "            [\"year\", \"week\", \"product_id\", \"promo\"]))\n",
    "    # let's assume above holidays are recently announced and we want to update\n",
    "    # our final predictions accordingly\n",
    "\n",
    "    new_final_coef = seasonality.update_coef(\n",
    "        coefs=coefs,\n",
    "        effects=effects,\n",
    "        df_extra=[holidays, new_promo],\n",
    "        lltd_summary=lltd_summary,\n",
    "        agg_level_summary=agg_level_summary)\n",
    "    new_final_coef.cache()\n",
    "\n",
    "    # to see whether it worked or not\n",
    "    filter_cond = \"product_id == 2 and year == 2020 and week == 7\"\n",
    "\n",
    "    # observe that for this product_id, AggLevel2=1 is satisfactory agg_level\n",
    "    agg_level_summary.filter('product_id == 2').show()\n",
    "    # observe promo and holiday effects\n",
    "    effects.filter('target_column = 1 and level = \"AggLevel2\"').show()\n",
    "\n",
    "    final_coef.filter(filter_cond).show()\n",
    "    new_final_coef.filter(filter_cond).show()\n",
    "\n",
    "    # ### I/O Control\n",
    "\n",
    "    summary_prev = seasonality.generate_summary(final_coef)\n",
    "    summary_prev.cache()\n",
    "    summary = seasonality.generate_summary(new_final_coef)\n",
    "    summary.cache()\n",
    "\n",
    "    seasonality.control(summary_prev, summary)\n",
    "\n",
    "# ## [Seasonality Calculation v2](api/noob.seasonality.rst#noob.seasonality.seasonality.Seasonality)\n",
    "\n",
    "# ### Coefficients calculation\n",
    "\n",
    "# #### Data Preparation\n",
    "\n",
    "from noob.seasonality.seasonality import Seasonality as SeasonalityV2\n",
    "from tests.seasonality.test_seasonality_new import SeasonalityTest as tables\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "tables.create_test_data(spark)\n",
    "sales_df = tables.sales_df\n",
    "holidays = tables.holidays\n",
    "agg_holidays = tables.agg_holidays\n",
    "calendar = tables.calendar\n",
    "aggregation_levels = tables.aggregation_levels\n",
    "join_cols = ['product_id']\n",
    "holiday_names = [\"NewYearsDay\", \"high_promo\", \"low_promo\"]\n",
    "\n",
    "# Initialization\n",
    "# +\n",
    "\n",
    "seasonality = SeasonalityV2(\n",
    "    join_cols=join_cols,\n",
    "    aggregation_levels=aggregation_levels,\n",
    "    calendar=calendar,\n",
    "    target_date=tables.target_date, # last positive sales date\n",
    "    period=\"week\",\n",
    ")\n",
    "\n",
    "# no holiday, no hijri effect, single method, 20 week future coefs\n",
    "# +\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(\n",
    "    sales_df,\n",
    "    methods=['prophet'],\n",
    "    seasonality_period=20\n",
    ")\n",
    "\n",
    "coefs.show(3)\n",
    "\n",
    "# Multiple methods\n",
    "# +\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(sales_df, methods=['prophet', 'dummy'])\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "# with hijri effect (method defaults to prophet)\n",
    "# +\n",
    "\n",
    "coefs, _ = seasonality.calculate_coef(sales_df, hijri=True)\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "# with holiday effects\n",
    "# +\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    sales_df, agg_holidays=agg_holidays, holiday_names=holiday_names)\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "# holiday and hijri effects\n",
    "# +\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    sales_df,\n",
    "    agg_holidays=agg_holidays,\n",
    "    holiday_names=holiday_names,\n",
    "    hijri=True\n",
    ")\n",
    "\n",
    "effects.show()\n",
    "\n",
    "# granular level effect cleaning - re-calculate agggregation level - notice the product_id in effects\n",
    "# +\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    sales_df,\n",
    "    holidays=holidays,\n",
    "    agg_holidays=agg_holidays,\n",
    "    holiday_names=holiday_names,\n",
    "    normalize_before_agg=True\n",
    ")\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "effects.show()\n",
    "\n",
    "# granular level effect cleaning - aggregate calculated effects to aggregation levels\n",
    "# +\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    sales_df,\n",
    "    holidays=holidays,\n",
    "    agg_holidays=agg_holidays,\n",
    "    holiday_names=holiday_names,\n",
    "    normalize_before_agg=True,\n",
    "    aggregate_effects=True\n",
    ")\n",
    "\n",
    "coefs.show(2)\n",
    "\n",
    "effects.show()\n",
    "\n",
    "# multiple methods\n",
    "# +\n",
    "\n",
    "methods = [\n",
    "    {\n",
    "        \"method\": \"prophet\",\n",
    "        \"name\": \"prophet_0\",\n",
    "    },\n",
    "    {\n",
    "        \"method\": \"prophet\",\n",
    "        \"name\": \"prophet_1\",\n",
    "        \"params\": {\n",
    "            **SeasonalityV2.default_prophet_params,\n",
    "            \"yearly_seasonality\": 5\n",
    "        }\n",
    "    },\n",
    "    {\"method\": \"dummy\"}\n",
    "]\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(\n",
    "    sales_df,\n",
    "    methods=methods,\n",
    "    holidays=holidays,\n",
    "    agg_holidays=agg_holidays,\n",
    "    holiday_names=holiday_names,\n",
    "    normalize_before_agg=True,\n",
    "    seasonality_period=20)\n",
    "\n",
    "coefs.cache()\n",
    "effects.cache()\n",
    "\n",
    "coefs.show(5)\n",
    "\n",
    "# ### Smart selection\n",
    "\n",
    "# #### Smart Selection in aggregated mode\n",
    "\n",
    "final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "    seasonality.smart_selection(sales_df, coefs, mode=\"aggregated\")\n",
    "final_coef.cache()\n",
    "\n",
    "agg_level_summary.show()\n",
    "\n",
    "final_coef.show(2)\n",
    "\n",
    "lltd_output.show(1)\n",
    "\n",
    "lltd_summary.show()\n",
    "\n",
    "# #### Smart Selection in individual mode\n",
    "\n",
    "final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "    seasonality.smart_selection(sales_df, coefs, mode=\"individual\")\n",
    "final_coef.cache()\n",
    "\n",
    "agg_level_summary.show()\n",
    "\n",
    "final_coef.show(2)\n",
    "\n",
    "lltd_output.show(1)\n",
    "\n",
    "lltd_summary.show()\n",
    "\n",
    "# ### Seasonality predict mode with new items & updated holidays\n",
    "\n",
    "# +\n",
    "new_aggregation_levels = aggregation_levels.unionByName(\n",
    "    spark.createDataFrame(\n",
    "        [\n",
    "            (3, \"h4_3\", \"h3\"),\n",
    "            (4, \"h4_2\", \"h3\")\n",
    "        ],\n",
    "        [\"product_id\", \"AggLevel1\", \"AggLevel2\"])\n",
    ")\n",
    "new_holidays = agg_holidays.filter(\n",
    "    (F.col(\"date\") != \"2019-07-08\") &\n",
    "    (F.col(\"holiday\") != \"low_promo\")\n",
    ").unionByName(spark.createDataFrame(\n",
    "    [(\"2019-07-08\", 2019, 28, \"h4_1\", \"agg_level_1\", \"high_promo\")],\n",
    "    [\"date\", \"year\", \"week\", \"target_column\", \"level\", \"holiday\"])\n",
    ")\n",
    "holiday_diff = new_holidays\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as output_path:\n",
    "    coefs.write.parquet(os.path.join(output_path, \"coefs\"))\n",
    "    coefs = spark.read.parquet(os.path.join(output_path, \"coefs\"))\n",
    "\n",
    "    effects.write.parquet(os.path.join(output_path, \"effects\"))\n",
    "    effects = spark.read.parquet(os.path.join(output_path, \"effects\"))\n",
    "\n",
    "    final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "        seasonality.smart_selection(sales_df, coefs, mode=\"individual\")\n",
    "\n",
    "    final_coef.write.parquet(os.path.join(output_path, \"final_coef\"))\n",
    "    final_coef = spark.read.parquet(\n",
    "        os.path.join(output_path, \"final_coef\"))\n",
    "\n",
    "    agg_level_summary.write.parquet(\n",
    "        os.path.join(output_path, \"agg_level_summary\"))\n",
    "    agg_level_summary = spark.read.parquet(\n",
    "        os.path.join(output_path, \"agg_level_summary\"))\n",
    "\n",
    "    lltd_summary.write.parquet(os.path.join(output_path, \"lltd_summary\"))\n",
    "    lltd_summary = spark.read.parquet(\n",
    "        os.path.join(output_path, \"lltd_summary\"))\n",
    "\n",
    "    seasonality = SeasonalityV2(\n",
    "        join_cols=join_cols,\n",
    "        aggregation_levels=new_aggregation_levels,\n",
    "        calendar=calendar,\n",
    "        target_date=tables.target_date,\n",
    "        period=\"week\",\n",
    "    )\n",
    "\n",
    "    predicted_coefs = seasonality.predict(\n",
    "        final_coef,\n",
    "        agg_level_summary,\n",
    "        coefs,\n",
    "        lltd_summary,\n",
    "        effects,\n",
    "        holiday_diff,\n",
    "        holiday_names\n",
    "    )\n",
    "\n",
    "    predicted_coefs.show(2)\n",
    "\n",
    "# ### I/O Control\n",
    "# +\n",
    "\n",
    "seasonality = SeasonalityV2(\n",
    "    join_cols=join_cols,\n",
    "    aggregation_levels=aggregation_levels,\n",
    "    calendar=calendar,\n",
    "    target_date=tables.target_date, # last positive sales date\n",
    "    period=\"week\",\n",
    ")\n",
    "\n",
    "coefs, effects = seasonality.calculate_coef(sales_df)\n",
    "coefs.cache()\n",
    "\n",
    "final_coef, lltd_output, lltd_summary, agg_level_summary = \\\n",
    "    seasonality.smart_selection(sales_df, coefs, mode=\"aggregated\")\n",
    "final_coef.cache()\n",
    "new_final_coef = final_coef\n",
    "\n",
    "summary_prev = seasonality.generate_summary(final_coef)\n",
    "summary_prev.cache()\n",
    "summary = seasonality.generate_summary(new_final_coef)\n",
    "summary.cache()\n",
    "\n",
    "seasonality.control(summary_prev, summary)\n",
    "\n",
    "\n",
    "# ## Attribute Generation\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from noob.preprocessing.attribute_generation import PromoAttributeGenerator\n",
    "\n",
    "# #### [PromoAttributeGenerator](api/noob.preprocessing.attribute_generation.rst#noob.preprocessing.attribute_generation.promo.PromoAttributeGenerator)\n",
    "# Calculates promo attributes.\n",
    "# Below generate dummy sales, lostsales and seasonality dataframes.\n",
    "\n",
    "data = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 0, 10, 1,), (1, 0, 15, 2,), (1, 1, 30, 3,),\n",
    "        (1, 0, 5, 4,), (1, 0, 10, 5,), (1, 0, 5, 6,),\n",
    "        (1, 1, 40, 7,), (1, 1, 30, 8,), (1, 0, 10, 9,),\n",
    "        (1, 0, 5, 10,), (2, 1, 20, 1,), (2, 1, 15, 2,),\n",
    "        (2, 1, 10, 3,), (2, 1, 15, 4,), (2, 0, 10, 5,),\n",
    "        (2, 0, 5, 6,), (2, 0, 10, 7,), (2, 0, 5, 8,),\n",
    "        (2, 0, 10, 9,), (2, 0, 5, 10,),\n",
    "    ],\n",
    "    [\"product_id\", \"is_promo\", \"sales_quantity\", \"week_index\"]\n",
    ")\n",
    "lostsales_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 10, 1,), (1, 15, 2,), (1, 30, 3,),\n",
    "        (1, 5, 4,), (1, 10, 5,), (1, 5, 6,),\n",
    "        (1, 40, 7,), (1, 30, 8,), (1, 10, 9,),\n",
    "        (1, 5, 10,), (2, 20, 1,), (2, 15, 2,),\n",
    "        (2, 10, 3,), (2, 15, 4,), (2, 10, 5,),\n",
    "        (2, 5, 6,), (2, 10, 7,), (2, 5, 8,),\n",
    "        (2, 10, 9,), (2, 5, 10,),\n",
    "    ],\n",
    "    [\"product_id\", \"lostsales\", \"week_index\"]\n",
    ")\n",
    "seasonality_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 1.0, 1,), (1, 1.5, 2,), (1, 1.7, 3,),\n",
    "        (1, 1.5, 4,), (1, 1.0, 5,), (1, 0.8, 6,),\n",
    "        (1, 0.6, 7,), (1, 0.8, 8,), (1, 1.0, 9,),\n",
    "        (1, 1.0, 10,), (2, 1.5, 1,), (2, 1.7, 2,),\n",
    "        (2, 1.5, 3,), (2, 1.0, 4,), (2, 0.8, 5,),\n",
    "        (2, 0.6, 6,), (2, 0.8, 7,), (2, 1.0, 8,),\n",
    "        (2, 1.2, 9,), (2, 1.0, 10,),\n",
    "    ],\n",
    "    [\"product_id\", \"multip\", \"week_index\"]\n",
    ")\n",
    "\n",
    "data.show(5)\n",
    "\n",
    "lostsales_df.show(5)\n",
    "\n",
    "seasonality_df.show(5)\n",
    "\n",
    "pag = PromoAttributeGenerator(level=[\"product_id\"], period=\"week\")\n",
    "\n",
    "data = pag.preprocess(data, lostsales_df, seasonality_df)\n",
    "\n",
    "# Preprocessing joins lostsales data and calculates demand by sales quantity and demand columns.\n",
    "# Then, seasonality is joined and demand is divided by multip column to get normalized demand\n",
    "\n",
    "data.show(5)\n",
    "\n",
    "result = pag.generate_promo_frequency(\n",
    "    data=data,\n",
    "    promo_col=\"is_promo\",\n",
    "    demand_col=\"normal_demand\",\n",
    "    last_n_periods=5,\n",
    "    cut_borders=(0, 0.10, 0.40, 1),\n",
    "    cut_labels=(\"low\", \"medium\", \"high\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n",
    "result = pag.generate_promo_sensitivity(\n",
    "    data=data,\n",
    "    promo_col=\"is_promo\",\n",
    "    demand_col=\"normal_demand\",\n",
    "    last_n_periods=5,\n",
    "    cut_borders=(0, 1.5, 3, np.inf),\n",
    "    cut_labels=(\"low\", \"medium\", \"high\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from warp.spark.date import create_calendar_table\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# #### [SeasonalAttributeGenerator](api/noob.preprocessing.attribute_generation.rst#noob.preprocessing.attribute_generation.seasonal.SeasonalAttributeGenerator)\n",
    "# Calculates seasonal attributes.\n",
    "#\n",
    "# Let's create a dummy seasonality dataframe first.\n",
    "# All multipliers are 0.5 when week < 45; otherwise 10.0\n",
    "\n",
    "data = [(1, 2020, w, 0.5) if w < 45 else\n",
    "        (1, 2020, w, 10.0) for w in range(1, 54)]\n",
    "columns = [\"product_id\", \"year\", \"week\", \"multiplier\"]\n",
    "seasonality = spark.createDataFrame(data, columns)\n",
    "seasonality.orderBy(F.desc(\"week\")).show(12)\n",
    "seasonality.cache()\n",
    "\n",
    "# Create a SeasonalAttributeGenerator instance with year = 2020 and smoothing_span = 3.\n",
    "\n",
    "from noob.preprocessing.attribute_generation import SeasonalAttributeGenerator\n",
    "calendar = create_calendar_table().select(\n",
    "    F.col(\"date\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_year\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_week\").alias(\"week\").cast(\"int\"),\n",
    "    F.col(\"month\").cast(\"string\"),\n",
    "    F.col(\"day_of_week\").cast(\"string\"),\n",
    "    F.col(\"iso_8601_year\").alias(\"year\").cast(\"int\"),\n",
    "    F.col(\"week_start_date\").cast(\"string\"),\n",
    "    F.col(\"week_end_date\").cast(\"string\"),\n",
    ")\n",
    "seas = SeasonalAttributeGenerator(\n",
    "    calendar,\n",
    "    year=2020,\n",
    "    smoothing_span=0,\n",
    "    labels=(\"not_seasonal\", \"seasonal\", \"highly_seasonal\"),\n",
    "    borders=(0, 0.25, 0.75, np.inf)\n",
    ")\n",
    "\n",
    "#\n",
    "# Let's generate the attributes\n",
    "\n",
    "results = seas.generate(seasonality)\n",
    "\n",
    "# * We see that this is a highly seasonal product and it has only a single season.\n",
    "#\n",
    "# * The season starts at week 45 and ends at week 1.\n",
    "#\n",
    "# * `intensity` column is created which indicates how strong the seasonality of a product is.\n",
    "#\n",
    "# Products are labeled as according to this `intensity` column. Since the `intensity` 1.70 is greater than\n",
    "# 0.75, this product is classified as `highly_seasonal`\n",
    "\n",
    "results.show()\n",
    "\n",
    "# #### Implement your own intensity calculation\n",
    "#  By default, `intensity` is calculated as:\n",
    "#\n",
    "#  $\\ std(multiplier) / mean(multiplier)$\n",
    "#\n",
    "#  You can write you own `intensity` function.\n",
    "#  Let's say that we want to define a new `intensity` function as follows.\n",
    "#\n",
    "#  $\\ max(multiplier)$\n",
    "#\n",
    "#  You can simply override the `get_seasonal_intensity_method` and return the seasonality\n",
    "#  dataframe with the `intensity` column.\n",
    "#\n",
    "#  Higher `intensity` should indicate higher seasonality. Feel free to experiment with\n",
    "#  more sophisticated `intensity` definitions.\n",
    "#\n",
    "\n",
    "# +\n",
    "from pyspark.sql import Window\n",
    "\n",
    "class MyClass(SeasonalAttributeGenerator):\n",
    "    def get_seasonal_intensity(self, seasonality):\n",
    "        window = Window.partitionBy(\"product_id\")\n",
    "        seasonality = seasonality.withColumn(\n",
    "            \"intensity\", F.max(\"multiplier\").over(window))\n",
    "        return seasonality\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "# We can now see that `intensity` is 10.0. Again, this product is\n",
    "# classified as `highly_seasonal` since 10.0 > 0.75\n",
    "\n",
    "my_attr_generator = MyClass(\n",
    "    calendar=calendar,\n",
    "    year=2020,\n",
    "    smoothing_span=0,\n",
    "    labels=(\"not_seasonal\", \"seasonal\", \"highly_seasonal\"),\n",
    "    borders=(0, 0.25, 0.75, np.inf)\n",
    ")\n",
    "my_attr_generator.generate(seasonality).show()\n",
    "\n",
    "# To display the time dependent features in \"month-date\" format, just pass display_date=True.\n",
    "\n",
    "from warp.spark.date import create_calendar_table\n",
    "calendar = create_calendar_table()\n",
    "results = seas.generate(seasonality, display_date=True)\n",
    "\n",
    "results.show()\n",
    "\n",
    "# #### [ProductLifeAttributeGenerator](api/noob.preprocessing.attribute_generation.rst#noob.preprocessing.attribute_generation.product_life.ProductLifeAttributeGenerator)\n",
    "\n",
    "\n",
    "from noob.preprocessing.attribute_generation import (\n",
    "    ProductLifeAttributeGenerator)\n",
    "from noob.utils.date import get_period_calendar\n",
    "from tests.preprocessing.test_attribute_generator import AttributeGeneratorTest\n",
    "\n",
    "AttributeGeneratorTest.create_test_data(spark)\n",
    "\n",
    "daily_data = AttributeGeneratorTest.data\n",
    "calendar, _ = get_period_calendar(AttributeGeneratorTest.calendar, \"day\")\n",
    "plag = ProductLifeAttributeGenerator(target_date=\"2019-01-01\",\n",
    "                                        level=[\"product_id\"],\n",
    "                                        calendar=calendar,\n",
    "                                        newness_threshold_days=300,\n",
    "                                        border=2,\n",
    "                                        last_n_days=50)\n",
    "result = plag.generate_product_life_status(daily_data)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+----+\n",
      "|store_id|product_id|      date|actual|prediction|forecast_horizon|error|abs_error|model_id|attr|\n",
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+----+\n",
      "|       1|         1|2020-02-02|     5|         2|               1|   -3|        3|       a|   1|\n",
      "|       1|         3|2020-02-02|     5|         6|               1|    1|        1|       a|   1|\n",
      "|       1|         1|2020-02-09|     5|         3|               1|   -2|        2|       a|   1|\n",
      "|       1|         3|2020-02-09|     5|         4|               1|   -1|        1|       a|   1|\n",
      "|       1|         1|2020-02-02|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         2|2020-02-02|     5|         5|               1|    0|        0|       b|   1|\n",
      "|       1|         3|2020-02-02|     5|         4|               1|   -1|        1|       b|   1|\n",
      "|       1|         1|2020-02-03|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         3|2020-02-03|     5|         2|               1|   -3|        3|       b|   1|\n",
      "|       1|         1|2020-02-04|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         3|2020-02-04|     5|         2|               1|   -3|        3|       b|   1|\n",
      "|       1|         1|2020-02-05|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         3|2020-02-05|     5|         2|               1|   -3|        3|       b|   1|\n",
      "|       1|         1|2020-02-06|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         3|2020-02-06|     5|         0|               1|   -5|        5|       b|   1|\n",
      "|       1|         1|2020-02-09|     5|         8|               1|    3|        3|       b|   1|\n",
      "|       1|         3|2020-02-09|     5|         3|               1|   -2|        2|       b|   1|\n",
      "|       1|         1|2020-02-02|     5|         5|               1|    0|        0|       c|   1|\n",
      "|       1|         2|2020-02-02|     5|         5|               1|    0|        0|       c|   1|\n",
      "|       1|         3|2020-02-02|     5|         5|               1|    0|        0|       c|   1|\n",
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'noob.tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3c43d0e0a5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnoob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecasting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSmartSelect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnoob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmartselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_smartselect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSmartSelectTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSmartSelectTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'noob.tests'"
     ]
    }
   ],
   "source": [
    "from noob.forecasting.smart_selection import BaseSmartSelect\n",
    "from noob.tests.smartselect.test_smartselect import SmartSelectTest\n",
    "\n",
    "SmartSelectTest.create_test_data(spark)\n",
    "\n",
    "error = SmartSelectTest.train\n",
    "error_ = SmartSelectTest.test\n",
    "smart = BaseSmartSelect(\n",
    "    [\"attr\"])\n",
    "error_ranked = smart.calculate_metrics(error,metric_scale=False)\n",
    "smart_select_forecasts = smart.get_smart_selection(error_ranked, error_)\n",
    "smart_select_forecasts.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[store_id: bigint, product_id: bigint, date: string, prediction: bigint, model_id: string, attr: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"store_id\",\n",
    "    \"product_id\",\n",
    "    \"date\",\n",
    "    \"actual\",\n",
    "    \"prediction\",\n",
    "    \"forecast_horizon\",\n",
    "    \"error\",\n",
    "    \"abs_error\",\n",
    "    \"model_id\",\n",
    "    \"attr\",\n",
    "]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-02\", 5, 2, 1, -3, 3, \"a\", 1),\n",
    "    (1, 3, \"2020-02-02\", 5, 6, 1, 1, 1, \"a\", 1),\n",
    "    (1, 1, \"2020-02-09\", 5, 3, 1, -2, 2, \"a\", 1),\n",
    "    (1, 3, \"2020-02-09\", 5, 4, 1, -1, 1, \"a\", 1),\n",
    "\n",
    "    (1, 1, \"2020-02-02\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "    (1, 2, \"2020-02-02\", 5, 5, 1, 0, 0, \"b\", 1),\n",
    "\n",
    "    (1, 3, \"2020-02-02\", 5, 4, 1, -1, 1, \"b\", 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "    (1, 3, \"2020-02-03\", 5, 2, 1, -3, 3, \"b\", 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "    (1, 3, \"2020-02-04\", 5, 2, 1, -3, 3, \"b\", 1),\n",
    "    (1, 1, \"2020-02-05\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "    (1, 3, \"2020-02-05\", 5, 2, 1, -3, 3, \"b\", 1),\n",
    "    (1, 1, \"2020-02-06\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "    (1, 3, \"2020-02-06\", 5, 0, 1, -5, 5, \"b\", 1),\n",
    "    (1, 1, \"2020-02-09\", 5, 8, 1, 3, 3, \"b\", 1),\n",
    "    (1, 3, \"2020-02-09\", 5, 3, 1, -2, 2, \"b\", 1),\n",
    "\n",
    "    (1, 1, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 2, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 3, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 4, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 5, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 6, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 7, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 8, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 9, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "    (1, 10, \"2020-02-02\", 5, 5, 1, 0, 0, \"c\", 1),\n",
    "\n",
    "    (1, 1, \"2020-02-02\", 5, None, 1, None, None, \"d\", 1),\n",
    "    (1, 1, \"2020-02-03\", 5, None, 1, None, None, \"d\", 1),\n",
    "    (1, 1, \"2020-02-09\", 5, None, 1, None, None, \"d\", 1),\n",
    "]\n",
    "train = F.broadcast(spark.createDataFrame(data, columns))\n",
    "train.cache()\n",
    "columns = [\n",
    "    \"store_id\",\n",
    "    \"product_id\",\n",
    "    \"date\",\n",
    "    \"prediction\",\n",
    "    \"model_id\",\n",
    "    \"attr\",\n",
    "]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-09\", None, \"a\", 1),\n",
    "    (1, 1, \"2020-02-09\", None, \"b\", 1),\n",
    "    (1, 1, \"2020-02-09\", 8, \"c\", 1),\n",
    "    (1, 1, \"2020-02-09\", 4, \"d\", 1),\n",
    "    (1, 2, \"2020-02-09\", 2, \"d\", 1),\n",
    "    (1, 2, \"2020-02-09\", 3, \"a\", 1),\n",
    "    (1, 3, \"2020-02-09\", 6, \"a\", 1),\n",
    "    (1, 3, \"2020-02-09\", None, \"b\", 1),\n",
    "    (1, 3, \"2020-02-09\", 7, \"c\", 1),\n",
    "]\n",
    "test = F.broadcast(spark.createDataFrame(data, columns))\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+-----------+-----------+\n",
      "|store_id|product_id|      date|actual|prediction|forecast_horizon|error|abs_error|model_id|time_attr_0|time_attr_1|\n",
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+-----------+-----------+\n",
      "|       1|         1|2020-02-01|     5|         5|               1|    0|        0|       a|          0|          0|\n",
      "|       1|         1|2020-02-01|     5|         0|               1|    5|        5|       b|          0|          0|\n",
      "|       1|         1|2020-02-01|     5|         2|               1|    3|        3|       c|          0|          0|\n",
      "|       1|         1|2020-02-01|     5|         2|               1|    3|        3|       d|          0|          0|\n",
      "|       1|         1|2020-02-02|     5|         0|               1|    5|        5|       a|          1|          0|\n",
      "|       1|         1|2020-02-02|     5|         5|               1|    0|        0|       b|          1|          0|\n",
      "|       1|         1|2020-02-02|     5|         4|               1|    1|        1|       c|          1|          0|\n",
      "|       1|         1|2020-02-02|     5|         3|               1|    2|        2|       d|          1|          0|\n",
      "|       1|         1|2020-02-03|     5|         3|               1|    2|        2|       a|          0|          1|\n",
      "|       1|         1|2020-02-03|     5|         2|               1|    3|        3|       b|          0|          1|\n",
      "|       1|         1|2020-02-03|     5|         5|               1|    0|        0|       c|          0|          1|\n",
      "|       1|         1|2020-02-03|     5|         1|               1|    4|        4|       d|          0|          1|\n",
      "|       1|         1|2020-02-04|     5|         2|               1|    3|        3|       a|          1|          1|\n",
      "|       1|         1|2020-02-04|     5|         0|               1|    5|        5|       b|          1|          1|\n",
      "|       1|         1|2020-02-04|     5|         3|               1|    2|        2|       c|          1|          1|\n",
      "|       1|         1|2020-02-04|     5|         5|               1|    0|        0|       d|          1|          1|\n",
      "+--------+----------+----------+------+----------+----------------+-----+---------+--------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-20 02:14:42] {noob.forecasting.smart_selection.base.BaseSmartSelect:logger.py:203} WARNING Environment variable ROCKS_ENV set as , which is in the denylist. Please set something else. Denylist: ['', 'local', 'debug']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-----------+-----------+\n",
      "|store_id|product_id|      date|prediction|model_id|time_attr_0|time_attr_1|\n",
      "+--------+----------+----------+----------+--------+-----------+-----------+\n",
      "|       1|         1|2020-02-09|         1|       a|          0|          0|\n",
      "|       1|         1|2020-02-09|         3|       b|          0|          0|\n",
      "|       1|         1|2020-02-09|         8|       c|          0|          0|\n",
      "|       1|         1|2020-02-09|         4|       d|          0|          0|\n",
      "|       1|         1|2020-02-10|         1|       a|          1|          0|\n",
      "|       1|         1|2020-02-10|         3|       b|          1|          0|\n",
      "|       1|         1|2020-02-10|         8|       c|          1|          0|\n",
      "|       1|         1|2020-02-10|         4|       d|          1|          0|\n",
      "|       1|         1|2020-02-11|         1|       a|          0|          1|\n",
      "|       1|         1|2020-02-11|         3|       b|          0|          1|\n",
      "|       1|         1|2020-02-11|         8|       c|          0|          1|\n",
      "|       1|         1|2020-02-11|         4|       d|          0|          1|\n",
      "|       1|         1|2020-02-12|         1|       a|          1|          1|\n",
      "|       1|         1|2020-02-12|         3|       b|          1|          1|\n",
      "|       1|         1|2020-02-12|         8|       c|          1|          1|\n",
      "|       1|         1|2020-02-12|         4|       d|          1|          1|\n",
      "+--------+----------+----------+----------+--------+-----------+-----------+\n",
      "\n",
      "+-----------+-----------+--------------------+\n",
      "|time_attr_0|time_attr_1|    models_and_ranks|\n",
      "+-----------+-----------+--------------------+\n",
      "|          1|          0|[b -> 1, c -> 2, ...|\n",
      "|          1|          1|[d -> 1, c -> 2, ...|\n",
      "|          0|          1|[c -> 1, a -> 2, ...|\n",
      "|          0|          0|[a -> 1, d -> 2, ...|\n",
      "+-----------+-----------+--------------------+\n",
      "\n",
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "|      date|store_id|product_id|original_id|prediction|reliable|    model_id|\n",
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "|2020-02-10|       1|         1|          b|         3|       2|smart_select|\n",
      "|2020-02-12|       1|         1|          d|         4|       2|smart_select|\n",
      "|2020-02-11|       1|         1|          c|         8|       2|smart_select|\n",
      "|2020-02-09|       1|         1|          a|         1|       2|smart_select|\n",
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"store_id\", \"product_id\", \"date\", \"actual\", \"prediction\",\n",
    "    \"forecast_horizon\", \"error\", \"abs_error\", \"model_id\", \"time_attr_0\",\n",
    "    \"time_attr_1\"\n",
    "]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-01\", 5, 5, 1, 0, 0, \"a\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 0, 1, 5, 5, \"b\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 2, 1, 3, 3, \"c\", 0, 0),\n",
    "    (1, 1, \"2020-02-01\", 5, 2, 1, 3, 3, \"d\", 0, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-02\", 5, 0, 1, 5, 5, \"a\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 5, 1, 0, 0, \"b\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 4, 1, 1, 1, \"c\", 1, 0),\n",
    "    (1, 1, \"2020-02-02\", 5, 3, 1, 2, 2, \"d\", 1, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-03\", 5, 3, 1, 2, 2, \"a\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 2, 1, 3, 3, \"b\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 5, 1, 0, 0, \"c\", 0, 1),\n",
    "    (1, 1, \"2020-02-03\", 5, 1, 1, 4, 4, \"d\", 0, 1),\n",
    "\n",
    "    (1, 1, \"2020-02-04\", 5, 2, 1, 3, 3, \"a\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 0, 1, 5, 5, \"b\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 3, 1, 2, 2, \"c\", 1, 1),\n",
    "    (1, 1, \"2020-02-04\", 5, 5, 1, 0, 0, \"d\", 1, 1),\n",
    "]\n",
    "train = spark.createDataFrame(data, columns)\n",
    "train.show()\n",
    "\n",
    "# #### Prepare test data\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"date\", \"prediction\", \"model_id\",\n",
    "           \"time_attr_0\", \"time_attr_1\"]\n",
    "data = [\n",
    "    (1, 1, \"2020-02-09\", 1, \"a\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 3, \"b\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 8, \"c\", 0, 0),\n",
    "    (1, 1, \"2020-02-09\", 4, \"d\", 0, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-10\", 1, \"a\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 3, \"b\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 8, \"c\", 1, 0),\n",
    "    (1, 1, \"2020-02-10\", 4, \"d\", 1, 0),\n",
    "\n",
    "    (1, 1, \"2020-02-11\", 1, \"a\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 3, \"b\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 8, \"c\", 0, 1),\n",
    "    (1, 1, \"2020-02-11\", 4, \"d\", 0, 1),\n",
    "\n",
    "    (1, 1, \"2020-02-12\", 1, \"a\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 3, \"b\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 8, \"c\", 1, 1),\n",
    "    (1, 1, \"2020-02-12\", 4, \"d\", 1, 1),\n",
    "]\n",
    "test = spark.createDataFrame(data, columns)\n",
    "test.show()\n",
    "\n",
    "# #### Create smart selection model\n",
    "\n",
    "smart_selection = BaseSmartSelect(group_by_cols=[\"time_attr_0\", \"time_attr_1\"])\n",
    "\n",
    "# #### Find model ranks\n",
    "\n",
    "metrics = smart_selection.calculate_metrics(train)\n",
    "metrics.show()\n",
    "\n",
    "# #### Make prediction\n",
    "\n",
    "result = smart_selection.get_smart_selection(metrics, test)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = train\n",
    "error_ = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erdemb/libs/warp/warp/core/logger.py:125: DeprecationWarning: This method is deprecated. Use `register_elasticsearch_handler_spark`.\n",
      "  warnings.warn(\n",
      "[2021-02-19 22:54:24] {noob.forecasting.smart_selection.base.BaseSmartSelect:logger.py:203} WARNING Environment variable ROCKS_ENV set as , which is in the denylist. Please set something else. Denylist: ['', 'local', 'debug']\n"
     ]
    }
   ],
   "source": [
    "smart = BaseSmartSelect(\n",
    "    [\"attr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-19 22:54:33] {noob.forecasting.smart_selection.base.BaseSmartSelect:logger.py:203} WARNING Environment variable ROCKS_ENV set as , which is in the denylist. Please set something else. Denylist: ['', 'local', 'debug']\n"
     ]
    }
   ],
   "source": [
    "smart = BaseSmartSelect(\n",
    "    [\"attr\"])\n",
    "error_ranked = smart.calculate_metrics(error,metric_scale=False)\n",
    "smart_select_forecasts = smart.get_smart_selection(error_ranked, error_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "|      date|store_id|product_id|original_id|prediction|reliable|    model_id|\n",
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "|2020-02-09|       1|         3|          c|         7|       2|smart_select|\n",
      "|2020-02-09|       1|         1|          c|         8|       2|smart_select|\n",
      "+----------+--------+----------+-----------+----------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smart_select_forecasts.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
