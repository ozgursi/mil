{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1e887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mode\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "import GPyOpt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import plot_prototypes\n",
    "from model import ShapeletGenerator, pairwise_dist\n",
    "from mil import get_data\n",
    "#from prototype_forest import PrototypeForest\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d447c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "\n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self, \n",
    "                 train_features,\n",
    "                 feature_types = [\"min\", \"mean\", \"max\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner = True,\n",
    "                 early_stopping_round = 10):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.train_features = train_features\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        if self.use_prototype_learner:\n",
    "            prototypes = find_prototype(bags, features, labels, self.early_stopping_round)\n",
    "            check = prototypes.cpu().detach().numpy()\n",
    "\n",
    "            check.resize(check.shape[1], check.shape[2])\n",
    "            \n",
    "            return check\n",
    "        \n",
    "        else:\n",
    "            number_of_rows = self.train_features.shape[0]\n",
    "            random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=prototype_count, \n",
    "                                              replace=False)\n",
    "            \n",
    "            prot = self.train_features[random_indices, :]\n",
    "            if len(prot.shape) == 1:\n",
    "                prot = prot.reshape(1, prot.shape[0])\n",
    "            return prot\n",
    "            \n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        for one_class in self.classes:\n",
    "            proba = y[y == one_class].shape[0] / y.shape[0]\n",
    "            probas.append(proba)\n",
    "        return np.asarray(probas)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "\n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        feature_list = []\n",
    "\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_sum = np.add.reduceat(distances[:, i], index)\n",
    "                group_mean = group_sum/bin_count\n",
    "                mean_vals = np.repeat(group_mean, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "\n",
    "        return np.array(np.transpose(feature_list))\n",
    "    \n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "        \n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_updated, labels, bag_ids):\n",
    "        bdc = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  max_depth=1, \n",
    "                                  criterion=\"entropy\",\n",
    "                                  min_samples_split=2)\n",
    "        bdc.fit(features_updated, labels.flatten())\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "        \n",
    "        labels_left = labels[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "    \n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if features.shape[0] < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            \n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "\n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            if features_left.shape[0] < self.min_samples_leaf or features_right.shape[0] < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "\n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left)\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right)\n",
    "\n",
    "            # splitting recursevely\n",
    "\n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "            \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.classes = np.unique(labels)\n",
    "        \n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "    \n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "       \n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "        \n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "            \n",
    "        return probas\n",
    "    \n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "                \n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "    \n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "        \n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = np.argmax(self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree))\n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)       \n",
    "        \n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"], \n",
    "                max_depth = 8, min_samples_leaf = 2, min_samples_split = 2, stratified = True, sample_rate = 0.8,\n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.stratified = stratified\n",
    "        self.sample_rate = sample_rate\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids, stratified, sample_rate):\n",
    "        if stratified:\n",
    "            ids, index  = np.unique(bag_ids, return_index=True)\n",
    "            group_min = np.minimum.reduceat(labels, index)\n",
    "\n",
    "            pos_bag_size = math.ceil(np.where(group_min == 1)[0].shape[0] * 0.8)\n",
    "            neg_bag_size = math.ceil(np.where(group_min == 0)[0].shape[0] * 0.8)\n",
    "\n",
    "            bags_pos = np.random.choice(np.where(group_min == 1)[0], pos_bag_size, replace=False)\n",
    "            bags_neg = np.random.choice(np.where(group_min == 0)[0], neg_bag_size, replace=False)\n",
    "            \n",
    "            df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1), \n",
    "                                              train_labels.reshape(train_labels.shape[0],1)], \n",
    "                                             axis=1))\n",
    "            \n",
    "            indices_pos = df[df[0].isin(bags_pos)].index.to_numpy()\n",
    "            indices_neg = df[df[0].isin(bags_neg)].index.to_numpy()\n",
    "\n",
    "            inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "        else:\n",
    "            sample_size = math.ceil(labels.shape[0] * sample_rate)\n",
    "            inbag_indices = np.random.choice(np.where(labels == 1)[0], sample_size, replace=False)\n",
    "        \n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "\n",
    "        return inbag_indices, outbag_indices\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            if self.use_prototype_learner:\n",
    "                print(f\"Tree {i} will be trained\")\n",
    "            \n",
    "            (inbag_indices,\n",
    "             outbag_indices) = self.sample(features, labels, bag_ids, self.stratified, self.sample_rate)      \n",
    "\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "\n",
    "\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_leaf=self.min_samples_leaf, \n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            preds = tree.predict(inbag_features, inbag_bag_ids)\n",
    "            score = metrics.roc_auc_score(inbag_labels, preds)\n",
    "            self._trees.append(tree)\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "\n",
    "        return mode(preds,1)[0]\n",
    "    \n",
    "    def predict_proba(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        \n",
    "        return np.sum(preds==1, axis=1)/self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc93e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(features, labels, bag_ids, stratified, sample_rate):\n",
    "    bags = np.unique(bag_ids)\n",
    "    positive_bags = np.unique(bag_ids[np.where(labels == 1)])\n",
    "    negative_bags = np.unique(bag_ids[np.where(labels == 0)])\n",
    "    if stratified:\n",
    "        pos_sample_size = math.ceil(positive_bags.shape[0] * sample_rate)\n",
    "        neg_sample_size = math.ceil(negative_bags.shape[0] * sample_rate)\n",
    "\n",
    "        sample_pos_bags = np.random.choice(positive_bags, pos_sample_size, replace=False)\n",
    "        sample_neg_bags = np.random.choice(negative_bags, neg_sample_size, replace=False)\n",
    "\n",
    "        indices_pos = np.where(np.isin(bag_ids, sample_pos_bags) == 1)[0]\n",
    "        indices_neg = np.where(np.isin(bag_ids, sample_neg_bags) == 1)[0]\n",
    "        inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "    else:\n",
    "        sample_size = math.ceil(bags.shape[0] * sample_rate)\n",
    "        sample_bags = np.random.choice(bags, sample_size, replace=False)        \n",
    "        inbag_indices = np.where(np.isin(bag_ids, sample_bags) == 1)[0]\n",
    "\n",
    "    oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "    oo_bag_mask[inbag_indices] = False\n",
    "\n",
    "    outbag_indices = np.where(oo_bag_mask == 1)\n",
    "\n",
    "    return inbag_indices, outbag_indices\n",
    "\n",
    "def get_parameter_scores(features, labels, bag_ids, params, fit_on_full = True):\n",
    "    keys, values = zip(*params.items())\n",
    "    params_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    param_vals_scores = dict()\n",
    "    for param_vals in params_list:\n",
    "        if param_vals[\"explained_variance\"] < 1:\n",
    "            pipe = Pipeline([('pca', PCA(n_components = param_vals[\"explained_variance\"], \n",
    "                             svd_solver = \"full\")), \n",
    "             ('scaler', StandardScaler()), ])\n",
    "        else:\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "        pipe.fit(features)\n",
    "\n",
    "        train_features = pipe.transform(features)\n",
    "        test_features = pipe.transform(features)\n",
    "\n",
    "        score_list = []\n",
    "        for i in range(0, param_vals[\"forest_size\"]):\n",
    "            (inbag_indices,\n",
    "             outbag_indices) = sample(features, labels, bag_ids, stratified = True, sample_rate = 0.8)      \n",
    "\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "\n",
    "            outbag_features = features[outbag_indices]\n",
    "            outbag_labels = labels[outbag_indices]\n",
    "            outbag_bag_ids = bag_ids[outbag_indices]\n",
    "\n",
    "            model = PrototypeTreeClassifier(max_depth=param_vals[\"max_depth\"], \n",
    "                                           min_samples_leaf=param_vals[\"min_samples_leaf\"],\n",
    "                                           min_samples_split=2)\n",
    "\n",
    "            model.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            preds = model.predict(outbag_features, outbag_bag_ids)\n",
    "\n",
    "            score = metrics.roc_auc_score(outbag_labels, preds)\n",
    "            score_list.append(score)\n",
    "\n",
    "        mean_score = sum(score_list)/len(score_list)\n",
    "        key = frozenset(param_vals.items())\n",
    "        param_vals_scores[key] = mean_score\n",
    "\n",
    "    return param_vals_scores\n",
    "\n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0,1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "    \n",
    "    sort_index = np.argsort(bag_ids)\n",
    "    bag_ids = bag_ids[sort_index]\n",
    "    features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False):\n",
    "    data = pd.read_csv(f\"./datasets/{dataset}.csv\", header=None)\n",
    "    testbags =  pd.read_csv(f\"./datasets/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin(['0','1'])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (train_features, train_labels, train_bag_ids,\n",
    "           test_features, test_labels, test_bag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28568bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bags(data,\n",
    "                    split_instances=False,\n",
    "                    instance_norm=True,\n",
    "                    split_ratio=0.2,\n",
    "                    stride_ratio=0.5):\n",
    "  bags = []\n",
    "  labels = []\n",
    "  current_bag = []\n",
    "  current_label = data[0, 0]\n",
    "  cur = data[0, 1]\n",
    "  instance_size = np.round(split_ratio * data[0, 2:].shape[0]).astype(\"int\")\n",
    "  stride = np.round(stride_ratio * instance_size).astype(\"int\")\n",
    "\n",
    "  for i in range(data.shape[0]):\n",
    "    if data[i, 1] == cur:\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "    else:\n",
    "      bags.append(np.array(current_bag))\n",
    "      labels.append(np.array(current_label))\n",
    "      current_label = data[i, 0]\n",
    "      current_bag = []\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "      cur = data[i, 1]\n",
    "  bags.append(np.array(current_bag))\n",
    "  labels.append(np.array(current_label, dtype=\"int32\"))\n",
    "  return bags, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc652841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels,\n",
    "                   early_stopping_round = 10):\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = random.choice(parameters[0])\n",
    "    reg_lambda_w = random.choice(parameters[1])\n",
    "    reg_lambda_p = random.choice(parameters[2])\n",
    "    lr_prot = random.choice(parameters[3])\n",
    "    lr_weights = random.choice(parameters[4])\n",
    "    reg_w = random.choice(parameters[5])\n",
    "    n_prototypes = random.choice(parameters[6])\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    \n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "            \n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes, features)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            cont = True\n",
    "            \n",
    "            max_stagnation = 0 # number of epochs without improvement to tolerate\n",
    "            best_prototype = None\n",
    "            best_score = 0\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_steps and max_stagnation < early_stopping_round:\n",
    "                i += 1\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        score_auc = roc_auc_score(y_true_list, y_score_list)\n",
    "                        print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                            \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        score_auc)\n",
    "                        \n",
    "                        if score_auc > best_score:\n",
    "                            best_score = score_auc\n",
    "                            best_prototype = model.prototypes\n",
    "                            max_stagnation = 0\n",
    "                        else:\n",
    "                            max_stagnation += 1\n",
    "                        \n",
    "                        print(\"max_stagnation \", max_stagnation)\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return best_prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cda171bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(preds, test_labels, test_bag_ids):\n",
    "    preds = preds.reshape(preds.shape[0])\n",
    "    df = pd.DataFrame({\"bag_ids\": test_bag_ids, \"preds\": preds})\n",
    "    predict_bags = df.groupby(\"bag_ids\").mean()[\"preds\"].to_numpy()\n",
    "\n",
    "    df = pd.DataFrame({\"bag_ids\": test_bag_ids, \"labels\": test_labels})\n",
    "    label_bags = df.groupby(\"bag_ids\").mean()[\"labels\"].to_numpy()\n",
    "    \n",
    "    return metrics.roc_auc_score(label_bags, predict_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c684ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gram_matrix(mat):\n",
    "  mat = mat.squeeze(dim=0)\n",
    "  mat = torch.mm(mat, mat.t())\n",
    "  return mat\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "  x_norm = (x.norm(dim=2)[:, :, None])\n",
    "  y_t = y.permute(0, 2, 1).contiguous()\n",
    "  y_norm = (y.norm(dim=2)[:, None])\n",
    "  y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "  dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "  return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "class ShapeletGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_prototypes, bag_size, n_classes, features):\n",
    "        n_prototypes = int(n_prototypes)\n",
    "        super(ShapeletGenerator, self).__init__()\n",
    "\n",
    "        number_of_rows = number_of_rows = features.shape[0]\n",
    "\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=1, \n",
    "                                              replace=False)\n",
    "            \n",
    "        prot = features[random_indices, :]\n",
    "        prot = prot.reshape(1, n_prototypes, prot.shape[1])\n",
    "        prot = prot.astype(\"float32\")\n",
    "        self.prototypes = torch.from_numpy(prot).requires_grad_()\n",
    "        #self.prototypes = (torch.randn(\n",
    "        #    (1, n_prototypes, bag_size))).requires_grad_()\n",
    "        if n_classes == 2:\n",
    "            n_classes = 1\n",
    "        self.linear_layer = torch.nn.Linear(3 * n_prototypes, n_classes, bias=False)\n",
    "        #self.linear_layer.weight = torch.nn.Parameter(self.linear_layer.weight/100000)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def pairwise_distances(self, x, y):\n",
    "        x_norm = (x.norm(dim=2)[:, :, None])\n",
    "        y_t = y.permute(0, 2, 1).contiguous()\n",
    "        y_norm = (y.norm(dim=2)[:, None])\n",
    "        y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "        dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "        return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "    def get_output(self, batch_inp):\n",
    "        dist = self.pairwise_distances(batch_inp, self.prototypes)\n",
    "        min_dist = dist.min(dim=1)[0]\n",
    "        max_dist = dist.max(dim=1)[0]\n",
    "        mean_dist = dist.mean(dim=1)\n",
    "        all_features = torch.cat([min_dist, max_dist, mean_dist], dim=1)\n",
    "        logits = self.linear_layer(all_features)\n",
    "\n",
    "        return logits, all_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, distances = self.get_output(x)\n",
    "        if self.n_classes == 1:\n",
    "          logits = logits.view(1)\n",
    "        return logits, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3cf186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BrownCreeper\"\n",
    "\n",
    "(train_features, \n",
    "     train_labels, \n",
    "     train_bag_ids,\n",
    "     test_features, \n",
    "     test_labels, \n",
    "     test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bb742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0 will be trained\n",
      "Step :  1780 Loss:  53.58619645365169  accuracy:  0.7033707865168539\n",
      "Eval Loss:  27.422971244069508  Eval Accuracy: 0.8061797752808989  AUC:  0.8484599183888377\n",
      "max_stagnation  0\n",
      "Step :  1780 Loss:  74.3324613764045  accuracy:  0.601685393258427\n",
      "Eval Loss:  208.71464168456154  Eval Accuracy: 0.7387640449438202  AUC:  0.7013623798867975\n",
      "max_stagnation  1\n",
      "Step :  1780 Loss:  124.14413623595506  accuracy:  0.6168539325842697\n",
      "Eval Loss:  120.4287008609686  Eval Accuracy: 0.8089887640449438  AUC:  0.8115374489930236\n",
      "max_stagnation  2\n",
      "Step :  1780 Loss:  172.08114466292136  accuracy:  0.599438202247191\n",
      "Eval Loss:  139.40592900349296  Eval Accuracy: 0.8314606741573034  AUC:  0.8086415690404107\n",
      "max_stagnation  3\n",
      "Step :  1780 Loss:  229.77189255617978  accuracy:  0.5831460674157304\n",
      "Eval Loss:  217.47140509922414  Eval Accuracy: 0.6938202247191011  AUC:  0.6931354482032381\n",
      "max_stagnation  4\n",
      "Step :  1780 Loss:  174.2573911516854  accuracy:  0.6292134831460674\n",
      "Eval Loss:  190.50473441417967  Eval Accuracy: 0.8370786516853933  AUC:  0.8473081479531394\n",
      "max_stagnation  5\n",
      "Step :  785 Loss:  7.437003010549363  accuracy:  0.9312101910828026\n",
      "Eval Loss:  9.49190724123815  Eval Accuracy: 0.9745222929936306  AUC:  0.4869281045751634\n",
      "max_stagnation  0\n",
      "Step :  785 Loss:  22.595822551751592  accuracy:  0.7745222929936306\n",
      "Eval Loss:  12.26350413462159  Eval Accuracy: 0.9745222929936306  AUC:  0.49673202614379086\n",
      "max_stagnation  0\n",
      "Step :  785 Loss:  16.589311305732483  accuracy:  0.756687898089172\n",
      "Eval Loss:  52.058453426239595  Eval Accuracy: 0.9745222929936306  AUC:  0.4934640522875817\n",
      "max_stagnation  1\n",
      "Step :  785 Loss:  47.76789410828025  accuracy:  0.780891719745223\n",
      "Eval Loss:  87.84135568825303  Eval Accuracy: 0.9745222929936306  AUC:  0.4803921568627451\n",
      "max_stagnation  2\n",
      "Step :  785 Loss:  43.62843351910828  accuracy:  0.7847133757961784\n",
      "Eval Loss:  37.996833169536224  Eval Accuracy: 0.9745222929936306  AUC:  0.5\n",
      "max_stagnation  0\n",
      "Step :  785 Loss:  13.413761445063694  accuracy:  0.7656050955414013\n",
      "Eval Loss:  75.1472230170183  Eval Accuracy: 0.9745222929936306  AUC:  0.5\n",
      "max_stagnation  1\n",
      "Step :  785 Loss:  54.627632364649685  accuracy:  0.7821656050955414\n",
      "Eval Loss:  67.92362928390503  Eval Accuracy: 0.9745222929936306  AUC:  0.49673202614379086\n",
      "max_stagnation  2\n",
      "Step :  785 Loss:  29.420277667197453  accuracy:  0.7617834394904459\n",
      "Eval Loss:  16.299861093994917  Eval Accuracy: 0.9681528662420382  AUC:  0.4803921568627451\n",
      "max_stagnation  3\n",
      "Step :  785 Loss:  48.83555433917198  accuracy:  0.7796178343949045\n",
      "Eval Loss:  47.272055073148884  Eval Accuracy: 0.9745222929936306  AUC:  0.4934640522875817\n",
      "max_stagnation  4\n",
      "Step :  785 Loss:  63.699099323248404  accuracy:  0.7490445859872611\n",
      "Eval Loss:  101.78046531737989  Eval Accuracy: 0.9745222929936306  AUC:  0.49019607843137253\n",
      "max_stagnation  5\n",
      "Step :  995 Loss:  88.63596105527638  accuracy:  0.6150753768844222\n",
      "Eval Loss:  216.60197138187274  Eval Accuracy: 0.7185929648241206  AUC:  0.5457947255880257\n",
      "max_stagnation  0\n",
      "Step :  995 Loss:  185.86166771356784  accuracy:  0.5246231155778894\n",
      "Eval Loss:  428.51704290284584  Eval Accuracy: 0.7135678391959799  AUC:  0.5556545497742932\n",
      "max_stagnation  0\n",
      "Step :  995 Loss:  245.51857726130655  accuracy:  0.5547738693467337\n",
      "Eval Loss:  114.82927895490847  Eval Accuracy: 0.7135678391959799  AUC:  0.7038488952245188\n",
      "max_stagnation  0\n",
      "Step :  995 Loss:  362.4675879396985  accuracy:  0.521608040201005\n",
      "Eval Loss:  936.1777712136657  Eval Accuracy: 0.7738693467336684  AUC:  0.6540152055119981\n",
      "max_stagnation  1\n",
      "Step :  995 Loss:  524.7288002512563  accuracy:  0.5065326633165829\n",
      "Eval Loss:  1214.7472148229128  Eval Accuracy: 0.6582914572864321  AUC:  0.5478142076502732\n",
      "max_stagnation  2\n",
      "Step :  995 Loss:  653.1540201005025  accuracy:  0.5095477386934674\n",
      "Eval Loss:  1022.6502015458879  Eval Accuracy: 0.6984924623115578  AUC:  0.5081967213114754\n",
      "max_stagnation  3\n",
      "Step :  995 Loss:  574.1068467336684  accuracy:  0.5145728643216081\n",
      "Eval Loss:  816.9534645967148  Eval Accuracy: 0.6683417085427136  AUC:  0.5413399857448324\n",
      "max_stagnation  4\n",
      "Step :  995 Loss:  609.7277010050251  accuracy:  0.4743718592964824\n",
      "Eval Loss:  949.2255542050654  Eval Accuracy: 0.7386934673366834  AUC:  0.5909954858636256\n",
      "max_stagnation  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.890020366598778"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "parameters = [[0.005, 0.01], [0.005, 0.01], [0.005, 0.01], [0.01, 0.02], [0.01, 0.05], [1],[1]]\n",
    "\n",
    "reg_lambda_dist = parameters[0]\n",
    "reg_lambda_w = parameters[1]\n",
    "reg_lambda_p = parameters[2]\n",
    "lr_prot = parameters[3]\n",
    "lr_weights = parameters[4]\n",
    "reg_w = parameters[5]\n",
    "n_prototypes = parameters[6]\n",
    "\n",
    "\n",
    "model = PrototypeForest(size = 1, \n",
    "                        max_depth = 3, \n",
    "                        min_samples_leaf= 3, \n",
    "                        min_samples_split=2,\n",
    "                        prototype_count=1,\n",
    "                        early_stopping_round=5,\n",
    "                        use_prototype_learner = True)\n",
    "\n",
    "model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "preds = model.predict(test_features, test_bag_ids)\n",
    "metrics.accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6886b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0 will be trained\n",
      "Step :  1765 Loss:  27.32640093838527  accuracy:  0.7019830028328612\n",
      "Eval Loss:  50.92960374495119  Eval Accuracy: 0.6203966005665722  AUC:  0.6069919517102615\n",
      "max_stagnation  0\n",
      "Step :  1765 Loss:  38.93760180594901  accuracy:  0.6073654390934844\n",
      "Eval Loss:  28.298802339695385  Eval Accuracy: 0.8016997167138811  AUC:  0.8491448692152918\n",
      "max_stagnation  0\n",
      "Step :  1765 Loss:  78.98396777620397  accuracy:  0.6124645892351275\n",
      "Eval Loss:  54.36075382337296  Eval Accuracy: 0.8441926345609065  AUC:  0.8327297116029512\n",
      "max_stagnation  1\n",
      "Step :  870 Loss:  8.32175433279454  accuracy:  0.8908045977011494\n",
      "Eval Loss:  17.695223753479706  Eval Accuracy: 0.9367816091954023  AUC:  0.49693251533742333\n",
      "max_stagnation  0\n",
      "Step :  870 Loss:  13.901850978807472  accuracy:  0.7034482758620689\n",
      "Eval Loss:  15.730571966061646  Eval Accuracy: 0.9367816091954023  AUC:  0.49693251533742333\n",
      "max_stagnation  1\n",
      "Step :  895 Loss:  12.700563023743017  accuracy:  0.6256983240223464\n",
      "Eval Loss:  4.905762951873589  Eval Accuracy: 0.7430167597765364  AUC:  0.7587596899224806\n",
      "max_stagnation  0\n",
      "Step :  895 Loss:  33.35729312150838  accuracy:  0.5229050279329609\n",
      "Eval Loss:  61.76848255441244  Eval Accuracy: 0.7430167597765364  AUC:  0.5455813953488372\n",
      "max_stagnation  1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PrototypeForest' object has no attribute '_train_perf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f81aa647eff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         early_stopping_round = 1)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b4e1f5f7c616>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_perf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrototypeForest' object has no attribute '_train_perf'"
     ]
    }
   ],
   "source": [
    "parameters = [0.005, 0.005, 0.005, 0.01,0.01,1,1]\n",
    "\n",
    "model = PrototypeForest(size = 20, \n",
    "                        max_depth = 3, \n",
    "                        min_samples_leaf= 3, \n",
    "                        min_samples_split=2,\n",
    "                        prototype_count=1,\n",
    "                        use_prototype_learner = True,\n",
    "                        early_stopping_round = 10)\n",
    "\n",
    "model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "preds = model.predict(test_features, test_bag_ids)\n",
    "metrics.accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad3ae83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold is 1\n",
      "> \u001b[0;32m<ipython-input-55-2b9f0df11447>\u001b[0m(160)\u001b[0;36mbuildDT\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    158 \u001b[0;31m             \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    159 \u001b[0;31m             \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 160 \u001b[0;31m             \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    161 \u001b[0;31m                                                 \u001b[0mfeatures_updated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    162 \u001b[0;31m                                                 \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-893b8dbb7354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                 use_prototype_learner = False)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-2b9f0df11447>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0minbag_bag_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minbag_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-2b9f0df11447>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-2b9f0df11447>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    158\u001b[0m              \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m              \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m              \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                                                  \u001b[0mfeatures_updated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-2b9f0df11447>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    158\u001b[0m              \u001b[0mlabels_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m              \u001b[0mbag_ids_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m              \u001b[0mbag_ids_right\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                                                  \u001b[0mfeatures_updated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = \"BrownCreeper\"\n",
    "accuracy_list = []\n",
    "\n",
    "for repl in range(1, 2):\n",
    "    for fold in range(1, 11):\n",
    "        print(f\"Fold is {fold}\")\n",
    "        (train_features, \n",
    "             train_labels, \n",
    "             train_bag_ids,\n",
    "             test_features, \n",
    "             test_labels, \n",
    "             test_bag_ids) = train_test_split(dataset, repl, fold, 1, fit_on_full = False)\n",
    "\n",
    "        parameters = [0.005, 0.005, 0.005, 0.01,0.01,1,1]\n",
    "\n",
    "        model = PrototypeForest(size = 100, \n",
    "                                max_depth = 3, \n",
    "                                min_samples_leaf=1, \n",
    "                                min_samples_split=2,\n",
    "                                prototype_count=1,\n",
    "                                use_prototype_learner = False)\n",
    "\n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "        preds = model.predict_proba(test_features, test_bag_ids)\n",
    "        #acc = metrics.roc_auc_score(test_labels, preds)\n",
    "        acc = get_auc(preds, test_labels, test_bag_ids)\n",
    "        accuracy_list.append(acc)\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f46a9254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2762 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1    2    3    4         5    6         7    8    9    ...  192  \\\n",
       "0     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "1     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "3     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "4     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "...   ...    ...  ...  ...  ...       ...  ...       ...  ...  ...  ...  ...   \n",
       "2757  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2758  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2759  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2760  0.0  100.0  0.0  0.0  0.0  0.006834  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2761  0.0  100.0  0.0  0.0  0.0  0.015945  0.0  0.041667  0.0  0.0  ...  0.0   \n",
       "\n",
       "      193  194       195       196       197  198  199  200  201  \n",
       "0     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...       ...       ...       ...  ...  ...  ...  ...  \n",
       "2757  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2758  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2759  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2760  0.0  0.0  0.000000  0.000000  0.006452  0.0  0.0  0.0  0.0  \n",
       "2761  0.0  0.0  0.018109  0.045455  0.000000  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2762 rows x 202 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels):\n",
    "    \n",
    "    def experiment_fn(parameters,\n",
    "                bags=bags,\n",
    "                features=features,\n",
    "                labels=labels,\n",
    "                n_classes=2,\n",
    "                folder=r\"./datasets\",\n",
    "                n_epochs=100,\n",
    "                batch_size=1,\n",
    "                display_every=5,\n",
    "                ):  \n",
    "        final_vals = []\n",
    "        reg_lambda_dist = 0.0005\n",
    "        reg_lambda_w = 0.005\n",
    "        reg_lambda_p = 0.00005\n",
    "        lr_prot = 0.00001\n",
    "        lr_weights = 0.00001\n",
    "        reg_w = 1\n",
    "        n_prototypes = 2\n",
    "        \n",
    "        for rep in range(1, 2):\n",
    "            vals = []\n",
    "            for fold in range(1, 2):\n",
    "                accs = [] \n",
    "\n",
    "                use_cuda = False\n",
    "                \n",
    "                data1 = np.vstack((labels, bags)).T\n",
    "                data = np.concatenate([data1, features], axis=1)\n",
    "\n",
    "                bags_train, labels_train = convert_to_bags(data)\n",
    "                bags_train = np.array(bags_train)\n",
    "                labels_train = np.array(labels_train)\n",
    "\n",
    "                bag_size = bags_train[0][0].shape[0]\n",
    "                step_per_epoch = len(bags_train)\n",
    "                lr_step = (step_per_epoch * 40)\n",
    "                display = (step_per_epoch * display_every)\n",
    "                max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "                model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    output_fn = torch.nn.Sigmoid()\n",
    "                else:\n",
    "                    output_fn = torch.nn.Softmax()\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "                else:\n",
    "                    loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "                optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "                optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                            lr=lr_weights)\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                train_loss_hist, eval_loss_hist = [], []\n",
    "                train_acc_hist, eval_acc_hist = [], []\n",
    "                eval_aucs = []\n",
    "                step_hist = []\n",
    "                time_hist = []\n",
    "\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    model = model.cuda()\n",
    "\n",
    "                for i in range(max_steps):\n",
    "                    np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                    start_time = time.time()\n",
    "                    batch_inp = bags_train[np_idx]\n",
    "                    targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                    batch_inp = torch.Tensor(batch_inp[0])\n",
    "                    batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                    if use_cuda and torch.cuda.is_available():\n",
    "                        targets = targets.cuda()\n",
    "                        batch_inp = batch_inp.cuda()\n",
    "\n",
    "                    logits, distances = model(batch_inp)\n",
    "                    out = output_fn(logits)\n",
    "\n",
    "                    if n_classes == 2:\n",
    "                        predicted = (out > 0.5).type(torch.int64)\n",
    "                    else:\n",
    "                        _, predicted = torch.max(out, 1)\n",
    "                    correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                    batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                    # N_prot x N_prot\n",
    "                    #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                    #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                    #cos_loss = cos_loss/M_prot_norm\n",
    "                    #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                    # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                    #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                    prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                    reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                    weight_reg = 0\n",
    "                    for param in model.linear_layer.parameters():\n",
    "                        weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                    reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                    total_loss += batch_loss\n",
    "                    min_loss = batch_loss + reg_loss\n",
    "                    min_loss.backward()\n",
    "\n",
    "                    optim1.step()\n",
    "                    optim2.step()\n",
    "\n",
    "                    if (i + 1) % lr_step == 0:\n",
    "                        print(\"LR DROP!\")\n",
    "                        optims = [optim1, optim2]\n",
    "                        for o in optims:\n",
    "                            for p in o.param_groups:\n",
    "                                p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                    if (i + 1) % display == 0:\n",
    "                        with torch.no_grad():\n",
    "                            print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                            total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                            train_loss_hist.append(total_loss.item() / display)\n",
    "                            train_acc_hist.append(correct / display)\n",
    "                            total_loss = 0\n",
    "                            correct = 0\n",
    "                            model = model.eval()\n",
    "                            e_loss = 0\n",
    "                            e_acc = 0\n",
    "                            y_true = []\n",
    "                            y_score = []\n",
    "\n",
    "                            for i in range(len(bags_train)):\n",
    "                                batch_inp = torch.Tensor(bags_train[i])\n",
    "                                batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                      batch_inp.shape[1])\n",
    "                                targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                                logits, distances = model(batch_inp)\n",
    "                                out = output_fn(logits)\n",
    "\n",
    "                                if n_classes == 2:\n",
    "                                    predicted = (out > 0.5).type(torch.int64)\n",
    "                                else:\n",
    "                                    _, predicted = torch.max(out, 1)\n",
    "                                y_true.append(targets)\n",
    "                                y_score.append(out)\n",
    "                                correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                                e_acc += correct\n",
    "                                eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                                e_loss += eval_loss\n",
    "\n",
    "                            y_true_list = [x.tolist() for x in y_true]\n",
    "                            y_score_list = [x.tolist() for x in y_score]\n",
    "                            print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                                \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                            roc_auc_score(y_true_list, y_score_list))\n",
    "                            eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                            eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                            eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                            accs.append(e_acc / len(bags_train))\n",
    "                            step_hist.append(i+1)\n",
    "                            model = model.train()\n",
    "\n",
    "                print(str(rep), \" \", str(fold), \" Final Best AUC: \",\n",
    "                    np.max(np.array(eval_aucs)))\n",
    "\n",
    "                end_time = time.time()\n",
    "                total_time = end_time - start_time\n",
    "                time_hist.append([total_time]*len(accs))\n",
    "                output_data = np.column_stack([step_hist, train_loss_hist,train_acc_hist, eval_loss_hist, eval_acc_hist, eval_aucs])\n",
    "                df = DataFrame(output_data, columns = [\"step_hist\", \"train_loss_hist\",\"train_acc_hist\", \"eval_loss_hist\", \"eval_acc_hist\", \"eval_aucs\"])\n",
    "                df['dataset'] = dataset\n",
    "                df['fold'] = fold\n",
    "                df['rep'] = rep\n",
    "                df['time_hist'] = total_time\n",
    "                export_csv = df.to_csv (r'.\\export_dataframe.csv', index = None, header=False, mode='a') #Don't forget to add '.csv' at the end of the path\n",
    "                vals.append(np.max(np.array(eval_aucs)))\n",
    "                prototypes = model.prototypes.squeeze(0).detach().numpy()\n",
    "                figure_file = \"shapelets_\" + dataset + \"_run_\" + str(0) + \"_\" + str(\n",
    "                rep) + \"_\" + str(fold) + \".png\"\n",
    "                files = \"{}_{}_run_{}_{}_{}.png\"\n",
    "                loss_file = files.format(\"loss\", dataset, \"0\", str(rep), str(fold))\n",
    "                accuracy_file = files.format(\"acc\", dataset, \"0\", str(rep), str(fold))\n",
    "\n",
    "                plt.plot(train_loss_hist, label=\"train_loss\")\n",
    "                plt.plot(eval_loss_hist, label=\"eval_loss\")\n",
    "                plt.title(\"Loss History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(loss_file)\n",
    "                plt.close()\n",
    "                plt.plot(train_loss_hist, label=\"train_loss\")\n",
    "                plt.title(\"Only Training Loss History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(\"only_train_\"+loss_file)\n",
    "                plt.close()\n",
    "                plt.plot(train_acc_hist, label=\"train_accuracy\")\n",
    "                plt.plot(eval_acc_hist, label=\"eval_accuracy\")\n",
    "                plt.title(\"Accuracy History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(accuracy_file)\n",
    "                plt.close()\n",
    "                plot_prototypes(prototypes, savefile=figure_file)\n",
    "\n",
    "                final_vals.append(vals)\n",
    "\n",
    "        print(np.mean(final_vals), \"mean final vals\")\n",
    "    \n",
    "        return np.mean(final_vals)\n",
    "\n",
    "    BOUNDS = [\n",
    "     {'name': 'reg_lambda_dist',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.0005, 0.005)},\n",
    "     {'name': 'reg_lambda_w',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.005, 0.05)},\n",
    "     {'name': 'reg_lambda_p',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00005, 0.0005)},\n",
    "     {'name': 'lr_prot',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00001, 0.0001)},\n",
    "     {'name': 'lr_weights',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00001, 0.0001)},\n",
    "     {'name': 'reg_w',\n",
    "      'type': 'discrete',\n",
    "      'domain': (1, 2)},\n",
    "     {'name': 'n_prototypes',\n",
    "      'type': 'discrete',\n",
    "      'domain': (2, 6)}  # will be x2. ie if 2, then number of prototypes will actually be 4, if 4 then 8, etc.\n",
    "    ]\n",
    "    \n",
    "    np.random.seed(777)\n",
    "    optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "         f=experiment_fn, domain=BOUNDS,\n",
    "         acquisition_type='MPI',\n",
    "         acquisition_par=0.3,\n",
    "         exact_eval=True,\n",
    "         maximize=True\n",
    "     )\n",
    "    \n",
    "    max_iter = 40\n",
    "    optimizer.run_optimization(max_iter, max_time=3600)\n",
    "    #con_plot = data_s + \"_optimizer_bayesopt.png\"\n",
    "    \n",
    "    parameters = optimizer.x_opt\n",
    "    parameters_output = np.transpose([parameters])\n",
    "    df_parameters = DataFrame(parameters_output)\n",
    "    df_parameters = df_parameters.T\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    print(parameters)\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = parameters[0]\n",
    "    reg_lambda_w = parameters[1]\n",
    "    reg_lambda_p = parameters[2]\n",
    "    lr_prot = parameters[3]\n",
    "    lr_weights = parameters[4]\n",
    "    reg_w = parameters[5]\n",
    "    n_prototypes = parameters[6]\n",
    "    n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "\n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            step_per_epoch = len(bags_train)\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            for i in range(max_steps):\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                # N_prot x N_prot\n",
    "                #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                #cos_loss = cos_loss/M_prot_norm\n",
    "                #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                            \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        roc_auc_score(y_true_list, y_score_list))\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return model.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16246005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels):\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = parameters[0]\n",
    "    reg_lambda_w = parameters[1]\n",
    "    reg_lambda_p = parameters[2]\n",
    "    lr_prot = parameters[3]\n",
    "    lr_weights = parameters[4]\n",
    "    reg_w = parameters[5]\n",
    "    n_prototypes = parameters[6]\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    \n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            for i in range(max_steps):\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                # N_prot x N_prot\n",
    "                #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                #cos_loss = cos_loss/M_prot_norm\n",
    "                #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                            \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        roc_auc_score(y_true_list, y_score_list))\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return model.prototypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
