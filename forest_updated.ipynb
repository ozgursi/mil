{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1e887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import mode\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "import GPyOpt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import plot_prototypes\n",
    "from model import ShapeletGenerator, pairwise_dist\n",
    "from mil import get_data\n",
    "from prototype_forest import PrototypeForest\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d447c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "\n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self, \n",
    "                 train_features,\n",
    "                 feature_types = [\"min\", \"mean\", \"max\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner = True):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.train_features = train_features\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        if self.use_prototype_learner:\n",
    "            prototypes = find_prototype(bags, features, labels)\n",
    "            check = prototypes.cpu().detach().numpy()\n",
    "\n",
    "            check.resize(check.shape[1], check.shape[2])\n",
    "            \n",
    "            return check\n",
    "        \n",
    "        else:\n",
    "            number_of_rows = self.train_features.shape[0]\n",
    "            random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=prototype_count, \n",
    "                                              replace=False)\n",
    "        \n",
    "            return self.train_features[random_indices, :]\n",
    "            \n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        for one_class in self.classes:\n",
    "            proba = y[y == one_class].shape[0] / y.shape[0]\n",
    "            probas.append(proba)\n",
    "        return np.asarray(probas)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "\n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        feature_list = []\n",
    "\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_sum = np.add.reduceat(distances[:, i], index)\n",
    "                group_mean = group_sum/bin_count\n",
    "                mean_vals = np.repeat(group_mean, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "\n",
    "        return np.array(np.transpose(feature_list))\n",
    "    \n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "        \n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_updated, labels, bag_ids):\n",
    "        bdc = tree.DecisionTreeClassifier(random_state=0, \n",
    "                                  max_depth=1, \n",
    "                                  criterion=\"entropy\",\n",
    "                                  min_samples_split=2)\n",
    "        bdc.fit(features_updated, labels.flatten())\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "        \n",
    "        labels_left = labels[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_updated[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_updated[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "    \n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if features.shape[0] < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "\n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            if features_left.shape[0] < self.min_samples_leaf or features_right.shape[0] < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "\n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left)\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right)\n",
    "\n",
    "            # splitting recursevely\n",
    "\n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "            \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.classes = np.unique(labels)\n",
    "        \n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "    \n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "       \n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "        \n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "            \n",
    "        return probas\n",
    "    \n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "                \n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "    \n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "        \n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = np.argmax(self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree))\n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)       \n",
    "        \n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"], \n",
    "                max_depth = 3, min_samples_leaf = 2, min_samples_split = 2, stratified = True, sample_rate = 0.8,\n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.stratified = stratified\n",
    "        self.sample_rate = sample_rate\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids, stratified, sample_rate):\n",
    "        if stratified:\n",
    "            pos_sample_size = math.ceil(np.where(labels == 1)[0].shape[0] * sample_rate)\n",
    "            neg_sample_size = math.ceil(np.where(labels == 0)[0].shape[0] * sample_rate)\n",
    "            indices_pos = np.random.choice(np.where(labels == 1)[0], pos_sample_size, replace=False)\n",
    "            indices_neg = np.random.choice(np.where(labels == 0)[0], neg_sample_size, replace=False)\n",
    "            inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "        else:\n",
    "            sample_size = math.ceil(labels.shape[0] * sample_rate)\n",
    "            inbag_indices = np.random.choice(np.where(labels == 1)[0], sample_size, replace=False)\n",
    "        \n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "\n",
    "        return inbag_indices, outbag_indices\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            if self.use_prototype_learner:\n",
    "                print(f\"Tree {i} will be trained\")\n",
    "            tree = PrototypeTreeClassifier(max_depth=self.max_depth, \n",
    "                                           min_samples_leaf=self.min_samples_leaf, \n",
    "                                           min_samples_split=self.min_samples_split,\n",
    "                                          prototype_count = self.prototype_count,\n",
    "                                          use_prototype_learner = self.use_prototype_learner,\n",
    "                                          train_features = features)\n",
    "            \n",
    "            (inbag_indices,\n",
    "             outbag_indices) = self.sample(features, labels, bag_ids, self.stratified, self.sample_rate)      \n",
    "            \n",
    "            tree.inbag_indices = inbag_indices\n",
    "            tree.outbag_indices = outbag_indices\n",
    "            \n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "                                    \n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "\n",
    "            self._trees.append(tree)\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        \n",
    "        return mode(preds,1)[0]\n",
    "    \n",
    "    def predict_proba(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        \n",
    "        return np.sum(preds==1, axis=1)/self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "bc93e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(features, labels, bag_ids, stratified, sample_rate):\n",
    "    bags = np.unique(bag_ids)\n",
    "    positive_bags = np.unique(bag_ids[np.where(labels == 1)])\n",
    "    negative_bags = np.unique(bag_ids[np.where(labels == 0)])\n",
    "    if stratified:\n",
    "        pos_sample_size = math.ceil(positive_bags.shape[0] * sample_rate)\n",
    "        neg_sample_size = math.ceil(negative_bags.shape[0] * sample_rate)\n",
    "\n",
    "        sample_pos_bags = np.random.choice(positive_bags, pos_sample_size, replace=False)\n",
    "        sample_neg_bags = np.random.choice(negative_bags, neg_sample_size, replace=False)\n",
    "\n",
    "        indices_pos = np.where(np.isin(bag_ids, sample_pos_bags) == 1)[0]\n",
    "        indices_neg = np.where(np.isin(bag_ids, sample_neg_bags) == 1)[0]\n",
    "        inbag_indices = np.concatenate((indices_pos, indices_neg))\n",
    "    else:\n",
    "        sample_size = math.ceil(bags.shape[0] * sample_rate)\n",
    "        sample_bags = np.random.choice(bags, sample_size, replace=False)        \n",
    "        inbag_indices = np.where(np.isin(bag_ids, sample_bags) == 1)[0]\n",
    "\n",
    "    oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "    oo_bag_mask[inbag_indices] = False\n",
    "\n",
    "    outbag_indices = np.where(oo_bag_mask == 1)\n",
    "\n",
    "    return inbag_indices, outbag_indices\n",
    "\n",
    "def get_parameter_scores(features, labels, bag_ids, params, fit_on_full = True):\n",
    "    keys, values = zip(*params.items())\n",
    "    params_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    param_vals_scores = dict()\n",
    "    for param_vals in params_list:\n",
    "        if param_vals[\"explained_variance\"] < 1:\n",
    "            pipe = Pipeline([('pca', PCA(n_components = param_vals[\"explained_variance\"], \n",
    "                             svd_solver = \"full\")), \n",
    "             ('scaler', StandardScaler()), ])\n",
    "        else:\n",
    "            pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "        pipe.fit(features)\n",
    "\n",
    "        train_features = pipe.transform(features)\n",
    "        test_features = pipe.transform(features)\n",
    "\n",
    "        score_list = []\n",
    "        for i in range(0, param_vals[\"forest_size\"]):\n",
    "            (inbag_indices,\n",
    "             outbag_indices) = sample(features, labels, bag_ids, stratified = True, sample_rate = 0.8)      \n",
    "\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "\n",
    "            outbag_features = features[outbag_indices]\n",
    "            outbag_labels = labels[outbag_indices]\n",
    "            outbag_bag_ids = bag_ids[outbag_indices]\n",
    "\n",
    "            model = PrototypeTreeClassifier(max_depth=param_vals[\"max_depth\"], \n",
    "                                           min_samples_leaf=param_vals[\"min_samples_leaf\"],\n",
    "                                           min_samples_split=2)\n",
    "\n",
    "            model.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            preds = model.predict(outbag_features, outbag_bag_ids)\n",
    "\n",
    "            score = metrics.roc_auc_score(outbag_labels, preds)\n",
    "            score_list.append(score)\n",
    "\n",
    "        mean_score = sum(score_list)/len(score_list)\n",
    "        key = frozenset(param_vals.items())\n",
    "        param_vals_scores[key] = mean_score\n",
    "\n",
    "    return param_vals_scores\n",
    "\n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0,1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "    \n",
    "    #sort_index = np.argsort(bag_ids)\n",
    "    #bag_ids = bag_ids[sort_index]\n",
    "    #features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False):\n",
    "    data = pd.read_csv(f\"./datasets/{dataset}.csv\", header=None)\n",
    "    testbags =  pd.read_csv(f\"./datasets/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin(['0','1'])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (train_features, train_labels, train_bag_ids,\n",
    "           test_features, test_labels, test_bag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "28568bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bags(data,\n",
    "                    split_instances=False,\n",
    "                    instance_norm=True,\n",
    "                    split_ratio=0.2,\n",
    "                    stride_ratio=0.5):\n",
    "  bags = []\n",
    "  labels = []\n",
    "  current_bag = []\n",
    "  current_label = data[0, 0]\n",
    "  cur = data[0, 1]\n",
    "  instance_size = np.round(split_ratio * data[0, 2:].shape[0]).astype(\"int\")\n",
    "  stride = np.round(stride_ratio * instance_size).astype(\"int\")\n",
    "\n",
    "  for i in range(data.shape[0]):\n",
    "    if data[i, 1] == cur:\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "    else:\n",
    "      bags.append(np.array(current_bag))\n",
    "      labels.append(np.array(current_label))\n",
    "      current_label = data[i, 0]\n",
    "      current_bag = []\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "      cur = data[i, 1]\n",
    "  bags.append(np.array(current_bag))\n",
    "  labels.append(np.array(current_label, dtype=\"int32\"))\n",
    "  return bags, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c684ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels):\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = parameters[0]\n",
    "    reg_lambda_w = parameters[1]\n",
    "    reg_lambda_p = parameters[2]\n",
    "    lr_prot = parameters[3]\n",
    "    lr_weights = parameters[4]\n",
    "    reg_w = parameters[5]\n",
    "    n_prototypes = parameters[6]\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    \n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            for i in range(max_steps):\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                # N_prot x N_prot\n",
    "                #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                #cos_loss = cos_loss/M_prot_norm\n",
    "                #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                            \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        roc_auc_score(y_true_list, y_score_list))\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return model.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(preds, test_labels, test_bag_ids):\n",
    "    preds = preds.reshape(preds.shape[0])\n",
    "    df = pd.DataFrame({\"bag_ids\": test_bag_ids, \"preds\": preds})\n",
    "    predict_bags = df.groupby(\"bag_ids\").mean()[\"preds\"].to_numpy()\n",
    "\n",
    "    df = pd.DataFrame({\"bag_ids\": test_bag_ids, \"labels\": test_labels})\n",
    "    label_bags = df.groupby(\"bag_ids\").mean()[\"labels\"].to_numpy()\n",
    "    \n",
    "    return metrics.roc_auc_score(label_bags, predict_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cf186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Newsgroups6\"\n",
    "\n",
    "(train_features, \n",
    "     train_labels, \n",
    "     train_bag_ids,\n",
    "     test_features, \n",
    "     test_labels, \n",
    "     test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6886b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0 will be trained\n",
      "Step :  450 Loss:  29.149641927083334  accuracy:  0.6688888888888889\n",
      "Eval Loss:  29.42895002963236  Eval Accuracy: 0.9052484904784023  AUC:  0.9239623537538315\n",
      "Step :  900 Loss:  38.8753515625  accuracy:  0.9177777777777778\n",
      "Eval Loss:  73.5089168406009  Eval Accuracy: 0.9094287041337669  AUC:  0.9339645123688641\n",
      "Step :  1350 Loss:  159.0303298611111  accuracy:  0.9133333333333333\n",
      "Eval Loss:  147.282591716153  Eval Accuracy: 0.931723176962378  AUC:  0.9469550576350214\n",
      "Step :  1800 Loss:  247.18012152777777  accuracy:  0.9266666666666666\n",
      "Eval Loss:  173.06962781084232  Eval Accuracy: 0.9321876451463075  AUC:  0.9495151750636791\n",
      "Step :  2250 Loss:  116.45603298611111  accuracy:  0.9111111111111111\n",
      "Eval Loss:  59.894210239077275  Eval Accuracy: 0.9442638179284719  AUC:  0.9644139360186503\n",
      "Step :  2700 Loss:  114.29388020833333  accuracy:  0.9377777777777778\n",
      "Eval Loss:  234.66668413032005  Eval Accuracy: 0.9396191360891779  AUC:  0.9572438803263825\n",
      "Step :  3150 Loss:  282.42703125  accuracy:  0.9555555555555556\n",
      "Eval Loss:  450.64827558311043  Eval Accuracy: 0.9410125406409661  AUC:  0.9533579415447049\n",
      "LR DROP!\n",
      "Step :  3600 Loss:  536.1694791666666  accuracy:  0.9377777777777778\n",
      "Eval Loss:  441.7291120271377  Eval Accuracy: 0.9535531816070599  AUC:  0.9617722229417606\n",
      "Step :  4050 Loss:  599.2846527777778  accuracy:  0.9511111111111111\n",
      "Eval Loss:  443.4993558924418  Eval Accuracy: 0.9581978634463539  AUC:  0.9670323360531883\n",
      "Step :  4500 Loss:  488.43128472222224  accuracy:  0.9666666666666667\n",
      "Eval Loss:  329.42935145294393  Eval Accuracy: 0.9637714816535068  AUC:  0.973985235073177\n",
      "Step :  4950 Loss:  117.10413194444445  accuracy:  0.9777777777777777\n",
      "Eval Loss:  200.9197789162079  Eval Accuracy: 0.9684161634928008  AUC:  0.9795220826317834\n",
      "Step :  5400 Loss:  178.33286458333333  accuracy:  0.96\n",
      "Eval Loss:  103.26328843391833  Eval Accuracy: 0.9702740362285184  AUC:  0.9813120062168113\n",
      "Step :  5850 Loss:  39.39888888888889  accuracy:  0.9755555555555555\n",
      "Eval Loss:  67.79548523274345  Eval Accuracy: 0.9688806316767301  AUC:  0.9806190907913483\n",
      "Step :  6300 Loss:  52.32464409722222  accuracy:  0.9555555555555556\n",
      "Eval Loss:  38.28850185134718  Eval Accuracy: 0.969809568044589  AUC:  0.9806739196131762\n",
      "Step :  6750 Loss:  6.400104166666667  accuracy:  0.9888888888888889\n",
      "Eval Loss:  84.91771198083121  Eval Accuracy: 0.9628425452856479  AUC:  0.9727323749082587\n",
      "LR DROP!\n",
      "Step :  7200 Loss:  112.73282118055556  accuracy:  0.9422222222222222\n",
      "Eval Loss:  296.13821696792155  Eval Accuracy: 0.5355318160705992  AUC:  0.5260246945559729\n",
      "Step :  7650 Loss:  56.539401041666665  accuracy:  0.9488888888888889\n",
      "Eval Loss:  96.75852901194742  Eval Accuracy: 0.9609846725499304  AUC:  0.9716263005655573\n",
      "Step :  8100 Loss:  145.2819357638889  accuracy:  0.9711111111111111\n",
      "Eval Loss:  144.0289447689632  Eval Accuracy: 0.9605202043660009  AUC:  0.970396753442991\n",
      "Step :  8550 Loss:  49.29654079861111  accuracy:  0.98\n",
      "Eval Loss:  166.40673251955997  Eval Accuracy: 0.9609846725499304  AUC:  0.971284375944394\n",
      "Step :  9000 Loss:  278.44378472222223  accuracy:  0.9488888888888889\n",
      "Eval Loss:  172.96143714494613  Eval Accuracy: 0.9605202043660009  AUC:  0.9725549367525796\n",
      "Step :  450 Loss:  19.219095052083333  accuracy:  0.6866666666666666\n",
      "Eval Loss:  23.83099198385084  Eval Accuracy: 0.7560594214229867  AUC:  0.9143359965254844\n",
      "Step :  900 Loss:  32.304210069444444  accuracy:  0.8266666666666667\n",
      "Eval Loss:  38.95229417638142  Eval Accuracy: 0.8639562157935887  AUC:  0.8906093069969869\n",
      "Step :  1350 Loss:  121.60061631944444  accuracy:  0.88\n",
      "Eval Loss:  99.93181559669493  Eval Accuracy: 0.9046129788897577  AUC:  0.9317153611596196\n",
      "Step :  1800 Loss:  180.58135416666667  accuracy:  0.8866666666666667\n",
      "Eval Loss:  137.25798840780575  Eval Accuracy: 0.9194683346364347  AUC:  0.9577516083207412\n",
      "Step :  2250 Loss:  173.35506944444444  accuracy:  0.9155555555555556\n",
      "Eval Loss:  103.45425639331229  Eval Accuracy: 0.8053166536356529  AUC:  0.49905898533283866\n",
      "Step :  2700 Loss:  82.50822916666667  accuracy:  0.9111111111111111\n",
      "Eval Loss:  61.05833079732053  Eval Accuracy: 0.928068803752932  AUC:  0.9585365412282052\n",
      "Step :  3150 Loss:  71.782265625  accuracy:  0.9311111111111111\n",
      "Eval Loss:  77.45663809347563  Eval Accuracy: 0.9311962470680218  AUC:  0.9692993964838624\n",
      "LR DROP!\n",
      "Step :  3600 Loss:  101.95111111111112  accuracy:  0.9444444444444444\n",
      "Eval Loss:  80.72644959398319  Eval Accuracy: 0.9296325254104769  AUC:  0.9684262434513523\n",
      "Step :  4050 Loss:  191.75467013888888  accuracy:  0.9155555555555556\n",
      "Eval Loss:  100.94394579895592  Eval Accuracy: 0.9327599687255669  AUC:  0.9746582035668077\n",
      "Step :  4500 Loss:  155.3244097222222  accuracy:  0.9333333333333333\n",
      "Eval Loss:  87.32180161658817  Eval Accuracy: 0.9351055512118843  AUC:  0.9722242329373231\n",
      "Step :  4950 Loss:  87.55092013888888  accuracy:  0.9333333333333333\n",
      "Eval Loss:  73.15678461572917  Eval Accuracy: 0.9374511336982018  AUC:  0.9686795935540496\n",
      "Step :  5400 Loss:  204.48789930555554  accuracy:  0.9311111111111111\n",
      "Eval Loss:  156.9864988927267  Eval Accuracy: 0.9304143862392494  AUC:  0.9704530442729304\n",
      "Step :  5850 Loss:  116.8821701388889  accuracy:  0.9466666666666667\n",
      "Eval Loss:  118.25612608169186  Eval Accuracy: 0.9327599687255669  AUC:  0.9740451868004597\n",
      "Step :  6300 Loss:  126.80495659722222  accuracy:  0.9066666666666666\n",
      "Eval Loss:  171.23180852670797  Eval Accuracy: 0.8146989835809226  AUC:  0.521410345732408\n",
      "Step :  6750 Loss:  57.125338541666665  accuracy:  0.9244444444444444\n",
      "Eval Loss:  149.38216268596844  Eval Accuracy: 0.9061767005473026  AUC:  0.9592423022285762\n",
      "LR DROP!\n",
      "Step :  7200 Loss:  62.59729166666666  accuracy:  0.9444444444444444\n",
      "Eval Loss:  21.05953962566146  Eval Accuracy: 0.9538702111024238  AUC:  0.9801776165184268\n",
      "Step :  7650 Loss:  20.688346354166665  accuracy:  0.94\n",
      "Eval Loss:  30.624444045295373  Eval Accuracy: 0.9554339327599687  AUC:  0.9806119309801935\n",
      "Step :  8100 Loss:  8.844703233506944  accuracy:  0.96\n",
      "Eval Loss:  29.3602245508273  Eval Accuracy: 0.9554339327599687  AUC:  0.9867240022077651\n",
      "Step :  8550 Loss:  0.2802206759982639  accuracy:  0.9533333333333334\n",
      "Eval Loss:  40.927671328478255  Eval Accuracy: 0.9554339327599687  AUC:  0.9925306960794071\n",
      "Step :  9000 Loss:  13.771407335069444  accuracy:  0.9533333333333334\n",
      "Eval Loss:  50.540517846152966  Eval Accuracy: 0.9562157935887412  AUC:  0.9923203250119889\n",
      "Step :  330 Loss:  9.886706912878788  accuracy:  0.9181818181818182\n",
      "Eval Loss:  26.87706133394181  Eval Accuracy: 0.8765714285714286  AUC:  0.8454437869822485\n",
      "Step :  660 Loss:  14.130300071022727  accuracy:  0.9424242424242424\n",
      "Eval Loss:  32.06916090338571  Eval Accuracy: 0.9657142857142857  AUC:  0.6306508875739646\n",
      "Step :  990 Loss:  19.148959812973484  accuracy:  0.9545454545454546\n",
      "Eval Loss:  3.305081965903257  Eval Accuracy: 0.9668571428571429  AUC:  0.9192110453648915\n",
      "Step :  1320 Loss:  15.181069483901515  accuracy:  0.9424242424242424\n",
      "Eval Loss:  29.115826043265205  Eval Accuracy: 0.9657142857142857  AUC:  0.8629585798816569\n",
      "Step :  1650 Loss:  40.63528645833333  accuracy:  0.9363636363636364\n",
      "Eval Loss:  27.25285489654541  Eval Accuracy: 0.9657142857142857  AUC:  0.9111439842209074\n",
      "Step :  1980 Loss:  9.849656723484848  accuracy:  0.9666666666666667\n",
      "Eval Loss:  72.50877198791504  Eval Accuracy: 0.9657142857142857  AUC:  0.9178698224852072\n",
      "Step :  2310 Loss:  26.477414772727272  accuracy:  0.9787878787878788\n",
      "Eval Loss:  117.60923637281145  Eval Accuracy: 0.9657142857142857  AUC:  0.9072978303747535\n",
      "LR DROP!\n",
      "Step :  2640 Loss:  0.03570758935176965  accuracy:  0.9878787878787879\n",
      "Eval Loss:  177.73361452892848  Eval Accuracy: 0.9657142857142857  AUC:  0.9185798816568047\n",
      "Step :  2970 Loss:  0.09662056547222715  accuracy:  0.9363636363636364\n",
      "Eval Loss:  214.37592314801898  Eval Accuracy: 0.9657142857142857  AUC:  0.909171597633136\n",
      "Step :  3300 Loss:  214.55485321969698  accuracy:  0.9818181818181818\n",
      "Eval Loss:  212.96962016841343  Eval Accuracy: 0.9657142857142857  AUC:  0.9049309664694279\n",
      "Step :  3630 Loss:  0.03360714189934008  accuracy:  0.9787878787878788\n",
      "Eval Loss:  219.34234521965882  Eval Accuracy: 0.9657142857142857  AUC:  0.9374556213017751\n",
      "Step :  3960 Loss:  18.93012547348485  accuracy:  0.9484848484848485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  222.84618774795533  Eval Accuracy: 0.9657142857142857  AUC:  0.9566074950690335\n",
      "Step :  4290 Loss:  3.3950617009943183  accuracy:  0.9727272727272728\n",
      "Eval Loss:  236.5028360394069  Eval Accuracy: 0.9657142857142857  AUC:  0.9762327416173571\n",
      "Step :  4620 Loss:  0.025205352089621803  accuracy:  0.9757575757575757\n",
      "Eval Loss:  269.2371659415109  Eval Accuracy: 0.9657142857142857  AUC:  0.9595660749506904\n",
      "Step :  4950 Loss:  0.018904014067216354  accuracy:  0.9757575757575757\n",
      "Eval Loss:  311.929709085192  Eval Accuracy: 0.9657142857142857  AUC:  0.9272189349112427\n",
      "LR DROP!\n",
      "Step :  5280 Loss:  4.007166637073864  accuracy:  0.9636363636363636\n",
      "Eval Loss:  361.1057479662214  Eval Accuracy: 0.9657142857142857  AUC:  0.9446745562130177\n",
      "Step :  5610 Loss:  948.9080492424242  accuracy:  0.9848484848484849\n",
      "Eval Loss:  322.2727210775103  Eval Accuracy: 0.9657142857142857  AUC:  0.9613412228796844\n",
      "Step :  5940 Loss:  0.018904014067216354  accuracy:  0.9787878787878788\n",
      "Eval Loss:  299.49114950942993  Eval Accuracy: 0.9657142857142857  AUC:  0.9785996055226825\n",
      "Step :  6270 Loss:  774.7076704545455  accuracy:  0.9636363636363636\n",
      "Eval Loss:  274.21233948407854  Eval Accuracy: 0.9657142857142857  AUC:  0.9791715976331361\n",
      "Step :  6600 Loss:  0.027305799542051375  accuracy:  0.9727272727272728\n",
      "Eval Loss:  249.0232988204956  Eval Accuracy: 0.9657142857142857  AUC:  0.9780078895463511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [0.005, 0.005, 0.005, 0.01,0.01,1,1]\n",
    "\n",
    "model = PrototypeForest(size = 1, \n",
    "                        max_depth = 3, \n",
    "                        min_samples_leaf= 3, \n",
    "                        min_samples_split=2,\n",
    "                        prototype_count=1)\n",
    "\n",
    "model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "preds = model.predict(test_features, test_bag_ids)\n",
    "metrics.accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "1bb44b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold is 1\n",
      "0.8342857142857143\n",
      "Fold is 2\n",
      "0.8092857142857144\n",
      "Fold is 3\n",
      "0.755\n",
      "Fold is 4\n",
      "0.9035714285714286\n",
      "Fold is 5\n",
      "0.8971428571428571\n",
      "Fold is 6\n",
      "0.8778571428571428\n",
      "Fold is 7\n",
      "0.6992857142857143\n",
      "Fold is 8\n",
      "0.7412280701754386\n",
      "Fold is 9\n",
      "0.924812030075188\n",
      "Fold is 10\n",
      "0.8872180451127819\n"
     ]
    }
   ],
   "source": [
    "dataset = \"BrownCreeper\"\n",
    "accuracy_list = []\n",
    "\n",
    "for repl in range(1, 2):\n",
    "    for fold in range(1, 11):\n",
    "        print(f\"Fold is {fold}\")\n",
    "        (train_features, \n",
    "             train_labels, \n",
    "             train_bag_ids,\n",
    "             test_features, \n",
    "             test_labels, \n",
    "             test_bag_ids) = train_test_split(dataset, repl, fold, 1, fit_on_full = False)\n",
    "\n",
    "        parameters = [0.005, 0.005, 0.005, 0.01,0.01,1,1]\n",
    "\n",
    "        model = PrototypeForest(size = 100, \n",
    "                                max_depth = 3, \n",
    "                                min_samples_leaf=1, \n",
    "                                min_samples_split=2,\n",
    "                                prototype_count=1,\n",
    "                                use_prototype_learner = False)\n",
    "\n",
    "        model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "        preds = model.predict_proba(test_features, test_bag_ids)\n",
    "        #acc = metrics.roc_auc_score(test_labels, preds)\n",
    "        acc = get_auc(preds, test_labels, test_bag_ids)\n",
    "        accuracy_list.append(acc)\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d5107183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2762 rows  202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1    2    3    4         5    6         7    8    9    ...  192  \\\n",
       "0     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "1     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "3     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "4     1.0    1.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "...   ...    ...  ...  ...  ...       ...  ...       ...  ...  ...  ...  ...   \n",
       "2757  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2758  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2759  0.0  100.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2760  0.0  100.0  0.0  0.0  0.0  0.006834  0.0  0.000000  0.0  0.0  ...  0.0   \n",
       "2761  0.0  100.0  0.0  0.0  0.0  0.015945  0.0  0.041667  0.0  0.0  ...  0.0   \n",
       "\n",
       "      193  194       195       196       197  198  199  200  201  \n",
       "0     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...       ...       ...       ...  ...  ...  ...  ...  \n",
       "2757  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2758  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2759  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  \n",
       "2760  0.0  0.0  0.000000  0.000000  0.006452  0.0  0.0  0.0  0.0  \n",
       "2761  0.0  0.0  0.018109  0.045455  0.000000  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2762 rows x 202 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels):\n",
    "    \n",
    "    def experiment_fn(parameters,\n",
    "                bags=bags,\n",
    "                features=features,\n",
    "                labels=labels,\n",
    "                n_classes=2,\n",
    "                folder=r\"./datasets\",\n",
    "                n_epochs=100,\n",
    "                batch_size=1,\n",
    "                display_every=5,\n",
    "                ):  \n",
    "        final_vals = []\n",
    "        reg_lambda_dist = 0.0005\n",
    "        reg_lambda_w = 0.005\n",
    "        reg_lambda_p = 0.00005\n",
    "        lr_prot = 0.00001\n",
    "        lr_weights = 0.00001\n",
    "        reg_w = 1\n",
    "        n_prototypes = 2\n",
    "        \n",
    "        for rep in range(1, 2):\n",
    "            vals = []\n",
    "            for fold in range(1, 2):\n",
    "                accs = [] \n",
    "\n",
    "                use_cuda = False\n",
    "                \n",
    "                data1 = np.vstack((labels, bags)).T\n",
    "                data = np.concatenate([data1, features], axis=1)\n",
    "\n",
    "                bags_train, labels_train = convert_to_bags(data)\n",
    "                bags_train = np.array(bags_train)\n",
    "                labels_train = np.array(labels_train)\n",
    "\n",
    "                bag_size = bags_train[0][0].shape[0]\n",
    "                step_per_epoch = len(bags_train)\n",
    "                lr_step = (step_per_epoch * 40)\n",
    "                display = (step_per_epoch * display_every)\n",
    "                max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "                model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    output_fn = torch.nn.Sigmoid()\n",
    "                else:\n",
    "                    output_fn = torch.nn.Softmax()\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "                else:\n",
    "                    loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "                optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "                optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                            lr=lr_weights)\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                train_loss_hist, eval_loss_hist = [], []\n",
    "                train_acc_hist, eval_acc_hist = [], []\n",
    "                eval_aucs = []\n",
    "                step_hist = []\n",
    "                time_hist = []\n",
    "\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    model = model.cuda()\n",
    "\n",
    "                for i in range(max_steps):\n",
    "                    np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                    start_time = time.time()\n",
    "                    batch_inp = bags_train[np_idx]\n",
    "                    targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                    batch_inp = torch.Tensor(batch_inp[0])\n",
    "                    batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                    if use_cuda and torch.cuda.is_available():\n",
    "                        targets = targets.cuda()\n",
    "                        batch_inp = batch_inp.cuda()\n",
    "\n",
    "                    logits, distances = model(batch_inp)\n",
    "                    out = output_fn(logits)\n",
    "\n",
    "                    if n_classes == 2:\n",
    "                        predicted = (out > 0.5).type(torch.int64)\n",
    "                    else:\n",
    "                        _, predicted = torch.max(out, 1)\n",
    "                    correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                    batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                    # N_prot x N_prot\n",
    "                    #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                    #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                    #cos_loss = cos_loss/M_prot_norm\n",
    "                    #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                    # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                    #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                    prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                    reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                    weight_reg = 0\n",
    "                    for param in model.linear_layer.parameters():\n",
    "                        weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                    reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                    total_loss += batch_loss\n",
    "                    min_loss = batch_loss + reg_loss\n",
    "                    min_loss.backward()\n",
    "\n",
    "                    optim1.step()\n",
    "                    optim2.step()\n",
    "\n",
    "                    if (i + 1) % lr_step == 0:\n",
    "                        print(\"LR DROP!\")\n",
    "                        optims = [optim1, optim2]\n",
    "                        for o in optims:\n",
    "                            for p in o.param_groups:\n",
    "                                p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                    if (i + 1) % display == 0:\n",
    "                        with torch.no_grad():\n",
    "                            print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                            total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                            train_loss_hist.append(total_loss.item() / display)\n",
    "                            train_acc_hist.append(correct / display)\n",
    "                            total_loss = 0\n",
    "                            correct = 0\n",
    "                            model = model.eval()\n",
    "                            e_loss = 0\n",
    "                            e_acc = 0\n",
    "                            y_true = []\n",
    "                            y_score = []\n",
    "\n",
    "                            for i in range(len(bags_train)):\n",
    "                                batch_inp = torch.Tensor(bags_train[i])\n",
    "                                batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                      batch_inp.shape[1])\n",
    "                                targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                                logits, distances = model(batch_inp)\n",
    "                                out = output_fn(logits)\n",
    "\n",
    "                                if n_classes == 2:\n",
    "                                    predicted = (out > 0.5).type(torch.int64)\n",
    "                                else:\n",
    "                                    _, predicted = torch.max(out, 1)\n",
    "                                y_true.append(targets)\n",
    "                                y_score.append(out)\n",
    "                                correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                                e_acc += correct\n",
    "                                eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                                e_loss += eval_loss\n",
    "\n",
    "                            y_true_list = [x.tolist() for x in y_true]\n",
    "                            y_score_list = [x.tolist() for x in y_score]\n",
    "                            print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                                \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                            roc_auc_score(y_true_list, y_score_list))\n",
    "                            eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                            eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                            eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                            accs.append(e_acc / len(bags_train))\n",
    "                            step_hist.append(i+1)\n",
    "                            model = model.train()\n",
    "\n",
    "                print(str(rep), \" \", str(fold), \" Final Best AUC: \",\n",
    "                    np.max(np.array(eval_aucs)))\n",
    "\n",
    "                end_time = time.time()\n",
    "                total_time = end_time - start_time\n",
    "                time_hist.append([total_time]*len(accs))\n",
    "                output_data = np.column_stack([step_hist, train_loss_hist,train_acc_hist, eval_loss_hist, eval_acc_hist, eval_aucs])\n",
    "                df = DataFrame(output_data, columns = [\"step_hist\", \"train_loss_hist\",\"train_acc_hist\", \"eval_loss_hist\", \"eval_acc_hist\", \"eval_aucs\"])\n",
    "                df['dataset'] = dataset\n",
    "                df['fold'] = fold\n",
    "                df['rep'] = rep\n",
    "                df['time_hist'] = total_time\n",
    "                export_csv = df.to_csv (r'.\\export_dataframe.csv', index = None, header=False, mode='a') #Don't forget to add '.csv' at the end of the path\n",
    "                vals.append(np.max(np.array(eval_aucs)))\n",
    "                prototypes = model.prototypes.squeeze(0).detach().numpy()\n",
    "                figure_file = \"shapelets_\" + dataset + \"_run_\" + str(0) + \"_\" + str(\n",
    "                rep) + \"_\" + str(fold) + \".png\"\n",
    "                files = \"{}_{}_run_{}_{}_{}.png\"\n",
    "                loss_file = files.format(\"loss\", dataset, \"0\", str(rep), str(fold))\n",
    "                accuracy_file = files.format(\"acc\", dataset, \"0\", str(rep), str(fold))\n",
    "\n",
    "                plt.plot(train_loss_hist, label=\"train_loss\")\n",
    "                plt.plot(eval_loss_hist, label=\"eval_loss\")\n",
    "                plt.title(\"Loss History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(loss_file)\n",
    "                plt.close()\n",
    "                plt.plot(train_loss_hist, label=\"train_loss\")\n",
    "                plt.title(\"Only Training Loss History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(\"only_train_\"+loss_file)\n",
    "                plt.close()\n",
    "                plt.plot(train_acc_hist, label=\"train_accuracy\")\n",
    "                plt.plot(eval_acc_hist, label=\"eval_accuracy\")\n",
    "                plt.title(\"Accuracy History\")\n",
    "                plt.legend()\n",
    "                plt.savefig(accuracy_file)\n",
    "                plt.close()\n",
    "                plot_prototypes(prototypes, savefile=figure_file)\n",
    "\n",
    "                final_vals.append(vals)\n",
    "\n",
    "        print(np.mean(final_vals), \"mean final vals\")\n",
    "    \n",
    "        return np.mean(final_vals)\n",
    "\n",
    "    BOUNDS = [\n",
    "     {'name': 'reg_lambda_dist',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.0005, 0.005)},\n",
    "     {'name': 'reg_lambda_w',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.005, 0.05)},\n",
    "     {'name': 'reg_lambda_p',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00005, 0.0005)},\n",
    "     {'name': 'lr_prot',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00001, 0.0001)},\n",
    "     {'name': 'lr_weights',\n",
    "      'type': 'continuous',\n",
    "      'domain': (0.00001, 0.0001)},\n",
    "     {'name': 'reg_w',\n",
    "      'type': 'discrete',\n",
    "      'domain': (1, 2)},\n",
    "     {'name': 'n_prototypes',\n",
    "      'type': 'discrete',\n",
    "      'domain': (2, 6)}  # will be x2. ie if 2, then number of prototypes will actually be 4, if 4 then 8, etc.\n",
    "    ]\n",
    "    \n",
    "    np.random.seed(777)\n",
    "    optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "         f=experiment_fn, domain=BOUNDS,\n",
    "         acquisition_type='MPI',\n",
    "         acquisition_par=0.3,\n",
    "         exact_eval=True,\n",
    "         maximize=True\n",
    "     )\n",
    "    \n",
    "    max_iter = 40\n",
    "    optimizer.run_optimization(max_iter, max_time=3600)\n",
    "    #con_plot = data_s + \"_optimizer_bayesopt.png\"\n",
    "    \n",
    "    parameters = optimizer.x_opt\n",
    "    parameters_output = np.transpose([parameters])\n",
    "    df_parameters = DataFrame(parameters_output)\n",
    "    df_parameters = df_parameters.T\n",
    "    \n",
    "    n_classes=2\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    print(parameters)\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = parameters[0]\n",
    "    reg_lambda_w = parameters[1]\n",
    "    reg_lambda_p = parameters[2]\n",
    "    lr_prot = parameters[3]\n",
    "    lr_weights = parameters[4]\n",
    "    reg_w = parameters[5]\n",
    "    n_prototypes = parameters[6]\n",
    "    n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "\n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            step_per_epoch = len(bags_train)\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "\n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            for i in range(max_steps):\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx]).type(torch.int64)\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                else:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "\n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                # N_prot x N_prot\n",
    "                #M_prot_norm = torch.mm(prot_norms.transpose(0, 1), prot_norms)\n",
    "                #cos_loss = torch.bmm(prototypes, prototypes.transpose(1,2)).squeeze(0)\n",
    "                #cos_loss = cos_loss/M_prot_norm\n",
    "                #cos_norm = cos_loss.norm(dim=0).sum() \n",
    "\n",
    "                # cos_loss = pd(model.prototypes, model.prototypes).sum()\n",
    "\n",
    "                #weight_reg = model.linear_layer.weight.norm(p=1).sum()\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() + reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]]).type(torch.int64)\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            else:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            y_true.append(targets)\n",
    "                            y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "\n",
    "                        y_true_list = [x.tolist() for x in y_true]\n",
    "                        y_score_list = [x.tolist() for x in y_score]\n",
    "                        print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                            \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        roc_auc_score(y_true_list, y_score_list))\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "\n",
    "    return model.prototypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
