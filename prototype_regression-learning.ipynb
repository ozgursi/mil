{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6879e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import ShapeletGenerator, pairwise_dist\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "def gram_matrix(mat):\n",
    "  mat = mat.squeeze(dim=0)\n",
    "  mat = torch.mm(mat, mat.t())\n",
    "  return mat\n",
    "\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "  x_norm = (x.norm(dim=2)[:, :, None])\n",
    "  y_t = y.permute(0, 2, 1).contiguous()\n",
    "  y_norm = (y.norm(dim=2)[:, None])\n",
    "  y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "  dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "  return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "class ShapeletGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_prototypes, bag_size, n_classes, features):\n",
    "        n_prototypes = int(n_prototypes)\n",
    "        super(ShapeletGenerator, self).__init__()\n",
    "\n",
    "        number_of_rows = features.shape[0]\n",
    "\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                          size=1, \n",
    "                                         replace=False)\n",
    "        \n",
    "        prot = features[random_indices, :]\n",
    "        prot = prot.reshape(1, n_prototypes, prot.shape[1])\n",
    "        prot = prot.astype(\"float32\")\n",
    "        self.prototypes = torch.from_numpy(prot).requires_grad_()\n",
    "        #self.prototypes = (torch.randn(\n",
    "        #    (1, n_prototypes, bag_size))).requires_grad_()\n",
    "        if n_classes == 2:\n",
    "            n_classes = 1\n",
    "        self.linear_layer = torch.nn.Linear(3 * n_prototypes, n_classes, bias=False)\n",
    "        #self.linear_layer.weight = torch.nn.Parameter(self.linear_layer.weight/100000)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def pairwise_distances(self, x, y):\n",
    "        #x_norm = (x.norm(dim=2)[:, :, None])\n",
    "        #y_t = y.permute(0, 2, 1).contiguous()\n",
    "        #y_norm = (y.norm(dim=2)[:, None])\n",
    "        #y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "        #dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "        dist = ((x - y).norm(dim=2)[:, None]).permute(0, 2, 1).contiguous()\n",
    "        return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "    def get_output(self, batch_inp):\n",
    "        dist = self.pairwise_distances(batch_inp, self.prototypes)\n",
    "        min_dist = dist.min(dim=1)[0]\n",
    "        max_dist = dist.max(dim=1)[0]\n",
    "        mean_dist = dist.mean(dim=1)\n",
    "        all_features = torch.cat([min_dist, max_dist, mean_dist], dim=1)\n",
    "        logits = self.linear_layer(all_features)\n",
    "\n",
    "        return logits, all_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, distances = self.get_output(x)\n",
    "        if self.n_classes == 1:\n",
    "          logits = logits.view(1)\n",
    "        return logits, distances\n",
    "\n",
    "\n",
    "def convert_to_bags(data,\n",
    "                    split_instances=False,\n",
    "                    instance_norm=False,\n",
    "                    split_ratio=0.2,\n",
    "                    stride_ratio=0.5):\n",
    "  bags = []\n",
    "  labels = []\n",
    "  current_bag = []\n",
    "  current_label = data[0, 0]\n",
    "  cur = data[0, 1]\n",
    "  instance_size = np.round(split_ratio * data[0, 2:].shape[0]).astype(\"int\")\n",
    "  stride = np.round(stride_ratio * instance_size).astype(\"int\")\n",
    "\n",
    "  for i in range(data.shape[0]):\n",
    "    if data[i, 1] == cur:\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "    else:\n",
    "      bags.append(np.array(current_bag))\n",
    "      labels.append(np.array(current_label))\n",
    "      current_label = data[i, 0]\n",
    "      current_bag = []\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "      cur = data[i, 1]\n",
    "  bags.append(np.array(current_bag))\n",
    "  labels.append(np.array(current_label, dtype=\"int32\"))\n",
    "  return bags, labels\n",
    "\n",
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels,\n",
    "                   early_stopping_round = 10):\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = generate_random(parameters[0][0], parameters[0][1])\n",
    "    reg_lambda_w = generate_random(parameters[1][0], parameters[1][1])\n",
    "    reg_lambda_p = generate_random(parameters[2][0], parameters[2][1])\n",
    "    lr_prot = generate_random(parameters[3][0], parameters[3][1])\n",
    "    lr_weights = generate_random(parameters[4][0], parameters[4][1])\n",
    "    reg_w = 1\n",
    "    n_prototypes = 1\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "            \n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes, features)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            elif n_classes == 3:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "            else:\n",
    "                output_fn = torch.nn.Linear(n_classes,1)\n",
    "            \n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "                \n",
    "            loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            cont = True\n",
    "            \n",
    "            max_stagnation = 0 # number of epochs without improvement to tolerate\n",
    "            best_prototype = None\n",
    "            best_score = 1000000000\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_steps and max_stagnation < early_stopping_round:\n",
    "                i += 1\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx])\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                elif n_classes == 3:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                else:\n",
    "                    predicted = out\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "                \n",
    "                batch_loss = loss(logits, targets.type(torch.float32))\n",
    "\n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() - reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "\n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        #print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        #total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        correct = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "                        loss_list = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]])\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            elif n_classes == 3:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            else:\n",
    "                                predicted = out\n",
    "                            #y_true.append(targets)\n",
    "                            #y_score.append(out)\n",
    "                            correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            e_acc += correct\n",
    "                            eval_loss = loss(logits, targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "                            loss_list.append(e_loss)\n",
    "\n",
    "                        #y_true_list = [x.tolist() for x in y_true]\n",
    "                        #y_score_list = [x.tolist() for x in y_score]\n",
    "                        #score_auc = roc_auc_score(y_true_list, y_score_list)\n",
    "                        #print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                        #    \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        #score_auc)\n",
    "                        loss_score = np.mean(loss_list)\n",
    "                        \n",
    "                        #if score_auc > best_score:\n",
    "                        if loss_score < best_score:\n",
    "                            #best_score = score_auc\n",
    "                            best_score = loss_score\n",
    "                            best_prototype = model.prototypes\n",
    "                            max_stagnation = 0\n",
    "                        else:\n",
    "                            max_stagnation += 1\n",
    "                        \n",
    "                        #print(\"max_stagnation \", max_stagnation)\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        #eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "    \n",
    "    return best_prototype, [x for x in model.linear_layer.parameters()]\n",
    "\n",
    "parameters = [[0.00001, 0.05], [0.00001, 0.05],[0.00001, 0.05], [0.00001, 0.05], [0.00001, 0.05], [1],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37c458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "        \n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self,\n",
    "                train_features,\n",
    "                 feature_types = [\"min\", \"max\", \"mean\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner=True,\n",
    "                 early_stopping_round = 3):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.train_features = train_features\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        if self.use_prototype_learner:\n",
    "            prototypes, _ = find_prototype(bags, features, labels, self.early_stopping_round)\n",
    "            check = prototypes.cpu().detach().numpy()\n",
    "\n",
    "            check.resize(check.shape[1], check.shape[2])\n",
    "            \n",
    "            return check\n",
    "        \n",
    "        else:\n",
    "            number_of_rows = features.shape[0]\n",
    "            random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=prototype_count, \n",
    "                                              replace=False)\n",
    "            \n",
    "            prot = features[random_indices, :]\n",
    "            if len(prot.shape) == 1:\n",
    "                prot = prot.reshape(1, prot.shape[0])\n",
    "            \n",
    "            return prot\n",
    "\n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        return np.asarray(np.sum(y)/y.size)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "        \n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        _, index  = np.unique(bag_ids, return_index=True)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_mean = np.add.reduceat(distances[:, i], index)\n",
    "                mean_vals = np.repeat(group_mean/bin_count, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "        \n",
    "        return np.array(np.transpose(feature_list))\n",
    "\n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        \n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_via_prototype, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        \n",
    "        bdc = tree.DecisionTreeRegressor(\n",
    "            min_samples_split=2,\n",
    "            criterion=\"mae\"\n",
    "        )\n",
    "        bdc.fit(features_via_prototype[index], labels[index])\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        labels_left = labels[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "\n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids)) < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            \n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "            \n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids_left)) < self.min_samples_leaf or len(np.unique(bag_ids_right)) < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "            \n",
    "            _, index_left  = np.unique(bag_ids_left, return_index=True)\n",
    "            _, index_right  = np.unique(bag_ids_right, return_index=True)\n",
    "            \n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left[index_left])\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right[index_right])\n",
    "\n",
    "            # splitting recursively\n",
    "            \n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "\n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "\n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "\n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "\n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "\n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree)\n",
    "            \n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)\n",
    "\n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"],\n",
    "                max_depth = 8, \n",
    "                min_samples_leaf = 2, \n",
    "                min_samples_split = 2, \n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        group_min = np.minimum.reduceat(labels, index)\n",
    "        \n",
    "        if self.size == 1:\n",
    "            bag_size = math.ceil(group_min.shape[0] * 1)\n",
    "        else:\n",
    "            bag_size = math.ceil(group_min.shape[0] * 0.8)\n",
    "  \n",
    "        bags_all = np.random.choice(ids, bag_size, replace=False)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1),\n",
    "                                          train_labels.reshape(train_labels.shape[0],1)],\n",
    "                                         axis=1))\n",
    "        \n",
    "        indices_all = df[df[0].isin(bags_all)].index.to_numpy()\n",
    "        inbag_indices = indices_all\n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "        \n",
    "        return inbag_indices, outbag_indices\n",
    "\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            \n",
    "            (inbag_indices, _) = self.sample(features, labels, bag_ids)\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            while tree.Tree.right is None:\n",
    "                tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            self._trees.append(tree)\n",
    "            \n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return np.sum(preds, axis=1)/self.size\n",
    "   \n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0, 1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "\n",
    "    #sort_index = np.argsort(bag_ids)\n",
    "    #bag_ids = bag_ids[sort_index]\n",
    "    #features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "\n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "          \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    \n",
    "    #for i in range(2, 94):\n",
    "    #    clean_data = train_data[(train_data[i] != 0) & (train_data[i] != -32767)]\n",
    "    #    mean = clean_data[i].mean()\n",
    "    #    train_data[(train_data[i] == 0) | (train_data[i] == -32767)] = mean\n",
    "\n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n",
    "\n",
    "def generate_random(lower, upper):\n",
    "    random_number = random.random()\n",
    "    random_number = random_number + lower\n",
    "    random_range = upper - lower\n",
    "    random_number = random_number*random_range\n",
    "    return random_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c3ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "> \u001b[0;32m<ipython-input-19-18fe482d550d>\u001b[0m(239)\u001b[0;36mfind_prototype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    237 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    238 \u001b[0;31m                \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 239 \u001b[0;31m                \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    240 \u001b[0;31m                \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    241 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-84b7c79a052e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             early_stopping_round= 5)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mearly_stopping_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             )\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b3d3abd4947e>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mfeatures_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_via_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b3d3abd4947e>\u001b[0m in \u001b[0;36mprototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_prototype_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18fe482d550d>\u001b[0m in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18fe482d550d>\u001b[0m in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"WheatYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=2,\n",
    "                            max_depth=2,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_learning_{dataset}.csv\")\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "all_df.to_csv(f\"./performance/predictions__{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56524dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.3755613497683672]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1604d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets_regression/cv/CornYields.csv_rep1_fold1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-92c4697f06d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m          \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m          \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m          test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     model = PrototypeForest(size=100,\n",
      "\u001b[0;32m<ipython-input-22-ba21a5cb1a6c>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(dataset, rep, fold, explained_variance, fit_on_full, custom, cols)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m#testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/{dataset}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtestbags\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets_regression/cv/CornYields.csv_rep1_fold1.txt'"
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"CornYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=100,\n",
    "                            max_depth=8,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_{dataset}.csv\")\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "all_df.to_csv(f\"./performance/predictions_{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf5b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    data = pd.read_csv(f\"./datasets_regression/syn_new/{dataset}.csv\", header=None, sep=\",\")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/syn_new/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", sep=\",\")\n",
    "    \n",
    "    data = np.round(data,2)\n",
    "    \n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "\n",
    "    train_data = data[~data[1].isin(testbags[\"x\"].tolist())]    \n",
    "    \n",
    "    test_data = data[data[1].isin(testbags[\"x\"].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    train_features = np.round(train_features,2)\n",
    "    test_features = np.round(test_features,2)\n",
    "\n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99ddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b68d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets_regression/syn_new/nBag_10_nFeat_2_nInsPerBag_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8ef07351219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m                  \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                  \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                  test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = True)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             model = PrototypeForest(size=1,\n",
      "\u001b[0;32m<ipython-input-9-a8ef07351219>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(dataset, rep, fold, explained_variance, fit_on_full, custom, cols)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/syn_new/{dataset}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtestbags\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets_regression/syn_new/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets_regression/syn_new/nBag_10_nFeat_2_nInsPerBag_2.csv'"
     ]
    }
   ],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    data = pd.read_csv(f\"./datasets_regression/syn_new/{dataset}.csv\", header=None, sep=\",\")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/syn_new/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", sep=\",\")\n",
    "    \n",
    "    data = np.round(data,2)\n",
    "    \n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "\n",
    "    train_data = data[~data[1].isin(testbags[\"x\"].tolist())]    \n",
    "    \n",
    "    test_data = data[data[1].isin(testbags[\"x\"].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    train_features = np.round(train_features,2)\n",
    "    test_features = np.round(test_features,2)\n",
    "\n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n",
    "\n",
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets2/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "dataset = [\"nBag_10_nFeat_2_nInsPerBag_2\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 6):\n",
    "        for fold in range(1, 11):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = True)\n",
    "\n",
    "            model = PrototypeForest(size=1,\n",
    "                                    max_depth=2,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "\n",
    "#perf_df.to_csv(f\"./performance/performance_learning_synthetic.csv\")\n",
    "\n",
    "#all_df.to_csv(f\"./performance/predictions_learning_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b4e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_25_nFeat_2_nInsPerBag_2, rep 1, fold 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets2/nBag_25_nFeat_2_nInsPerBag_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e563c9d9fa43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                  \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                  \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                  test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = True)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             model = PrototypeForest(size=100,\n",
      "\u001b[0;32m<ipython-input-40-b8ea502429fe>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(dataset, rep, fold, explained_variance, fit_on_full, custom, cols)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplained_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_on_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets2/{dataset}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtestbags\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./datasets2/{dataset}.csv_rep{rep}_fold{fold}.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmin_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestbags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets2/nBag_25_nFeat_2_nInsPerBag_2.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets_regression/syn_new/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 6):\n",
    "        for fold in range(1, 11):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = True)\n",
    "\n",
    "            model = PrototypeForest(size=100,\n",
    "                                    max_depth=4,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "\n",
    "perf_df.to_csv(f\"./performance/performance_multitree_synthetic.csv\")\n",
    "\n",
    "all_df.to_csv(f\"./performance/predictions_multitree_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "772033a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 1\n",
      "> \u001b[0;32m<ipython-input-30-f265749327ad>\u001b[0m(74)\u001b[0;36mget_output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     72 \u001b[0;31m        \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 74 \u001b[0;31m        \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m        \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m        \u001b[0mmean_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b8ea502429fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                     early_stopping_round= 5)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mearly_stopping_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             )\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b3d3abd4947e>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mfeatures_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_via_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b3d3abd4947e>\u001b[0m in \u001b[0;36mprototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_prototype_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f265749327ad>\u001b[0m in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mbatch_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f265749327ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m           \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f265749327ad>\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(self, batch_inp)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmean_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f265749327ad>\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(self, batch_inp)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmean_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    data = pd.read_csv(f\"./datasets2/{dataset}.csv\", header=None)\n",
    "    testbags =  pd.read_csv(f\"./datasets2/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "    \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]\n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance,\n",
    "                         svd_solver = \"full\")),\n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "    #train_features = pipe.transform(train_features)\n",
    "    #test_features = pipe.transform(test_features)\n",
    "    return (\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        train_bag_ids,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "        test_bag_ids)\n",
    "\n",
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets2/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "datasets = [\"nBag_10_nFeat_2_nInsPerBag_2\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 6):\n",
    "        for fold in range(1, 11):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = False)\n",
    "\n",
    "            model = PrototypeForest(size=1,\n",
    "                                    max_depth=2,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "            \n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "\n",
    "#perf_df.to_csv(f\"./performance/performance_learning_synthetic.csv\")\n",
    "\n",
    "#all_df.to_csv(f\"./performance/predictions_learning_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b9c561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rep</th>\n",
       "      <th>fold</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.476602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.328550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.138767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.141307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.353076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.518415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.400337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.691329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.954532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.219492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.053719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.346720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.681666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.319183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.144115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.034253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.143819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.256916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.576051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.237676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.174750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.346720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.400451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.143819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.141307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.757512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.345009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.348509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.044354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.044884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.138759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.141307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.045046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.346720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.289516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.364425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.430697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.483019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.362692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.045046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>nBag_10_nFeat_2_nInsPerBag_2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.835068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         dataset  rep  fold     score\n",
       "0   nBag_10_nFeat_2_nInsPerBag_2    1     1  0.476602\n",
       "1   nBag_10_nFeat_2_nInsPerBag_2    1     2  0.328550\n",
       "2   nBag_10_nFeat_2_nInsPerBag_2    1     3  0.138767\n",
       "3   nBag_10_nFeat_2_nInsPerBag_2    1     4  1.141307\n",
       "4   nBag_10_nFeat_2_nInsPerBag_2    1     5  0.023132\n",
       "5   nBag_10_nFeat_2_nInsPerBag_2    1     6  0.353076\n",
       "6   nBag_10_nFeat_2_nInsPerBag_2    1     7  0.518415\n",
       "7   nBag_10_nFeat_2_nInsPerBag_2    1     8  0.400337\n",
       "8   nBag_10_nFeat_2_nInsPerBag_2    1     9  0.691329\n",
       "9   nBag_10_nFeat_2_nInsPerBag_2    1    10  0.954532\n",
       "10  nBag_10_nFeat_2_nInsPerBag_2    2     1  0.219492\n",
       "11  nBag_10_nFeat_2_nInsPerBag_2    2     2  0.036200\n",
       "12  nBag_10_nFeat_2_nInsPerBag_2    2     3  0.053719\n",
       "13  nBag_10_nFeat_2_nInsPerBag_2    2     4  0.346720\n",
       "14  nBag_10_nFeat_2_nInsPerBag_2    2     5  0.681666\n",
       "15  nBag_10_nFeat_2_nInsPerBag_2    2     6  1.319183\n",
       "16  nBag_10_nFeat_2_nInsPerBag_2    2     7  0.144115\n",
       "17  nBag_10_nFeat_2_nInsPerBag_2    2     8  0.034253\n",
       "18  nBag_10_nFeat_2_nInsPerBag_2    2     9  0.143819\n",
       "19  nBag_10_nFeat_2_nInsPerBag_2    2    10  1.256916\n",
       "20  nBag_10_nFeat_2_nInsPerBag_2    3     1  0.014827\n",
       "21  nBag_10_nFeat_2_nInsPerBag_2    3     2  0.016080\n",
       "22  nBag_10_nFeat_2_nInsPerBag_2    3     3  0.576051\n",
       "23  nBag_10_nFeat_2_nInsPerBag_2    3     4  0.237676\n",
       "24  nBag_10_nFeat_2_nInsPerBag_2    3     5  0.174750\n",
       "25  nBag_10_nFeat_2_nInsPerBag_2    3     6  0.346720\n",
       "26  nBag_10_nFeat_2_nInsPerBag_2    3     7  0.400451\n",
       "27  nBag_10_nFeat_2_nInsPerBag_2    3     8  0.143819\n",
       "28  nBag_10_nFeat_2_nInsPerBag_2    3     9  1.141307\n",
       "29  nBag_10_nFeat_2_nInsPerBag_2    3    10  0.900755\n",
       "30  nBag_10_nFeat_2_nInsPerBag_2    4     1  0.757512\n",
       "31  nBag_10_nFeat_2_nInsPerBag_2    4     2  0.345009\n",
       "32  nBag_10_nFeat_2_nInsPerBag_2    4     3  0.082480\n",
       "33  nBag_10_nFeat_2_nInsPerBag_2    4     4  0.348509\n",
       "34  nBag_10_nFeat_2_nInsPerBag_2    4     5  0.044354\n",
       "35  nBag_10_nFeat_2_nInsPerBag_2    4     6  0.044884\n",
       "36  nBag_10_nFeat_2_nInsPerBag_2    4     7  0.138759\n",
       "37  nBag_10_nFeat_2_nInsPerBag_2    4     8  1.141307\n",
       "38  nBag_10_nFeat_2_nInsPerBag_2    4     9  0.045046\n",
       "39  nBag_10_nFeat_2_nInsPerBag_2    4    10  0.964066\n",
       "40  nBag_10_nFeat_2_nInsPerBag_2    5     1  0.066518\n",
       "41  nBag_10_nFeat_2_nInsPerBag_2    5     2  0.034253\n",
       "42  nBag_10_nFeat_2_nInsPerBag_2    5     3  0.346720\n",
       "43  nBag_10_nFeat_2_nInsPerBag_2    5     4  0.289516\n",
       "44  nBag_10_nFeat_2_nInsPerBag_2    5     5  0.364425\n",
       "45  nBag_10_nFeat_2_nInsPerBag_2    5     6  0.430697\n",
       "46  nBag_10_nFeat_2_nInsPerBag_2    5     7  0.483019\n",
       "47  nBag_10_nFeat_2_nInsPerBag_2    5     8  0.362692\n",
       "48  nBag_10_nFeat_2_nInsPerBag_2    5     9  0.045046\n",
       "49  nBag_10_nFeat_2_nInsPerBag_2    5    10  0.835068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
