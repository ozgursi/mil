{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6879e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import ShapeletGenerator, pairwise_dist\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "def gram_matrix(mat):\n",
    "  mat = mat.squeeze(dim=0)\n",
    "  mat = torch.mm(mat, mat.t())\n",
    "  return mat\n",
    "\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "  x_norm = (x.norm(dim=2)[:, :, None])\n",
    "  y_t = y.permute(0, 2, 1).contiguous()\n",
    "  y_norm = (y.norm(dim=2)[:, None])\n",
    "  y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "  dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "  return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "class ShapeletGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_prototypes, bag_size, n_classes, features, batch_size):\n",
    "        n_prototypes = int(n_prototypes)\n",
    "        super(ShapeletGenerator, self).__init__()\n",
    "\n",
    "        number_of_rows = features.shape[0]\n",
    "\n",
    "        random_indices = np.random.choice(number_of_rows, \n",
    "                                          size=1, \n",
    "                                         replace=False)\n",
    "        \n",
    "        prot = features[random_indices, :]\n",
    "        prot = prot.reshape(1, n_prototypes, prot.shape[1])\n",
    "        prot = prot.astype(\"float32\")\n",
    "        self.prototypes = torch.from_numpy(prot).requires_grad_()\n",
    "        self.batch_size = batch_size\n",
    "        #self.prototypes = (torch.randn(\n",
    "        #    (1, n_prototypes, bag_size))).requires_grad_()\n",
    "        if n_classes == 2:\n",
    "            n_classes = 1\n",
    "        self.linear_layer = torch.nn.Linear(3 * n_prototypes, self.batch_size, bias=False)\n",
    "        #self.linear_layer.weight = torch.nn.Parameter(self.linear_layer.weight/100000)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def pairwise_distances(self, x, y):\n",
    "        #x_norm = (x.norm(dim=2)[:, :, None])\n",
    "        #y_t = y.permute(0, 2, 1).contiguous()\n",
    "        #y_norm = (y.norm(dim=2)[:, None])\n",
    "        #y_t = torch.cat([y_t] * x.shape[0], dim=0)\n",
    "        #dist = x_norm + y_norm - 2.0 * torch.bmm(x, y_t)\n",
    "        dist = ((x - y).norm(dim=2)[:, None]).permute(0, 2, 1).contiguous()\n",
    "        return torch.clamp(dist, 0.0, np.inf)\n",
    "\n",
    "    def get_output(self, batch_inp):\n",
    "        dist = self.pairwise_distances(batch_inp, self.prototypes)\n",
    "        min_dist = dist.min(dim=1)[0]\n",
    "        max_dist = dist.max(dim=1)[0]\n",
    "        mean_dist = dist.mean(dim=1)\n",
    "        all_features = torch.cat([min_dist, max_dist, mean_dist], dim=1)\n",
    "        logits = self.linear_layer(all_features)\n",
    "\n",
    "        return logits, all_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits, distances = self.get_output(x)\n",
    "        if self.n_classes == 1:\n",
    "          logits = logits.view(1)\n",
    "        return logits, distances\n",
    "\n",
    "\n",
    "def convert_to_bags(data,\n",
    "                    split_instances=False,\n",
    "                    instance_norm=False,\n",
    "                    split_ratio=0.2,\n",
    "                    stride_ratio=0.5):\n",
    "  bags = []\n",
    "  labels = []\n",
    "  current_bag = []\n",
    "  current_label = data[0, 0]\n",
    "  cur = data[0, 1]\n",
    "  instance_size = np.round(split_ratio * data[0, 2:].shape[0]).astype(\"int\")\n",
    "  stride = np.round(stride_ratio * instance_size).astype(\"int\")\n",
    "\n",
    "  for i in range(data.shape[0]):\n",
    "    if data[i, 1] == cur:\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "    else:\n",
    "      bags.append(np.array(current_bag))\n",
    "      labels.append(np.array(current_label))\n",
    "      current_label = data[i, 0]\n",
    "      current_bag = []\n",
    "      instance = data[i, 2:]\n",
    "      if instance_norm:\n",
    "        instance = (instance - np.mean(instance)) / (1e-08 + np.std(instance))\n",
    "      if split_instances:\n",
    "        size = instance.shape[0]\n",
    "        window = instance_size\n",
    "        while True:\n",
    "          current_bag.append(instance[window - instance_size:window])\n",
    "          window += stride\n",
    "          if window >= size:\n",
    "            window = size\n",
    "            current_bag.append(instance[window - instance_size:window])\n",
    "            break\n",
    "      else:\n",
    "        current_bag.append(instance)\n",
    "      cur = data[i, 1]\n",
    "  bags.append(np.array(current_bag))\n",
    "  labels.append(np.array(current_label, dtype=\"int32\"))\n",
    "  return bags, labels\n",
    "\n",
    "def find_prototype(bags,\n",
    "                   features,\n",
    "                   labels,\n",
    "                   early_stopping_round = 10):\n",
    "    n_epochs=100\n",
    "    batch_size=1\n",
    "    display_every=5\n",
    "    final_vals = []\n",
    "    reg_lambda_dist = generate_random(parameters[0][0], parameters[0][1])\n",
    "    reg_lambda_w = generate_random(parameters[1][0], parameters[1][1])\n",
    "    reg_lambda_p = generate_random(parameters[2][0], parameters[2][1])\n",
    "    lr_prot = generate_random(parameters[3][0], parameters[3][1])\n",
    "    lr_weights = generate_random(parameters[4][0], parameters[4][1])\n",
    "    reg_w = 1\n",
    "    n_prototypes = 1\n",
    "    #reg_lambda_dist = 0.0005\n",
    "    #reg_lambda_w = 0.005\n",
    "    #reg_lambda_p = 0.00005\n",
    "    #lr_prot = 0.00001\n",
    "    #lr_weights = 0.00001\n",
    "    #reg_w = 1\n",
    "    #n_prototypes = 2\n",
    "    #n_prototypes = n_prototypes*2\n",
    "    \n",
    "    data1 = np.vstack((labels, bags)).T\n",
    "    data = np.concatenate([data1, features], axis=1)\n",
    "    n_classes = len(np.unique(labels))\n",
    "    bags_train, labels_train = convert_to_bags(data)\n",
    "    bags_train = np.array(bags_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    for rep in range(1, 2):\n",
    "        vals = []\n",
    "        for fold in range(1, 2):\n",
    "            accs = [] \n",
    "\n",
    "            use_cuda = False\n",
    "\n",
    "            bag_size = bags_train[0][0].shape[0]\n",
    "            #step_per_epoch = len(bags_train)\n",
    "            step_per_epoch = len(np.unique(bags))\n",
    "            lr_step = (step_per_epoch * 40)\n",
    "            display = (step_per_epoch * display_every)\n",
    "            max_steps = n_epochs * step_per_epoch\n",
    "            #max_steps = 3\n",
    "            model = ShapeletGenerator(n_prototypes, bag_size, n_classes, features, batch_size)\n",
    "\n",
    "            if n_classes == 2:\n",
    "                output_fn = torch.nn.Sigmoid()\n",
    "            elif n_classes == 3:\n",
    "                output_fn = torch.nn.Softmax()\n",
    "            else:\n",
    "                output_fn = torch.nn.Linear(batch_size, batch_size)\n",
    "            \n",
    "            if n_classes == 2:\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "            else:\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "                \n",
    "            loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "            optim1 = torch.optim.Adam([model.prototypes], lr=lr_prot)\n",
    "            optim2 = torch.optim.Adam(list(model.linear_layer.parameters()),\n",
    "                        lr=lr_weights)\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            train_loss_hist, eval_loss_hist = [], []\n",
    "            train_acc_hist, eval_acc_hist = [], []\n",
    "            eval_aucs = []\n",
    "            step_hist = []\n",
    "            time_hist = []\n",
    "\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            cont = True\n",
    "            \n",
    "            max_stagnation = 0 # number of epochs without improvement to tolerate\n",
    "            best_prototype = None\n",
    "            best_score = 1000000000\n",
    "            i = 0\n",
    "            \n",
    "            while i < max_steps and max_stagnation < early_stopping_round:\n",
    "                i += 1\n",
    "                np_idx = np.random.choice(bags_train.shape[0], batch_size)\n",
    "                start_time = time.time()\n",
    "                batch_inp = bags_train[np_idx]\n",
    "                targets = torch.Tensor(labels_train[np_idx])\n",
    "                batch_inp = torch.Tensor(batch_inp[0])\n",
    "                batch_inp = batch_inp.view(1, batch_inp.shape[0], batch_inp.shape[1])\n",
    "                if use_cuda and torch.cuda.is_available():\n",
    "                    targets = targets.cuda()\n",
    "                    batch_inp = batch_inp.cuda()\n",
    "\n",
    "                logits, distances = model(batch_inp)\n",
    "                out = output_fn(logits)\n",
    "\n",
    "                if n_classes == 2:\n",
    "                    predicted = (out > 0.5).type(torch.int64)\n",
    "                elif n_classes == 3:\n",
    "                    _, predicted = torch.max(out, 1)\n",
    "                else:\n",
    "                    predicted = out\n",
    "                correct += (predicted == targets).type(torch.float32).mean().item()\n",
    "                \n",
    "                batch_loss = loss(out.reshape(batch_size), targets.type(torch.float32))\n",
    "                batch_loss = batch_loss/batch_size\n",
    "                \n",
    "                prototypes_pairwise = pairwise_dist(model.prototypes, model.prototypes)\n",
    "                reg_prototypes = prototypes_pairwise.sum()\n",
    "\n",
    "                weight_reg = 0\n",
    "                for param in model.linear_layer.parameters():\n",
    "                    weight_reg += param.norm(p=reg_w).sum()\n",
    "\n",
    "                reg_loss = reg_lambda_w*weight_reg + reg_lambda_dist*distances.sum() - reg_prototypes*reg_lambda_p\n",
    "                total_loss += batch_loss\n",
    "                min_loss = batch_loss + reg_loss\n",
    "                min_loss.backward()\n",
    "\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "                \n",
    "                if (i + 1) % lr_step == 0:\n",
    "                    print(\"LR DROP!\")\n",
    "                    optims = [optim1, optim2]\n",
    "                    for o in optims:\n",
    "                        for p in o.param_groups:\n",
    "                            p[\"lr\"] = p[\"lr\"] / 2\n",
    "                                    \n",
    "                if (i + 1) % display == 0:\n",
    "                    with torch.no_grad():\n",
    "                        #print(\"Step : \", str(i + 1), \"Loss: \",\n",
    "                        #total_loss.item() / display, \" accuracy: \", correct / (display))\n",
    "                        train_loss_hist.append(total_loss.item() / display)\n",
    "                        train_acc_hist.append(correct / display)\n",
    "                        total_loss = 0\n",
    "                        model = model.eval()\n",
    "                        e_loss = 0\n",
    "                        e_acc = 0\n",
    "                        y_true = []\n",
    "                        y_score = []\n",
    "                        loss_list = []\n",
    "\n",
    "                        for i in range(len(bags_train)):\n",
    "                            batch_inp = torch.Tensor(bags_train[i])\n",
    "                            batch_inp = batch_inp.view(1, batch_inp.shape[0],\n",
    "                                                  batch_inp.shape[1])\n",
    "                            targets = torch.Tensor([labels_train[i]])\n",
    "                            logits, distances = model(batch_inp)\n",
    "                            out = output_fn(logits)\n",
    "\n",
    "                            if n_classes == 2:\n",
    "                                predicted = (out > 0.5).type(torch.int64)\n",
    "                            elif n_classes == 3:\n",
    "                                _, predicted = torch.max(out, 1)\n",
    "                            else:\n",
    "                                predicted = out\n",
    "                            #y_true.append(targets)\n",
    "                            #y_score.append(out)\n",
    "                            #correct = (predicted == targets).type(torch.float32).mean().item()\n",
    "                            #e_acc += correct\n",
    "                            eval_loss = loss(out.reshape(batch_size), targets.type(torch.float32)).item()\n",
    "                            e_loss += eval_loss\n",
    "                            loss_list.append(e_loss)\n",
    "\n",
    "                        #y_true_list = [x.tolist() for x in y_true]\n",
    "                        #y_score_list = [x.tolist() for x in y_score]\n",
    "                        #score_auc = roc_auc_score(y_true_list, y_score_list)\n",
    "                        #print(\"Eval Loss: \", e_loss / len(bags_train),\n",
    "                        #    \" Eval Accuracy:\", e_acc / len(bags_train), \" AUC: \",\n",
    "                        #score_auc)\n",
    "                        loss_score = np.mean(loss_list)\n",
    "                        \n",
    "                        #if score_auc > best_score:\n",
    "                        if loss_score < best_score:\n",
    "                            #best_score = score_auc\n",
    "                            best_score = loss_score\n",
    "                            best_prototype = model.prototypes\n",
    "                            max_stagnation = 0\n",
    "                        else:\n",
    "                            max_stagnation += 1\n",
    "                                                \n",
    "                        #print(\"max_stagnation \", max_stagnation)\n",
    "                        eval_loss_hist.append(e_loss / len(bags_train))\n",
    "                        eval_acc_hist.append(e_acc / len(bags_train))\n",
    "                        #eval_aucs.append(roc_auc_score(y_true_list, y_score_list))\n",
    "                        accs.append(e_acc / len(bags_train))\n",
    "                        step_hist.append(i+1)\n",
    "                        model = model.train()\n",
    "    \n",
    "    return best_prototype, [x for x in model.linear_layer.parameters()]\n",
    "\n",
    "parameters = [[0.00001, 0.05], [0.00001, 0.05],[0.00001, 0.05], [0.00001, 0.05], [0.00001, 0.05], [1],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        \n",
    "        self.prototype = None\n",
    "        \n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        \n",
    "        self.is_terminal = False\n",
    "        \n",
    "class PrototypeTreeClassifier:\n",
    "    def __init__(self,\n",
    "                train_features,\n",
    "                 feature_types = [\"min\", \"max\", \"mean\"], \n",
    "                 max_depth = 3, \n",
    "                 min_samples_leaf = 1, \n",
    "                 min_samples_split = 2, \n",
    "                 prototype_count = 1,\n",
    "                 use_prototype_learner=True,\n",
    "                 early_stopping_round = 3):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.feature_types = feature_types\n",
    "        self.train_features = train_features\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.Tree = None\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def prototype(self, bags, features, labels, prototype_count):\n",
    "        if self.use_prototype_learner:\n",
    "            prototypes, _ = find_prototype(bags, features, labels, self.early_stopping_round)\n",
    "            check = prototypes.cpu().detach().numpy()\n",
    "\n",
    "            check.resize(check.shape[1], check.shape[2])\n",
    "            \n",
    "            return check\n",
    "        \n",
    "        else:\n",
    "            number_of_rows = features.shape[0]\n",
    "            random_indices = np.random.choice(number_of_rows, \n",
    "                                              size=prototype_count, \n",
    "                                              replace=False)\n",
    "            \n",
    "            prot = features[random_indices, :]\n",
    "            if len(prot.shape) == 1:\n",
    "                prot = prot.reshape(1, prot.shape[0])\n",
    "            \n",
    "            return prot\n",
    "\n",
    "    def nodeProbas(self, y):\n",
    "        # for each unique label calculate the probability for it\n",
    "        probas = []\n",
    "\n",
    "        return np.asarray(np.sum(y)/y.size)\n",
    "\n",
    "    def features_via_prototype(self, feature_types, features, bag_ids, prototypes):\n",
    "        distances = self.calculate_distances(features, prototypes)\n",
    "        \n",
    "        bin_count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        _, index  = np.unique(bag_ids, return_index=True)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            if \"max\" in feature_types:\n",
    "                group_max = np.maximum.reduceat(distances[:, i], index)\n",
    "                max_vals = np.repeat(group_max, bin_count)\n",
    "                feature_list.append(max_vals)\n",
    "\n",
    "            if \"min\" in feature_types:\n",
    "                group_min = np.minimum.reduceat(distances[:, i], index)\n",
    "                min_vals = np.repeat(group_min, bin_count)\n",
    "                feature_list.append(min_vals)\n",
    "\n",
    "            if \"mean\" in feature_types:\n",
    "                group_mean = np.add.reduceat(distances[:, i], index)\n",
    "                mean_vals = np.repeat(group_mean/bin_count, bin_count)\n",
    "                feature_list.append(mean_vals)\n",
    "        \n",
    "        return np.array(np.transpose(feature_list))\n",
    "\n",
    "    def dist1d(self, features, prototypes, distance_type=\"l2\"):\n",
    "        if distance_type == \"l2\":\n",
    "\n",
    "            distance = np.linalg.norm(features - prototypes, axis=1)\n",
    "        elif distance_type == \"l1\":\n",
    "            distance = np.abs(features - prototypes)\n",
    "            distance = np.sum(distance, axis=1)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distances(self, features, prototypes):\n",
    "        feature_list = []\n",
    "        \n",
    "        for i in range(0, prototypes.shape[0]):\n",
    "            data = self.dist1d(features, prototypes[i], distance_type=\"l2\")\n",
    "            feature_list.append(data)\n",
    "        data = np.column_stack(feature_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calcBestSplit(self, features, features_via_prototype, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        \n",
    "        bdc = tree.DecisionTreeRegressor(\n",
    "            min_samples_split=2,\n",
    "            criterion=\"absolute_error\"\n",
    "        )\n",
    "        bdc.fit(features_via_prototype[index], labels[index])\n",
    "        \n",
    "        threshold = bdc.tree_.threshold[0]\n",
    "        split_col = bdc.tree_.feature[0]\n",
    "\n",
    "        features_left = features[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        features_right = features[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        labels_left = labels[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        labels_right = labels[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        bag_ids_left = bag_ids[features_via_prototype[:,split_col] <= bdc.tree_.threshold[0]]\n",
    "        bag_ids_right = bag_ids[features_via_prototype[:,split_col] > bdc.tree_.threshold[0]]\n",
    "\n",
    "        return split_col, threshold, features_left, features_right, labels_left, labels_right, bag_ids_left, bag_ids_right\n",
    "\n",
    "    def buildDT(self, features, labels, bag_ids, node):\n",
    "            '''\n",
    "            Recursively builds decision tree from the top to bottom\n",
    "            '''\n",
    "            # checking for the terminal conditions\n",
    "\n",
    "            if node.depth >= self.max_depth:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids)) < self.min_samples_split:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if np.unique(labels).shape[0] == 1:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.prototype = self.prototype(bag_ids, features, labels, self.prototype_count)\n",
    "            features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "            \n",
    "            # calculating current split\n",
    "            (splitCol, \n",
    "             thresh, \n",
    "             features_left, \n",
    "             features_right, \n",
    "             labels_left, \n",
    "             labels_right, \n",
    "             bag_ids_left, \n",
    "             bag_ids_right) = self.calcBestSplit(features, \n",
    "                                                 features_updated, \n",
    "                                                 labels, \n",
    "                                                 bag_ids)\n",
    "            \n",
    "            if splitCol is None:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "\n",
    "            if len(np.unique(bag_ids_left)) < self.min_samples_leaf or len(np.unique(bag_ids_right)) < self.min_samples_leaf:\n",
    "                node.is_terminal = True\n",
    "                return\n",
    "            \n",
    "            node.column = splitCol\n",
    "            node.threshold = thresh\n",
    "            \n",
    "            _, index_left  = np.unique(bag_ids_left, return_index=True)\n",
    "            _, index_right  = np.unique(bag_ids_right, return_index=True)\n",
    "            \n",
    "            # creating left and right child nodes\n",
    "            node.left = Node()\n",
    "            node.left.depth = node.depth + 1\n",
    "            node.left.probas = self.nodeProbas(labels_left[index_left])\n",
    "\n",
    "            node.right = Node()\n",
    "            node.right.depth = node.depth + 1\n",
    "            node.right.probas = self.nodeProbas(labels_right[index_right])\n",
    "\n",
    "            # splitting recursively\n",
    "            \n",
    "            self.buildDT(features_right, labels_right, bag_ids_right, node.right)\n",
    "            self.buildDT(features_left, labels_left, bag_ids_left, node.left)\n",
    "\n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        '''\n",
    "        Standard fit function to run all the model training\n",
    "        '''\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        \n",
    "        self.buildDT(features, labels, bag_ids, self.Tree)\n",
    "\n",
    "    def predictSample(self, features, bag_ids, node):\n",
    "        '''\n",
    "        Passes one object through decision tree and return the probability of it to belong to each class\n",
    "        '''\n",
    "\n",
    "        # if we have reached the terminal node of the tree\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, node.prototype)\n",
    "\n",
    "        if features_updated[0][node.column] > node.threshold:\n",
    "            probas = self.predictSample(features, bag_ids, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(features, bag_ids, node.left)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def predict(self, features, bag_ids):\n",
    "        '''\n",
    "        Returns the labels for each X\n",
    "        '''\n",
    "\n",
    "        if type(features) == pd.DataFrame:\n",
    "            X = np.asarray(features)\n",
    "\n",
    "        sort_index = np.argsort(bag_ids)\n",
    "        bag_ids = bag_ids[sort_index]\n",
    "        features = features[sort_index]\n",
    "\n",
    "        features_updated = self.features_via_prototype(self.feature_types, features, bag_ids, self.Tree.prototype)\n",
    "\n",
    "        index  = np.unique(bag_ids, return_index=True)[1]\n",
    "        count  = np.unique(bag_ids, return_counts=True)[1]\n",
    "        index = np.append(index, bag_ids.shape[0])   \n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(index) - 1):\n",
    "            pred = self.predictSample(features[index[i]:index[i+1]], \n",
    "                                                bag_ids[index[i]:index[i+1]], \n",
    "                                                self.Tree)\n",
    "            \n",
    "            pred = np.repeat(pred, count[i])\n",
    "            predictions = np.concatenate((predictions, pred), axis=0)\n",
    "        \n",
    "        return np.asarray(predictions)\n",
    "\n",
    "class PrototypeForest:\n",
    "    def __init__(self, size,\n",
    "                feature_types = [\"min\", \"mean\", \"max\"],\n",
    "                max_depth = 8, \n",
    "                min_samples_leaf = 2, \n",
    "                min_samples_split = 2, \n",
    "                prototype_count = 1,\n",
    "                use_prototype_learner = True,\n",
    "                early_stopping_round = 10):\n",
    "        self.size = size\n",
    "        self._trees = []\n",
    "        self._tuning_trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.prototype_count = prototype_count\n",
    "        self.use_prototype_learner = use_prototype_learner\n",
    "        self.early_stopping_round = early_stopping_round\n",
    "        \n",
    "    def sample(self, features, labels, bag_ids):\n",
    "        ids, index  = np.unique(bag_ids, return_index=True)\n",
    "        group_min = np.minimum.reduceat(labels, index)\n",
    "        \n",
    "        if self.size == 1:\n",
    "            bag_size = math.ceil(group_min.shape[0] * 1)\n",
    "        else:\n",
    "            bag_size = math.ceil(group_min.shape[0] * 0.8)\n",
    "  \n",
    "        bags_all = np.random.choice(ids, bag_size, replace=False)\n",
    "        \n",
    "        df = pd.DataFrame(np.concatenate([train_bag_ids.reshape(train_bag_ids.shape[0],1),\n",
    "                                          train_labels.reshape(train_labels.shape[0],1)],\n",
    "                                         axis=1))\n",
    "        \n",
    "        indices_all = df[df[0].isin(bags_all)].index.to_numpy()\n",
    "        inbag_indices = indices_all\n",
    "        oo_bag_mask = np.ones(labels.shape[0], dtype=bool)\n",
    "        oo_bag_mask[inbag_indices] = False\n",
    "        outbag_indices = np.where(oo_bag_mask == 1)\n",
    "        \n",
    "        return inbag_indices, outbag_indices\n",
    "\n",
    "    \n",
    "    def fit(self, features, labels, bag_ids):\n",
    "        for i in range(self.size):\n",
    "            if i % 10:\n",
    "                print(f\"Tree number {i}\")\n",
    "            (inbag_indices, _) = self.sample(features, labels, bag_ids)\n",
    "            inbag_features = features[inbag_indices]\n",
    "            inbag_labels = labels[inbag_indices]\n",
    "            inbag_bag_ids = bag_ids[inbag_indices]\n",
    "            tree = PrototypeTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                prototype_count = self.prototype_count,\n",
    "                use_prototype_learner = self.use_prototype_learner,\n",
    "                train_features = inbag_features,\n",
    "                early_stopping_round = self.early_stopping_round\n",
    "            )\n",
    "            tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            while tree.Tree.right is None:\n",
    "                tree.fit(inbag_features, inbag_labels, inbag_bag_ids)\n",
    "            self._trees.append(tree)\n",
    "            \n",
    "    def predict(self, features, bag_ids):\n",
    "        temp = [t.predict(features, bag_ids) for t in self._trees]\n",
    "        preds = np.transpose(np.array(temp))\n",
    "        return np.sum(preds, axis=1)/self.size\n",
    "   \n",
    "def split_features_labels_bags(data):\n",
    "    features = data[data.columns[~data.columns.isin([0, 1])]].to_numpy()\n",
    "    labels = data[0].to_numpy()\n",
    "    bag_ids = data[1].to_numpy()\n",
    "\n",
    "    #sort_index = np.argsort(bag_ids)\n",
    "    #bag_ids = bag_ids[sort_index]\n",
    "    #features = features[sort_index]\n",
    "    \n",
    "    return (features, labels, bag_ids)\n",
    "\n",
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "\n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "          \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]    \n",
    "    \n",
    "    #for i in range(2, 94):\n",
    "    #    clean_data = train_data[(train_data[i] != 0) & (train_data[i] != -32767)]\n",
    "    #    mean = clean_data[i].mean()\n",
    "    #    train_data[(train_data[i] == 0) | (train_data[i] == -32767)] = mean\n",
    "\n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n",
    "\n",
    "def generate_random(lower, upper):\n",
    "    random_number = random.random()\n",
    "    random_number = random_number + lower\n",
    "    random_range = upper - lower\n",
    "    random_number = random_number*random_range\n",
    "    return random_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c3ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tree number 1\n",
      "2\n",
      "Tree number 1\n",
      "3\n",
      "Tree number 1\n",
      "4\n",
      "Tree number 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m (train_features,\n\u001b[1;32m     10\u001b[0m      train_labels,\n\u001b[1;32m     11\u001b[0m      train_bag_ids,\n\u001b[1;32m     12\u001b[0m      test_features,\n\u001b[1;32m     13\u001b[0m      test_labels,\n\u001b[1;32m     14\u001b[0m      test_bag_ids) \u001b[38;5;241m=\u001b[39m train_test_split(dataset, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, fit_on_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, cols \u001b[38;5;241m=\u001b[39m col_no)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m PrototypeForest(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     17\u001b[0m                         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     18\u001b[0m                         min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     19\u001b[0m                         min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     20\u001b[0m                         prototype_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m                         early_stopping_round\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m probas \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_features, test_bag_ids)\n\u001b[1;32m     27\u001b[0m pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(probas, test_labels))\n",
      "Cell \u001b[0;32mIn[4], line 328\u001b[0m, in \u001b[0;36mPrototypeForest.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    326\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(inbag_features, inbag_labels, inbag_bag_ids)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minbag_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trees\u001b[38;5;241m.\u001b[39mappend(tree)\n",
      "Cell \u001b[0;32mIn[4], line 217\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree \u001b[38;5;241m=\u001b[39m Node()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTree\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.buildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    163\u001b[0m     node\u001b[38;5;241m.\u001b[39mis_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m node\u001b[38;5;241m.\u001b[39mprototype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m features_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_via_prototype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types, features, bag_ids, node\u001b[38;5;241m.\u001b[39mprototype)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# calculating current split\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.prototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprototype\u001b[39m(\u001b[38;5;28mself\u001b[39m, bags, features, labels, prototype_count):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_prototype_learner:\n\u001b[0;32m---> 55\u001b[0m         prototypes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_prototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         check \u001b[38;5;241m=\u001b[39m prototypes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     58\u001b[0m         check\u001b[38;5;241m.\u001b[39mresize(check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    261\u001b[0m min_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    263\u001b[0m optim1\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 264\u001b[0m \u001b[43moptim2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m lr_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR DROP!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/libs/mil/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/libs/mil/.venv/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/libs/mil/.venv/lib/python3.8/site-packages/torch/optim/functional.py:86\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"WheatYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=2,\n",
    "                            max_depth=2,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_learning_{dataset}.csv\")\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "all_df.to_csv(f\"./performance/predictions__{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1604d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m      9\u001b[0m (train_features,\n\u001b[1;32m     10\u001b[0m      train_labels,\n\u001b[1;32m     11\u001b[0m      train_bag_ids,\n\u001b[1;32m     12\u001b[0m      test_features,\n\u001b[1;32m     13\u001b[0m      test_labels,\n\u001b[1;32m     14\u001b[0m      test_bag_ids) \u001b[38;5;241m=\u001b[39m train_test_split(dataset, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, fit_on_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, cols \u001b[38;5;241m=\u001b[39m col_no)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m PrototypeForest(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     17\u001b[0m                         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     18\u001b[0m                         min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     19\u001b[0m                         min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     20\u001b[0m                         prototype_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m                         early_stopping_round\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m probas \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_features, test_bag_ids)\n\u001b[1;32m     27\u001b[0m pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(probas, test_labels))\n",
      "Cell \u001b[0;32mIn[4], line 326\u001b[0m, in \u001b[0;36mPrototypeForest.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    316\u001b[0m inbag_bag_ids \u001b[38;5;241m=\u001b[39m bag_ids[inbag_indices]\n\u001b[1;32m    317\u001b[0m tree \u001b[38;5;241m=\u001b[39m PrototypeTreeClassifier(\n\u001b[1;32m    318\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth,\n\u001b[1;32m    319\u001b[0m     min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     early_stopping_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_round\n\u001b[1;32m    325\u001b[0m )\n\u001b[0;32m--> 326\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minbag_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(inbag_features, inbag_labels, inbag_bag_ids)\n",
      "Cell \u001b[0;32mIn[4], line 217\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree \u001b[38;5;241m=\u001b[39m Node()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTree\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.buildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    163\u001b[0m     node\u001b[38;5;241m.\u001b[39mis_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m node\u001b[38;5;241m.\u001b[39mprototype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m features_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_via_prototype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types, features, bag_ids, node\u001b[38;5;241m.\u001b[39mprototype)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# calculating current split\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.prototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprototype\u001b[39m(\u001b[38;5;28mself\u001b[39m, bags, features, labels, prototype_count):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_prototype_learner:\n\u001b[0;32m---> 55\u001b[0m         prototypes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_prototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         check \u001b[38;5;241m=\u001b[39m prototypes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     58\u001b[0m         check\u001b[38;5;241m.\u001b[39mresize(check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 227\u001b[0m, in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m max_steps \u001b[38;5;129;01mand\u001b[39;00m max_stagnation \u001b[38;5;241m<\u001b[39m early_stopping_round:\n\u001b[1;32m    226\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 227\u001b[0m     np_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbags_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    229\u001b[0m     batch_inp \u001b[38;5;241m=\u001b[39m bags_train[np_idx]\n",
      "File \u001b[0;32mmtrand.pyx:940\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/libs/mil/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3030\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2912\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2914\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2916\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2917\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3031\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/libs/mil/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for i in range(1, 46):\n",
    "    col_no = i*2 + 2\n",
    "    dataset = \"CornYields\"\n",
    "    print(i)\n",
    "    \n",
    "    (train_features,\n",
    "         train_labels,\n",
    "         train_bag_ids,\n",
    "         test_features,\n",
    "         test_labels,\n",
    "         test_bag_ids) = train_test_split(dataset, 1, 1, 1, fit_on_full = True, cols = col_no)\n",
    "\n",
    "    model = PrototypeForest(size=100,\n",
    "                            max_depth=8,\n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=4,\n",
    "                            prototype_count=1,\n",
    "                            early_stopping_round= 5)\n",
    "\n",
    "    model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "    probas = model.predict(test_features, test_bag_ids)\n",
    "    \n",
    "    pred_list = list(zip(probas, test_labels))\n",
    "    names = dataset.split(\"_\")\n",
    "\n",
    "    pred_list = [(i, x[0], x[1]) for x in pred_list]\n",
    "\n",
    "    pred_all.extend(pred_list)\n",
    "\n",
    "    #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "    #pred_df.to_csv(f\"./performance/prediction_reg_corn_{i}.csv\")\n",
    "\n",
    "    _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "    score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "    mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "    info_list_row = [i, mean]\n",
    "    \n",
    "    info_list.append(info_list_row)\n",
    "\n",
    "perf_df = pd.DataFrame(info_list, columns=[\"i\", \"score\"])\n",
    "perf_df.to_csv(f\"./performance/performance_{dataset}.csv\")\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"i\", \"prediction\", \"label\"])\n",
    "all_df.to_csv(f\"./performance/predictions_{dataset}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf5b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    #data = pd.read_csv(f\"./datasets_regression/{dataset}.csv\", header=None, sep=\" \")\n",
    "    #testbags =  pd.read_csv(f\"./datasets_regression/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    data = pd.read_csv(f\"./datasets_regression/syn_new/{dataset}.csv\", header=None, sep=\",\")\n",
    "    testbags =  pd.read_csv(f\"./datasets_regression/syn_new/cv/{dataset}.csv_rep{rep}_fold{fold}.txt\", sep=\",\")\n",
    "    \n",
    "    data = np.round(data,2)\n",
    "    \n",
    "    if cols:\n",
    "        data = data[list(range(cols))]\n",
    "    \n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "\n",
    "    train_data = data[~data[1].isin(testbags[\"x\"].tolist())]    \n",
    "    \n",
    "    test_data = data[data[1].isin(testbags[\"x\"].tolist())]\n",
    "    \n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance, \n",
    "                         svd_solver = \"full\")), \n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    \n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "\n",
    "    train_features = pipe.transform(train_features)\n",
    "    test_features = pipe.transform(test_features)\n",
    "    \n",
    "    train_features = np.round(train_features,2)\n",
    "    test_features = np.round(test_features,2)\n",
    "\n",
    "    return (\n",
    "        train_features, \n",
    "        train_labels, \n",
    "        train_bag_ids,\n",
    "        test_features, \n",
    "        test_labels,\n",
    "        test_bag_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b4e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_50_nFeat_5_nInsPerBag_5, rep 1, fold 1\n",
      "Tree number 1\n",
      "Tree number 2\n",
      "Tree number 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     14\u001b[0m (train_features,\n\u001b[1;32m     15\u001b[0m      train_labels,\n\u001b[1;32m     16\u001b[0m      train_bag_ids,\n\u001b[1;32m     17\u001b[0m      test_features,\n\u001b[1;32m     18\u001b[0m      test_labels,\n\u001b[1;32m     19\u001b[0m      test_bag_ids) \u001b[38;5;241m=\u001b[39m train_test_split(dataset, rep, fold, \u001b[38;5;241m1\u001b[39m, fit_on_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m PrototypeForest(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     22\u001b[0m                         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     23\u001b[0m                         min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     24\u001b[0m                         min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     25\u001b[0m                         prototype_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     26\u001b[0m                         early_stopping_round\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m probas \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_features, test_bag_ids)\n\u001b[1;32m     32\u001b[0m pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(probas, test_labels))\n",
      "Cell \u001b[0;32mIn[4], line 326\u001b[0m, in \u001b[0;36mPrototypeForest.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    316\u001b[0m inbag_bag_ids \u001b[38;5;241m=\u001b[39m bag_ids[inbag_indices]\n\u001b[1;32m    317\u001b[0m tree \u001b[38;5;241m=\u001b[39m PrototypeTreeClassifier(\n\u001b[1;32m    318\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth,\n\u001b[1;32m    319\u001b[0m     min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     early_stopping_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_round\n\u001b[1;32m    325\u001b[0m )\n\u001b[0;32m--> 326\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minbag_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minbag_bag_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(inbag_features, inbag_labels, inbag_bag_ids)\n",
      "Cell \u001b[0;32mIn[4], line 217\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.fit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree \u001b[38;5;241m=\u001b[39m Node()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTree\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTree\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 208\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.buildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# splitting recursively\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildDT(features_right, labels_right, bag_ids_right, node\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_ids_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 207\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.buildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    203\u001b[0m node\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mprobas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodeProbas(labels_right[index_right])\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# splitting recursively\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildDT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_ids_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildDT(features_left, labels_left, bag_ids_left, node\u001b[38;5;241m.\u001b[39mleft)\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.buildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    163\u001b[0m     node\u001b[38;5;241m.\u001b[39mis_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m node\u001b[38;5;241m.\u001b[39mprototype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprototype_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m features_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_via_prototype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types, features, bag_ids, node\u001b[38;5;241m.\u001b[39mprototype)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# calculating current split\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mPrototypeTreeClassifier.prototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprototype\u001b[39m(\u001b[38;5;28mself\u001b[39m, bags, features, labels, prototype_count):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_prototype_learner:\n\u001b[0;32m---> 55\u001b[0m         prototypes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfind_prototype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         check \u001b[38;5;241m=\u001b[39m prototypes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     58\u001b[0m         check\u001b[38;5;241m.\u001b[39mresize(check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], check\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 246\u001b[0m, in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m--> 246\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    248\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss(out\u001b[38;5;241m.\u001b[39mreshape(batch_size), targets\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    249\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m/\u001b[39mbatch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets_regression/syn_new/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 6):\n",
    "        for fold in range(1, 11):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = True)\n",
    "\n",
    "            model = PrototypeForest(size=100,\n",
    "                                    max_depth=4,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "\n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "\n",
    "perf_df.to_csv(f\"./performance/performance_multitree_synthetic.csv\")\n",
    "\n",
    "all_df.to_csv(f\"./performance/predictions_multitree_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "772033a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 1\n",
      "Current loss 207.18628338972727, best loss 207.18628338972727\n",
      "Current loss 63.85491459671822, best loss 63.85491459671822\n",
      "Current loss 393.7146174112956, best loss 63.85491459671822\n",
      "Current loss 21.071499375005562, best loss 21.071499375005562\n",
      "Current loss 542.390574031406, best loss 21.071499375005562\n",
      "Current loss 145.28053846893212, best loss 21.071499375005562\n",
      "Current loss 193.66240331861707, best loss 21.071499375005562\n",
      "Current loss 1066.951173570421, best loss 21.071499375005562\n",
      "Current loss 246.03356319003635, best loss 21.071499375005562\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 2\n",
      "Current loss 74.60791065378322, best loss 74.60791065378322\n",
      "Current loss 92.2435548570421, best loss 74.60791065378322\n",
      "Current loss 256.5294826295641, best loss 74.60791065378322\n",
      "Current loss 126.05996805429459, best loss 74.60791065378322\n",
      "Current loss 38.63717661301295, best loss 38.63717661301295\n",
      "Current loss 270.21420584784613, best loss 38.63717661301295\n",
      "Current loss 380.52531263563367, best loss 38.63717661301295\n",
      "Current loss 129.9472951889038, best loss 38.63717661301295\n",
      "Current loss 35.69875296334956, best loss 35.69875296334956\n",
      "Current loss 295.3003936343723, best loss 35.69875296334956\n",
      "Current loss 440.362273534139, best loss 35.69875296334956\n",
      "Current loss 216.99370574951172, best loss 35.69875296334956\n",
      "Current loss 19.687484489546883, best loss 19.687484489546883\n",
      "Current loss 200.2808961868286, best loss 19.687484489546883\n",
      "Current loss 569.2827491760254, best loss 19.687484489546883\n",
      "Current loss 563.8521291944716, best loss 19.687484489546883\n",
      "Current loss 184.25832086139255, best loss 19.687484489546883\n",
      "Current loss 21.300166307224167, best loss 19.687484489546883\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 3\n",
      "Current loss 60.13699462717503, best loss 60.13699462717503\n",
      "Current loss 91.94805153210957, best loss 60.13699462717503\n",
      "Current loss 68.70523849295245, best loss 60.13699462717503\n",
      "Current loss 64.52043529351552, best loss 60.13699462717503\n",
      "Current loss 122.67668543756008, best loss 60.13699462717503\n",
      "Current loss 68.01000783840816, best loss 60.13699462717503\n",
      "Current loss 112.4040724022521, best loss 112.4040724022521\n",
      "Current loss 199.74248282114664, best loss 112.4040724022521\n",
      "Current loss 125.95949212710063, best loss 112.4040724022521\n",
      "Current loss 545.0547313690186, best loss 112.4040724022521\n",
      "Current loss 17.966219295863993, best loss 17.966219295863993\n",
      "Current loss 747.8824937608507, best loss 17.966219295863993\n",
      "Current loss 634.5830527411567, best loss 17.966219295863993\n",
      "Current loss 38.84305722183652, best loss 17.966219295863993\n",
      "Current loss 1146.0465757581924, best loss 17.966219295863993\n",
      "Current loss 1074.088858710395, best loss 17.966219295863993\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 4\n",
      "Current loss 31.2002120030423, best loss 31.2002120030423\n",
      "Current loss 50.106361224419544, best loss 31.2002120030423\n",
      "Current loss 350.75057220458984, best loss 31.2002120030423\n",
      "Current loss 24.264955467647976, best loss 24.264955467647976\n",
      "Current loss 491.612238989936, best loss 24.264955467647976\n",
      "Current loss 52.35431996981303, best loss 24.264955467647976\n",
      "Current loss 433.9426049126519, best loss 24.264955467647976\n",
      "Current loss 825.8412907918295, best loss 24.264955467647976\n",
      "Current loss 74.17550927731726, best loss 24.264955467647976\n",
      "Current loss 121.51112359679408, best loss 121.51112359679408\n",
      "Current loss 41.49361807604631, best loss 41.49361807604631\n",
      "Current loss 52.68875908686055, best loss 41.49361807604631\n",
      "Current loss 112.47091449631586, best loss 41.49361807604631\n",
      "Current loss 73.32110761902813, best loss 41.49361807604631\n",
      "Current loss 32.44392312897576, best loss 32.44392312897576\n",
      "Current loss 39.918146888415016, best loss 32.44392312897576\n",
      "Current loss 78.08331660181284, best loss 32.44392312897576\n",
      "Current loss 109.71988952159882, best loss 32.44392312897576\n",
      "Current loss 108.60929617616866, best loss 32.44392312897576\n",
      "Current loss 77.27346734702587, best loss 32.44392312897576\n",
      "Current loss 110.92251166866885, best loss 110.92251166866885\n",
      "Current loss 38.387407276365494, best loss 38.387407276365494\n",
      "Current loss 104.1742687092887, best loss 38.387407276365494\n",
      "Current loss 146.6634054713779, best loss 38.387407276365494\n",
      "Current loss 84.80384264389674, best loss 38.387407276365494\n",
      "Current loss 23.621884091860718, best loss 23.621884091860718\n",
      "Current loss 64.94538657750107, best loss 23.621884091860718\n",
      "Current loss 175.88168578677707, best loss 23.621884091860718\n",
      "Current loss 238.28559313880072, best loss 23.621884091860718\n",
      "Current loss 168.72963198026022, best loss 23.621884091860718\n",
      "Current loss 41.284582176142266, best loss 23.621884091860718\n",
      "Current loss 166.1786028544108, best loss 166.1786028544108\n",
      "Current loss 30.16782529964904, best loss 30.16782529964904\n",
      "Current loss 541.190478430854, best loss 30.16782529964904\n",
      "Current loss 142.4132456249661, best loss 30.16782529964904\n",
      "Current loss 251.11448785993787, best loss 30.16782529964904\n",
      "Current loss 792.9485897488064, best loss 30.16782529964904\n",
      "Current loss 154.64640384250217, best loss 30.16782529964904\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 5\n",
      "Current loss 199.47654048601785, best loss 199.47654048601785\n",
      "Current loss 167.95476055145264, best loss 167.95476055145264\n",
      "Current loss 137.03928905063205, best loss 137.03928905063205\n",
      "Current loss 108.48108475406964, best loss 108.48108475406964\n",
      "Current loss 82.85723947650857, best loss 82.85723947650857\n",
      "Current loss 60.605824417952036, best loss 60.605824417952036\n",
      "Current loss 42.60127565595839, best loss 42.60127565595839\n",
      "Current loss 30.884020942780708, best loss 30.884020942780708\n",
      "Current loss 26.199626922193502, best loss 26.199626922193502\n",
      "Current loss 29.30527945028411, best loss 26.199626922193502\n",
      "Current loss 39.62013869111737, best loss 26.199626922193502\n",
      "Current loss 55.60993989639812, best loss 26.199626922193502\n",
      "Current loss 72.69726400905185, best loss 26.199626922193502\n",
      "Current loss 84.75789215829637, best loss 26.199626922193502\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 6\n",
      "Current loss 154.5586683485243, best loss 154.5586683485243\n",
      "Current loss 104.92613087760077, best loss 104.92613087760077\n",
      "Current loss 62.69683051109314, best loss 62.69683051109314\n",
      "Current loss 31.094504449930454, best loss 31.094504449930454\n",
      "Current loss 11.443889149361187, best loss 11.443889149361187\n",
      "Current loss 4.851980509029494, best loss 4.851980509029494\n",
      "Current loss 10.592941243615416, best loss 4.851980509029494\n",
      "Current loss 24.128578571809662, best loss 4.851980509029494\n",
      "Current loss 37.99401389227973, best loss 4.851980509029494\n",
      "Current loss 48.85873148176405, best loss 4.851980509029494\n",
      "Current loss 58.288625134362114, best loss 4.851980509029494\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 7\n",
      "Current loss 36.205415951042795, best loss 36.205415951042795\n",
      "Current loss 81.5731325911151, best loss 36.205415951042795\n",
      "Current loss 4238.773593478732, best loss 36.205415951042795\n",
      "Current loss 5079.32906765408, best loss 36.205415951042795\n",
      "Current loss 6047.462076822917, best loss 36.205415951042795\n",
      "Current loss 4315.035590277777, best loss 36.205415951042795\n",
      "Current loss 49.68415141126348, best loss 49.68415141126348\n",
      "Current loss 687.1348156399197, best loss 49.68415141126348\n",
      "Current loss 25.860167637467384, best loss 25.860167637467384\n",
      "Current loss 1009.90466732449, best loss 25.860167637467384\n",
      "Current loss 104.42952239016692, best loss 25.860167637467384\n",
      "Current loss 1166.907224867079, best loss 25.860167637467384\n",
      "Current loss 1706.7117784288193, best loss 25.860167637467384\n",
      "Current loss 23.694691660710507, best loss 23.694691660710507\n",
      "Current loss 1735.3824055989583, best loss 23.694691660710507\n",
      "Current loss 2237.374315049913, best loss 23.694691660710507\n",
      "Current loss 189.80606036716037, best loss 23.694691660710507\n",
      "Current loss 1184.1200731065537, best loss 23.694691660710507\n",
      "Current loss 3913.914991590712, best loss 23.694691660710507\n",
      "Current loss 1791.0929378933376, best loss 1791.0929378933376\n",
      "Current loss 3395.3660685221353, best loss 1791.0929378933376\n",
      "Current loss 51.38770071085956, best loss 51.38770071085956\n",
      "Current loss 10428.510891384549, best loss 51.38770071085956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss 1322.7802658081055, best loss 51.38770071085956\n",
      "Current loss 13568.216959635416, best loss 51.38770071085956\n",
      "Current loss 19219.08656141493, best loss 51.38770071085956\n",
      "Current loss 1284.2622642786914, best loss 51.38770071085956\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 8\n",
      "Current loss 142.855400774214, best loss 142.855400774214\n",
      "Current loss 86.88648151223444, best loss 86.88648151223444\n",
      "Current loss 49.16844390663836, best loss 49.16844390663836\n",
      "Current loss 33.76842034856478, best loss 33.76842034856478\n",
      "Current loss 38.49998701446586, best loss 33.76842034856478\n",
      "Current loss 53.873097638418486, best loss 33.76842034856478\n",
      "Current loss 64.52428223979142, best loss 33.76842034856478\n",
      "Current loss 63.98545309404532, best loss 33.76842034856478\n",
      "Current loss 51.155560191306805, best loss 33.76842034856478\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 9\n",
      "Current loss 47.209384670481086, best loss 47.209384670481086\n",
      "Current loss 13.596235040161345, best loss 13.596235040161345\n",
      "Current loss 15.03015012599321, best loss 13.596235040161345\n",
      "Current loss 16.30476595626937, best loss 13.596235040161345\n",
      "Current loss 13.025552711524345, best loss 13.025552711524345\n",
      "Current loss 33.29717089732488, best loss 13.025552711524345\n",
      "Current loss 18.820959673159653, best loss 13.025552711524345\n",
      "Current loss 31.051669511530136, best loss 13.025552711524345\n",
      "Current loss 57.9451806710826, best loss 13.025552711524345\n",
      "Current loss 62.32071738772922, best loss 13.025552711524345\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 1, fold 10\n",
      "Current loss 136.65965527130498, best loss 136.65965527130498\n",
      "Current loss 139.31883036428027, best loss 136.65965527130498\n",
      "Current loss 326.1668408711751, best loss 136.65965527130498\n",
      "Current loss 541.8523839314779, best loss 136.65965527130498\n",
      "Current loss 30.59137873392966, best loss 30.59137873392966\n",
      "Current loss 364.32351048787433, best loss 30.59137873392966\n",
      "Current loss 638.9890535142687, best loss 30.59137873392966\n",
      "Current loss 310.12718921237524, best loss 30.59137873392966\n",
      "Current loss 21.011824346250958, best loss 21.011824346250958\n",
      "Current loss 452.4860651228163, best loss 21.011824346250958\n",
      "Current loss 1110.7175801595051, best loss 21.011824346250958\n",
      "Current loss 747.4336505466038, best loss 21.011824346250958\n",
      "Current loss 23.940913560489815, best loss 21.011824346250958\n",
      "Current loss 934.5071885850695, best loss 21.011824346250958\n",
      "Current loss 188.9298424522082, best loss 188.9298424522082\n",
      "Current loss 989.5025397406685, best loss 188.9298424522082\n",
      "Current loss 358.2640034622616, best loss 188.9298424522082\n",
      "Current loss 1808.3143683539497, best loss 188.9298424522082\n",
      "Current loss 14.767279755117166, best loss 14.767279755117166\n",
      "Current loss 3411.5579291449653, best loss 14.767279755117166\n",
      "Current loss 939.6398107740614, best loss 14.767279755117166\n",
      "Current loss 1607.8645109269355, best loss 14.767279755117166\n",
      "Current loss 3911.6461046006943, best loss 14.767279755117166\n",
      "Current loss 157.8188073900011, best loss 14.767279755117166\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 1\n",
      "Current loss 1927.8514607747395, best loss 1927.8514607747395\n",
      "Current loss 3065.1417066786025, best loss 1927.8514607747395\n",
      "Current loss 536.6261431376139, best loss 536.6261431376139\n",
      "Current loss 10161.612887912326, best loss 536.6261431376139\n",
      "Current loss 729.1838370429145, best loss 536.6261431376139\n",
      "Current loss 19082.78721110026, best loss 536.6261431376139\n",
      "Current loss 2111.975090874566, best loss 536.6261431376139\n",
      "Current loss 14045.846286349826, best loss 536.6261431376139\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 2\n",
      "Current loss 248.2831302218967, best loss 248.2831302218967\n",
      "Current loss 208.79750225279065, best loss 208.79750225279065\n",
      "Current loss 142.51188156339856, best loss 142.51188156339856\n",
      "Current loss 72.90923155264721, best loss 72.90923155264721\n",
      "Current loss 30.103634119033813, best loss 30.103634119033813\n",
      "Current loss 40.380129641956756, best loss 30.103634119033813\n",
      "Current loss 98.58283053504096, best loss 30.103634119033813\n",
      "Current loss 136.68934398227267, best loss 30.103634119033813\n",
      "Current loss 91.96083414554596, best loss 30.103634119033813\n",
      "Current loss 38.50913100461993, best loss 30.103634119033813\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 3\n",
      "Current loss 41.913634121418, best loss 41.913634121418\n",
      "Current loss 140.67162058088513, best loss 41.913634121418\n",
      "Current loss 54.145529216051926, best loss 41.913634121418\n",
      "Current loss 60.40396099093292, best loss 41.913634121418\n",
      "Current loss 159.479154586792, best loss 41.913634121418\n",
      "Current loss 143.93953127331204, best loss 41.913634121418\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 4\n",
      "Current loss 138.91522688335843, best loss 138.91522688335843\n",
      "Current loss 131.13455286953183, best loss 131.13455286953183\n",
      "Current loss 74.15818126179511, best loss 74.15818126179511\n",
      "Current loss 1492.835688273112, best loss 74.15818126179511\n",
      "Current loss 167.22211268213061, best loss 74.15818126179511\n",
      "Current loss 1369.1405639648438, best loss 74.15818126179511\n",
      "Current loss 1542.5667368570964, best loss 74.15818126179511\n",
      "Current loss 241.78531636132135, best loss 74.15818126179511\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 5\n",
      "Current loss 254.80730459425183, best loss 254.80730459425183\n",
      "Current loss 624.1492792765299, best loss 254.80730459425183\n",
      "Current loss 81.04173570209079, best loss 81.04173570209079\n",
      "Current loss 1250.7232326931423, best loss 81.04173570209079\n",
      "Current loss 123.16573768191867, best loss 81.04173570209079\n",
      "Current loss 1338.194127400716, best loss 81.04173570209079\n",
      "Current loss 1932.2005343967014, best loss 81.04173570209079\n",
      "Current loss 27.639672140280407, best loss 27.639672140280407\n",
      "Current loss 2112.6153157552085, best loss 27.639672140280407\n",
      "Current loss 2612.4681294759116, best loss 27.639672140280407\n",
      "Current loss 407.5874256557888, best loss 27.639672140280407\n",
      "Current loss 814.4745059543186, best loss 27.639672140280407\n",
      "Current loss 3969.942911783854, best loss 27.639672140280407\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 6\n",
      "Current loss 141.8243559334013, best loss 141.8243559334013\n",
      "Current loss 60.958677742216324, best loss 60.958677742216324\n",
      "Current loss 31.924005879296196, best loss 31.924005879296196\n",
      "Current loss 65.7698011967457, best loss 31.924005879296196\n",
      "Current loss 105.4199408756362, best loss 31.924005879296196\n",
      "Current loss 81.76178007231404, best loss 31.924005879296196\n",
      "Current loss 42.99671810865402, best loss 31.924005879296196\n",
      "Current loss 36.8949966625207, best loss 31.924005879296196\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 7\n",
      "Current loss 182.28183587392172, best loss 182.28183587392172\n",
      "Current loss 142.90932533476087, best loss 142.90932533476087\n",
      "Current loss 83.56123423741923, best loss 83.56123423741923\n",
      "Current loss 29.669774235950577, best loss 29.669774235950577\n",
      "Current loss 35.97988383306397, best loss 29.669774235950577\n",
      "Current loss 80.57472472720676, best loss 29.669774235950577\n",
      "Current loss 60.92881189286709, best loss 29.669774235950577\n",
      "Current loss 27.031455317926074, best loss 27.031455317926074\n",
      "Current loss 32.9302362203598, best loss 27.031455317926074\n",
      "Current loss 44.39873139725791, best loss 27.031455317926074\n",
      "Current loss 53.36412792073356, best loss 27.031455317926074\n",
      "Current loss 59.812258478668, best loss 27.031455317926074\n",
      "Current loss 57.94337978793515, best loss 27.031455317926074\n",
      "Current loss 86.54541840818193, best loss 86.54541840818193\n",
      "Current loss 28.923252723283237, best loss 28.923252723283237\n",
      "Current loss 20.425563317723572, best loss 20.425563317723572\n",
      "Current loss 73.56619907750024, best loss 20.425563317723572\n",
      "Current loss 67.892115758525, best loss 20.425563317723572\n",
      "Current loss 19.79528132536345, best loss 19.79528132536345\n",
      "Current loss 22.908818437536766, best loss 19.79528132536345\n",
      "Current loss 52.67533950010935, best loss 19.79528132536345\n",
      "Current loss 75.57330641978317, best loss 19.79528132536345\n",
      "Current loss 79.64933125343588, best loss 19.79528132536345\n",
      "Current loss 63.22181686758995, best loss 19.79528132536345\n",
      "Current loss 476.5965525309245, best loss 476.5965525309245\n",
      "Current loss 49.89402128093772, best loss 49.89402128093772\n",
      "Current loss 1319.4194327460395, best loss 49.89402128093772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss 86.70870027277205, best loss 49.89402128093772\n",
      "Current loss 857.1017015245226, best loss 49.89402128093772\n",
      "Current loss 1542.2198401557075, best loss 49.89402128093772\n",
      "Current loss 263.87993812561035, best loss 49.89402128093772\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 8\n",
      "Current loss 27.38820694306762, best loss 27.38820694306762\n",
      "Current loss 31.690476546684902, best loss 27.38820694306762\n",
      "Current loss 542.9401308695475, best loss 27.38820694306762\n",
      "Current loss 88.71866148047977, best loss 27.38820694306762\n",
      "Current loss 162.09717739952936, best loss 27.38820694306762\n",
      "Current loss 383.44539557562933, best loss 27.38820694306762\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 9\n",
      "Current loss 252.5300842920939, best loss 252.5300842920939\n",
      "Current loss 250.12721316019693, best loss 250.12721316019693\n",
      "Current loss 247.5124355951945, best loss 247.5124355951945\n",
      "Current loss 244.58961624569363, best loss 244.58961624569363\n",
      "Current loss 240.96552234225803, best loss 240.96552234225803\n",
      "Current loss 236.244751824273, best loss 236.244751824273\n",
      "Current loss 229.94102827707925, best loss 229.94102827707925\n",
      "Current loss 221.0761627621121, best loss 221.0761627621121\n",
      "Current loss 208.8544486363729, best loss 208.8544486363729\n",
      "Current loss 194.01423507266574, best loss 194.01423507266574\n",
      "Current loss 177.57616186141968, best loss 177.57616186141968\n",
      "Current loss 159.82020865546332, best loss 159.82020865546332\n",
      "Current loss 141.3117331928677, best loss 141.3117331928677\n",
      "Current loss 122.46291473176744, best loss 122.46291473176744\n",
      "Current loss 103.7778637541665, best loss 103.7778637541665\n",
      "Current loss 85.88095927900739, best loss 85.88095927900739\n",
      "Current loss 69.15085150011711, best loss 69.15085150011711\n",
      "Current loss 54.162845302670675, best loss 54.162845302670675\n",
      "Current loss 41.368551361064114, best loss 41.368551361064114\n",
      "Current loss 31.120885054270428, best loss 31.120885054270428\n",
      "Current loss 23.640231317943996, best loss 23.640231317943996\n",
      "Current loss 19.091039803293015, best loss 19.091039803293015\n",
      "Current loss 17.416950101653736, best loss 17.416950101653736\n",
      "Current loss 18.49259080272168, best loss 17.416950101653736\n",
      "Current loss 22.188707639773686, best loss 17.416950101653736\n",
      "Current loss 28.31214915588498, best loss 17.416950101653736\n",
      "Current loss 36.35703834187653, best loss 17.416950101653736\n",
      "Current loss 45.57981435561346, best loss 17.416950101653736\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 2, fold 10\n",
      "Current loss 1962.7414143880208, best loss 1962.7414143880208\n",
      "Current loss 5984.441816541884, best loss 1962.7414143880208\n",
      "Current loss 478.91461012098523, best loss 478.91461012098523\n",
      "Current loss 12494.11432562934, best loss 478.91461012098523\n",
      "Current loss 20.883214356843382, best loss 20.883214356843382\n",
      "Current loss 19567.023952907984, best loss 20.883214356843382\n",
      "Current loss 10843.435574001736, best loss 20.883214356843382\n",
      "Current loss 3635.4178805881074, best loss 20.883214356843382\n",
      "Current loss 34434.4365234375, best loss 20.883214356843382\n",
      "Current loss 19184.963297526043, best loss 20.883214356843382\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 1\n",
      "Current loss 237.35071256425647, best loss 237.35071256425647\n",
      "Current loss 779.7232115003798, best loss 237.35071256425647\n",
      "Current loss 1789.1472676595051, best loss 237.35071256425647\n",
      "Current loss 70.73888444569376, best loss 70.73888444569376\n",
      "Current loss 3338.8064982096353, best loss 70.73888444569376\n",
      "Current loss 528.7712042066786, best loss 70.73888444569376\n",
      "Current loss 3027.123487684462, best loss 70.73888444569376\n",
      "Current loss 6743.971537272136, best loss 70.73888444569376\n",
      "Current loss 864.2046186659071, best loss 70.73888444569376\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 2\n",
      "Current loss 83.44344624297486, best loss 83.44344624297486\n",
      "Current loss 62.36741679906845, best loss 62.36741679906845\n",
      "Current loss 469.79125213623047, best loss 62.36741679906845\n",
      "Current loss 441.6451882256402, best loss 62.36741679906845\n",
      "Current loss 939.9266543918186, best loss 62.36741679906845\n",
      "Current loss 12.127307776846768, best loss 12.127307776846768\n",
      "Current loss 949.2867660522461, best loss 12.127307776846768\n",
      "Current loss 1034.9529156155056, best loss 12.127307776846768\n",
      "Current loss 20.292501502566868, best loss 12.127307776846768\n",
      "Current loss 648.9611379835341, best loss 12.127307776846768\n",
      "Current loss 1699.173334757487, best loss 12.127307776846768\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 3\n",
      "Current loss 478.97867970996435, best loss 478.97867970996435\n",
      "Current loss 1688.049577501085, best loss 478.97867970996435\n",
      "Current loss 34.818788078096176, best loss 34.818788078096176\n",
      "Current loss 4601.501519097223, best loss 34.818788078096176\n",
      "Current loss 23.298714134428238, best loss 23.298714134428238\n",
      "Current loss 6343.441094292535, best loss 23.298714134428238\n",
      "Current loss 6102.900397406684, best loss 23.298714134428238\n",
      "Current loss 902.6169300079346, best loss 23.298714134428238\n",
      "Current loss 10610.923746744791, best loss 23.298714134428238\n",
      "Current loss 9182.227335611979, best loss 23.298714134428238\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 4\n",
      "Current loss 230.63620283206305, best loss 230.63620283206305\n",
      "Current loss 752.6799721188015, best loss 230.63620283206305\n",
      "Current loss 3122.9378289116753, best loss 230.63620283206305\n",
      "Current loss 18531.475043402777, best loss 230.63620283206305\n",
      "Current loss 2586.4747348361543, best loss 230.63620283206305\n",
      "Current loss 21148.55886501736, best loss 230.63620283206305\n",
      "Current loss 1015.2983542548286, best loss 1015.2983542548286\n",
      "Current loss 762.9686099158394, best loss 762.9686099158394\n",
      "Current loss 687.6074341668022, best loss 687.6074341668022\n",
      "Current loss 4630.691460503473, best loss 687.6074341668022\n",
      "Current loss 3495.0711805555557, best loss 687.6074341668022\n",
      "Current loss 1682.7395646837022, best loss 687.6074341668022\n",
      "Current loss 9652.416381835938, best loss 687.6074341668022\n",
      "Current loss 3808.238776312934, best loss 687.6074341668022\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 5\n",
      "Current loss 615.7626022762722, best loss 615.7626022762722\n",
      "Current loss 46.70821331275834, best loss 46.70821331275834\n",
      "Current loss 2181.719930013021, best loss 46.70821331275834\n",
      "Current loss 427.76348325941296, best loss 46.70821331275834\n",
      "Current loss 1751.2011049058701, best loss 46.70821331275834\n",
      "Current loss 3684.3969048394097, best loss 46.70821331275834\n",
      "Current loss 133.04966690805225, best loss 46.70821331275834\n",
      "dataset nBag_10_nFeat_2_nInsPerBag_2, rep 3, fold 6\n",
      "Current loss 40.04747274518013, best loss 40.04747274518013\n",
      "Current loss 63.59412260726094, best loss 40.04747274518013\n",
      "Current loss 178.78319358825684, best loss 40.04747274518013\n",
      "Current loss 239.09998936123318, best loss 40.04747274518013\n",
      "Current loss 160.3326487408744, best loss 40.04747274518013\n",
      "Current loss 16.630867163340252, best loss 16.630867163340252\n",
      "Current loss 137.0840631590949, best loss 16.630867163340252\n",
      "Current loss 397.930536058214, best loss 16.630867163340252\n",
      "Current loss 488.0695739322239, best loss 16.630867163340252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-fb0b49be1aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                     early_stopping_round= 5)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mearly_stopping_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             )\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbag_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbag_bag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3d3abd4947e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, labels, bag_ids)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredictSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3d3abd4947e>\u001b[0m in \u001b[0;36mbuildDT\u001b[0;34m(self, features, labels, bag_ids, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mfeatures_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_via_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3d3abd4947e>\u001b[0m in \u001b[0;36mprototype\u001b[0;34m(self, bags, features, labels, prototype_count)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_prototype_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-2b56d64a9d75>\u001b[0m in \u001b[0;36mfind_prototype\u001b[0;34m(bags, features, labels, early_stopping_round)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mmin_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0moptim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_test_split(dataset, rep, fold, explained_variance, fit_on_full = False, custom=False, cols=None):\n",
    "    data = pd.read_csv(f\"./datasets2/{dataset}.csv\", header=None)\n",
    "    testbags =  pd.read_csv(f\"./datasets2/{dataset}.csv_rep{rep}_fold{fold}.txt\", header=None)\n",
    "    if custom:\n",
    "        min_limit = testbags.min()[0]\n",
    "        max_limit = testbags.max()[0]\n",
    "        size = testbags.size\n",
    "        size_pos = size // 2\n",
    "        pos = list(range(min_limit, min_limit + size_pos))\n",
    "        neg = list(range(max_limit - size_pos + 1, max_limit + 1))\n",
    "        testbags = pd.DataFrame([*pos, *neg])\n",
    "    \n",
    "    train_data = data[~data[1].isin(testbags[0].tolist())]\n",
    "    test_data = data[data[1].isin(testbags[0].tolist())]\n",
    "    (train_features, train_labels, train_bag_ids) = split_features_labels_bags(train_data)\n",
    "    (test_features, test_labels, test_bag_ids) = split_features_labels_bags(test_data)\n",
    "    \n",
    "    if explained_variance < 1:\n",
    "        pipe = Pipeline([('pca', PCA(n_components = explained_variance,\n",
    "                         svd_solver = \"full\")),\n",
    "         ('scaler', StandardScaler()), ])\n",
    "    else:\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ])\n",
    "    if fit_on_full:\n",
    "        pipe.fit(data[data.columns[~data.columns.isin([0,1])]].to_numpy())\n",
    "    else:\n",
    "        pipe.fit(train_features)\n",
    "    #train_features = pipe.transform(train_features)\n",
    "    #test_features = pipe.transform(test_features)\n",
    "    return (\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        train_bag_ids,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "        test_bag_ids)\n",
    "\n",
    "import os\n",
    "\n",
    "folders = os.listdir(\"/home/erdemb/libs/mil/datasets2/\")\n",
    "datasets = [x for x in folders if x != \"cv\"]\n",
    "datasets = [x.split(\".\")[0] for x in datasets]\n",
    "\n",
    "info_list = []\n",
    "pred_all = []\n",
    "\n",
    "datasets = [\"nBag_10_nFeat_2_nInsPerBag_2\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for rep in range(1, 6):\n",
    "        for fold in range(1, 11):\n",
    "            print(f\"dataset {dataset}, rep {rep}, fold {fold}\")\n",
    "            (train_features,\n",
    "                 train_labels,\n",
    "                 train_bag_ids,\n",
    "                 test_features,\n",
    "                 test_labels,\n",
    "                 test_bag_ids) = train_test_split(dataset, rep, fold, 1, fit_on_full = False)\n",
    "\n",
    "            model = PrototypeForest(size=1,\n",
    "                                    max_depth=2,\n",
    "                                    min_samples_leaf=2,\n",
    "                                    min_samples_split=4,\n",
    "                                    prototype_count=1,\n",
    "                                    early_stopping_round= 5)\n",
    "\n",
    "            model.fit(train_features, train_labels, train_bag_ids)\n",
    "            \n",
    "            probas = model.predict(test_features, test_bag_ids)\n",
    "            \n",
    "            pred_list = list(zip(probas, test_labels))\n",
    "            names = dataset.split(\"_\")\n",
    "            \n",
    "            pred_list = [(names[1], names[3], names[5], rep, fold, x[0], x[1]) for x in pred_list]\n",
    "            \n",
    "            pred_all.extend(pred_list)\n",
    "\n",
    "            #pred_df = pd.DataFrame(probas, columns=[\"prediction\"])\n",
    "            #pred_df.to_csv(f\"./performance/prediction_{dataset}_rep_{rep}_fold_{fold}.csv\")\n",
    "\n",
    "            _, index  = np.unique(test_bag_ids, return_index=True)\n",
    "\n",
    "            score = metrics.mean_absolute_error(test_labels[index], probas[index])\n",
    "\n",
    "            mean = score/(np.sum(test_labels)/test_labels.size)\n",
    "            info_list_row = [dataset, rep, fold, mean]\n",
    "\n",
    "            info_list.append(info_list_row)\n",
    "\n",
    "all_df = pd.DataFrame(pred_all, columns=[\"nBag\", \"nFeat\",\"nInsPerBag\",\"rep\", \"fold\", \"prediction\", \"label\"])\n",
    "           \n",
    "perf_df = pd.DataFrame(info_list, columns=[\"dataset\", \"rep\", \"fold\", \"score\"])\n",
    "\n",
    "\n",
    "#perf_df.to_csv(f\"./performance/performance_learning_synthetic.csv\")\n",
    "\n",
    "#all_df.to_csv(f\"./performance/predictions_learning_synthetic.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
